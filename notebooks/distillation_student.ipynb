{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9dafc480",
   "metadata": {},
   "source": [
    "# Distillation. Student"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bba164c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: Currently logged in as: dmykhailov (dmykhailov-kyiv-school-of-economics) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin\n"
     ]
    }
   ],
   "source": [
    "machine = \"local\"\n",
    "!wandb login"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34bdd7d3",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ff175635",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch: 2.9.1+cu128\n",
      "Device: NVIDIA GeForce RTX 5050 Laptop GPU\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import gc\n",
    "import logging\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "from typing import cast\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "\n",
    "import cv2\n",
    "import timm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import pytorch_lightning as pl\n",
    "from tqdm import tqdm\n",
    "from torch.optim import AdamW\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint, EarlyStopping, LearningRateMonitor\n",
    "from pytorch_lightning.loggers import WandbLogger\n",
    "\n",
    "import warnings\n",
    "from notebooks_config import setup_logging, CustomLogger\n",
    "\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "print(f\"PyTorch: {torch.__version__}\")\n",
    "print(f\"Device: {torch.cuda.get_device_name(0) if torch.cuda.is_available() else 'CPU'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ba1c03c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;2;105;254;105m\n",
      "[2025-12-11 14:59:29]\n",
      "SUCCESS: Logging configured successfully âœ…\u001b[0m\n",
      "\u001b[38;2;105;254;105m\n",
      "[2025-12-11 14:59:29]\n",
      "SUCCESS: Logging configuration test completed.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "logger = setup_logging(level=logging.DEBUG, full_color=True, include_function=False)\n",
    "logger = cast(CustomLogger, logger)  # Type hinting\n",
    "logger.success(\"Logging configuration test completed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e452128a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 1488\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DESCRIPTION_FULL: swinv2_tiny_window8_256-local_train[5]Folds_log_fusion-mean_epochs20_bs16_gradacc1_lr0.0001_wd0.05_dr0.2_hr0.5\n",
      "Effective batch size: 16\n"
     ]
    }
   ],
   "source": [
    "cpu_count = os.cpu_count()\n",
    "NUM_WORKERS = 0 if machine == \"local\" else cpu_count // 2 if cpu_count else 0\n",
    "\n",
    "LR = 1e-4\n",
    "EPOCHS = 25\n",
    "N_FOLDS = 5\n",
    "GRAD_ACCUM = 1\n",
    "BATCH_SIZE = 16\n",
    "DROPOUT_RATE = 0.3\n",
    "DISTILL_ALPHA = 0.5  # Weight for distillation loss\n",
    "WEIGHT_DECAY = 0.05\n",
    "HIDDEN_RATIO = 0.5\n",
    "TRAIN_SPLIT_RATIO = 0.02 # Used if N_FOLDS = 0\n",
    "\n",
    "MODEL = \"swinv2_tiny_window8_256\"\n",
    "MODEL_STAGE = \"student\"  # 'teacher' or 'student'\n",
    "PROJECT_NAME = \"csiro-image2biomass-prediction\"\n",
    "CHECKPOINTS_DIR = f\"./kaggle/checkpoints/{MODEL_STAGE}/\"\n",
    "USE_OOF_SOFT_TARGETS = False  # Whether to use OOF soft targets or 100% ensemble soft targets\n",
    "\n",
    "# Each patch is 1000x1000, resize to 768x768 for vision transformers\n",
    "SIZE = 768\n",
    "USE_LOG_TARGET = True   # Whether to use log1p transformation on target variable\n",
    "FUSION_METHOD = 'mean'  # ('concat', 'mean', 'max')\n",
    "\n",
    "DESCRIPTION = machine + \\\n",
    "    (f\"_train{TRAIN_SPLIT_RATIO}\" if N_FOLDS == 0 else f\"_train[{N_FOLDS}]Folds\") + (\n",
    "        f\"_log\" if USE_LOG_TARGET else \"\") + f\"_fusion-{FUSION_METHOD}\"\n",
    "DESCRIPTION_FULL = MODEL + \"-\" + DESCRIPTION + \\\n",
    "    f\"_epochs{EPOCHS}_bs{BATCH_SIZE}_gradacc{GRAD_ACCUM}_lr{LR}_wd{WEIGHT_DECAY}_dr{DROPOUT_RATE}_hr{HIDDEN_RATIO}\"\n",
    "SUBMISSION_NAME = f\"{DESCRIPTION_FULL}_submission.csv\"\n",
    "SUBMISSION_ENSEMBLE_NAME = f\"{DESCRIPTION_FULL}_ensemble_submission.csv\"\n",
    "SUBMISSION_MSG = DESCRIPTION_FULL.replace(\"_\", \" \")\n",
    "\n",
    "SEED = 1488\n",
    "torch.manual_seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "pl.seed_everything(SEED)\n",
    "\n",
    "print(\"DESCRIPTION_FULL:\", DESCRIPTION_FULL)\n",
    "print(f\"Effective batch size: {BATCH_SIZE * GRAD_ACCUM}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1804fc71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "NUM_WORKERS: 0\n",
      "\n",
      "NVIDIA GeForce RTX 5050 Laptop GPU\n",
      "Memory Usage:\n",
      "Allocated: 0.0 GB\n",
      "Cached:    0.0 GB\n"
     ]
    }
   ],
   "source": [
    "# setting device on GPU if available, else CPU\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Using device:', DEVICE)\n",
    "print('NUM_WORKERS:', NUM_WORKERS)\n",
    "print()\n",
    "\n",
    "# Additional Info when using cuda\n",
    "if DEVICE.type == 'cuda':\n",
    "    # clean GPU memory\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "\n",
    "    torch.set_float32_matmul_precision('high') if machine == \"local\" else None\n",
    "\n",
    "    print(torch.cuda.get_device_name(0))\n",
    "    print('Memory Usage:')\n",
    "    print('Allocated:', round(torch.cuda.memory_allocated(0)/1024**3, 1), 'GB')\n",
    "    print('Cached:   ', round(torch.cuda.memory_reserved(0)/1024**3, 1), 'GB')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b8b9504",
   "metadata": {},
   "source": [
    "## Data Loading and Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e402a80c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset size: (1071, 9)\n"
     ]
    },
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "sample_id",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "image_path",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "Sampling_Date",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "State",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "Species",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "Pre_GSHH_NDVI",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Height_Ave_cm",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "target_name",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "target",
         "rawType": "float64",
         "type": "float"
        }
       ],
       "ref": "c6faf785-67da-459d-a755-1c9d730d6efa",
       "rows": [
        [
         "0",
         "ID1011485656__Dry_Clover_g",
         "train/ID1011485656.jpg",
         "2015/9/4",
         "Tas",
         "Ryegrass_Clover",
         "0.62",
         "4.6667",
         "Dry_Clover_g",
         "0.0"
        ],
        [
         "1",
         "ID1011485656__Dry_Dead_g",
         "train/ID1011485656.jpg",
         "2015/9/4",
         "Tas",
         "Ryegrass_Clover",
         "0.62",
         "4.6667",
         "Dry_Dead_g",
         "31.9984"
        ],
        [
         "2",
         "ID1011485656__Dry_Green_g",
         "train/ID1011485656.jpg",
         "2015/9/4",
         "Tas",
         "Ryegrass_Clover",
         "0.62",
         "4.6667",
         "Dry_Green_g",
         "16.275"
        ],
        [
         "5",
         "ID1012260530__Dry_Clover_g",
         "train/ID1012260530.jpg",
         "2015/4/1",
         "NSW",
         "Lucerne",
         "0.55",
         "16.0",
         "Dry_Clover_g",
         "0.0"
        ],
        [
         "6",
         "ID1012260530__Dry_Dead_g",
         "train/ID1012260530.jpg",
         "2015/4/1",
         "NSW",
         "Lucerne",
         "0.55",
         "16.0",
         "Dry_Dead_g",
         "0.0"
        ]
       ],
       "shape": {
        "columns": 9,
        "rows": 5
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sample_id</th>\n",
       "      <th>image_path</th>\n",
       "      <th>Sampling_Date</th>\n",
       "      <th>State</th>\n",
       "      <th>Species</th>\n",
       "      <th>Pre_GSHH_NDVI</th>\n",
       "      <th>Height_Ave_cm</th>\n",
       "      <th>target_name</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ID1011485656__Dry_Clover_g</td>\n",
       "      <td>train/ID1011485656.jpg</td>\n",
       "      <td>2015/9/4</td>\n",
       "      <td>Tas</td>\n",
       "      <td>Ryegrass_Clover</td>\n",
       "      <td>0.62</td>\n",
       "      <td>4.6667</td>\n",
       "      <td>Dry_Clover_g</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ID1011485656__Dry_Dead_g</td>\n",
       "      <td>train/ID1011485656.jpg</td>\n",
       "      <td>2015/9/4</td>\n",
       "      <td>Tas</td>\n",
       "      <td>Ryegrass_Clover</td>\n",
       "      <td>0.62</td>\n",
       "      <td>4.6667</td>\n",
       "      <td>Dry_Dead_g</td>\n",
       "      <td>31.9984</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ID1011485656__Dry_Green_g</td>\n",
       "      <td>train/ID1011485656.jpg</td>\n",
       "      <td>2015/9/4</td>\n",
       "      <td>Tas</td>\n",
       "      <td>Ryegrass_Clover</td>\n",
       "      <td>0.62</td>\n",
       "      <td>4.6667</td>\n",
       "      <td>Dry_Green_g</td>\n",
       "      <td>16.2750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>ID1012260530__Dry_Clover_g</td>\n",
       "      <td>train/ID1012260530.jpg</td>\n",
       "      <td>2015/4/1</td>\n",
       "      <td>NSW</td>\n",
       "      <td>Lucerne</td>\n",
       "      <td>0.55</td>\n",
       "      <td>16.0000</td>\n",
       "      <td>Dry_Clover_g</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>ID1012260530__Dry_Dead_g</td>\n",
       "      <td>train/ID1012260530.jpg</td>\n",
       "      <td>2015/4/1</td>\n",
       "      <td>NSW</td>\n",
       "      <td>Lucerne</td>\n",
       "      <td>0.55</td>\n",
       "      <td>16.0000</td>\n",
       "      <td>Dry_Dead_g</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    sample_id              image_path Sampling_Date State  \\\n",
       "0  ID1011485656__Dry_Clover_g  train/ID1011485656.jpg      2015/9/4   Tas   \n",
       "1    ID1011485656__Dry_Dead_g  train/ID1011485656.jpg      2015/9/4   Tas   \n",
       "2   ID1011485656__Dry_Green_g  train/ID1011485656.jpg      2015/9/4   Tas   \n",
       "5  ID1012260530__Dry_Clover_g  train/ID1012260530.jpg      2015/4/1   NSW   \n",
       "6    ID1012260530__Dry_Dead_g  train/ID1012260530.jpg      2015/4/1   NSW   \n",
       "\n",
       "           Species  Pre_GSHH_NDVI  Height_Ave_cm   target_name   target  \n",
       "0  Ryegrass_Clover           0.62         4.6667  Dry_Clover_g   0.0000  \n",
       "1  Ryegrass_Clover           0.62         4.6667    Dry_Dead_g  31.9984  \n",
       "2  Ryegrass_Clover           0.62         4.6667   Dry_Green_g  16.2750  \n",
       "5          Lucerne           0.55        16.0000  Dry_Clover_g   0.0000  \n",
       "6          Lucerne           0.55        16.0000    Dry_Dead_g   0.0000  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "PATH_DATA = './kaggle/input/csiro-biomass'\n",
    "PATH_TRAIN_CSV = os.path.join(PATH_DATA, 'train.csv')\n",
    "PATH_TEST_CSV = os.path.join(PATH_DATA, 'test.csv')\n",
    "PATH_TRAIN_IMG = os.path.join(PATH_DATA, 'train')\n",
    "PATH_TEST_IMG = os.path.join(PATH_DATA, 'test')\n",
    "\n",
    "df = pd.read_csv(PATH_TRAIN_CSV)\n",
    "# Remove unneeded targets\n",
    "df = df[~df['target_name'].isin(['Dry_Total_g', 'GDM_g'])]\n",
    "print(f\"Dataset size: {df.shape}\")\n",
    "display(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e1285873",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(357, 9)\n",
      "               image_path Sampling_Date State            Species  \\\n",
      "0  train/ID1011485656.jpg      2015/9/4   Tas    Ryegrass_Clover   \n",
      "1  train/ID1012260530.jpg      2015/4/1   NSW            Lucerne   \n",
      "2  train/ID1025234388.jpg      2015/9/1    WA  SubcloverDalkeith   \n",
      "3  train/ID1028611175.jpg     2015/5/18   Tas           Ryegrass   \n",
      "4  train/ID1035947949.jpg     2015/9/11   Tas           Ryegrass   \n",
      "\n",
      "   Height_Ave_cm  Pre_GSHH_NDVI  Dry_Clover_g  Dry_Dead_g  Dry_Green_g  \n",
      "0         4.6667           0.62        0.0000     31.9984      16.2750  \n",
      "1        16.0000           0.55        0.0000      0.0000       7.6000  \n",
      "2         1.0000           0.38        6.0500      0.0000       0.0000  \n",
      "3         5.0000           0.66        0.0000     30.9703      24.2376  \n",
      "4         3.5000           0.54        0.4343     23.2239      10.5261  \n"
     ]
    }
   ],
   "source": [
    "# pivot the dataframe to have one row per image with multiple target columns\n",
    "tabular_df = df.pivot_table(index=['image_path', 'Sampling_Date', 'State', 'Species', 'Height_Ave_cm', 'Pre_GSHH_NDVI'],\n",
    "                            columns='target_name', values='target', aggfunc='first').reset_index()\n",
    "tabular_df.columns.name = None  # remove the aggregation name\n",
    "print(tabular_df.shape)\n",
    "print(tabular_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0c96a1db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['image_path', 'Sampling_Date', 'State', 'Species', 'Height_Ave_cm', 'Pre_GSHH_NDVI', 'Dry_Clover_g', 'Dry_Dead_g', 'Dry_Green_g']\n"
     ]
    }
   ],
   "source": [
    "print(tabular_df.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0220a26c",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_cols  = ['Dry_Clover_g', 'Dry_Dead_g', 'Dry_Green_g']\n",
    "num_features = ['Height_Ave_cm', 'Pre_GSHH_NDVI']\n",
    "cat_features = ['Species', 'State']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b44b2e92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BUG: data leakage, will be fixed later in this code\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', StandardScaler(), num_features), # normalizing numeric features\n",
    "        ('cat', OneHotEncoder(sparse_output=False, handle_unknown='ignore'), cat_features) # OHE for categorical features\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cdb7c40c",
   "metadata": {},
   "outputs": [],
   "source": [
    "tabular_data = preprocessor.fit_transform(tabular_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6bd47682",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(357, 21)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[-0.28520388, -0.24631873,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  1.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  1.        ,  0.        ,\n",
       "         0.        ],\n",
       "       [ 0.81823967, -0.70706013,  0.        ,  0.        ,  0.        ,\n",
       "         1.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  1.        ,  0.        ,  0.        ,\n",
       "         0.        ],\n",
       "       [-0.64220462, -1.82600352,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  1.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         1.        ],\n",
       "       [-0.25275281,  0.01696207,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  1.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  1.        ,  0.        ,\n",
       "         0.        ],\n",
       "       [-0.39879723, -0.77288032,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  1.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  1.        ,  0.        ,\n",
       "         0.        ]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(tabular_data.shape)\n",
    "display(tabular_data[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2b5017a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;2;105;254;105m\n",
      "[2025-12-11 15:00:09]\n",
      "SUCCESS: Loaded soft targets: (357, 15)\u001b[0m\n",
      "\u001b[38;2;161;247;255m\n",
      "[2025-12-11 15:00:09]\n",
      "INFO: Columns: ['image_path', 'Sampling_Date', 'State', 'Species', 'Height_Ave_cm', 'Pre_GSHH_NDVI', 'Dry_Clover_g', 'Dry_Dead_g', 'Dry_Green_g', 'Season', 'strat_group', 'fold', 'Dry_Clover_g_soft', 'Dry_Dead_g_soft', 'Dry_Green_g_soft']\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Load OOF soft targets from Teacher\n",
    "path_soft_targets = './kaggle/input/{type}.csv'\n",
    "path_soft_targets = path_soft_targets.format(type='train_with_oof_soft_targets' if USE_OOF_SOFT_TARGETS else 'train_with_soft_targets')\n",
    "\n",
    "soft_targets_df = pd.read_csv(path_soft_targets)\n",
    "\n",
    "logger.success(f\"Loaded soft targets: {soft_targets_df.shape}\")\n",
    "logger.info(f\"Columns: {soft_targets_df.columns.tolist()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "be47d480",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;2;161;247;255m\n",
      "[2025-12-11 15:00:15]\n",
      "INFO: Soft targets preview:\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "Dry_Clover_g_soft",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Dry_Dead_g_soft",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Dry_Green_g_soft",
         "rawType": "float64",
         "type": "float"
        }
       ],
       "ref": "a29f1618-bd9c-40c1-9f20-bbc739ba8fb5",
       "rows": [
        [
         "0",
         "0.36956924",
         "43.185364",
         "20.15381"
        ],
        [
         "1",
         "0.051065993",
         "0.5456077",
         "7.336673"
        ],
        [
         "2",
         "6.852712",
         "0.061617006",
         "0.0"
        ],
        [
         "3",
         "0.5613593",
         "32.825825",
         "19.57612"
        ],
        [
         "4",
         "1.1006649",
         "16.536076",
         "8.233045"
        ]
       ],
       "shape": {
        "columns": 3,
        "rows": 5
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Dry_Clover_g_soft</th>\n",
       "      <th>Dry_Dead_g_soft</th>\n",
       "      <th>Dry_Green_g_soft</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.369569</td>\n",
       "      <td>43.185364</td>\n",
       "      <td>20.153810</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.051066</td>\n",
       "      <td>0.545608</td>\n",
       "      <td>7.336673</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6.852712</td>\n",
       "      <td>0.061617</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.561359</td>\n",
       "      <td>32.825825</td>\n",
       "      <td>19.576120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.100665</td>\n",
       "      <td>16.536076</td>\n",
       "      <td>8.233045</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Dry_Clover_g_soft  Dry_Dead_g_soft  Dry_Green_g_soft\n",
       "0           0.369569        43.185364         20.153810\n",
       "1           0.051066         0.545608          7.336673\n",
       "2           6.852712         0.061617          0.000000\n",
       "3           0.561359        32.825825         19.576120\n",
       "4           1.100665        16.536076          8.233045"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Verify we have soft target columns\n",
    "soft_cols = ['Dry_Clover_g_soft', 'Dry_Dead_g_soft', 'Dry_Green_g_soft']\n",
    "assert all(col in soft_targets_df.columns for col in soft_cols), \"Missing soft target columns!\"\n",
    "\n",
    "logger.info(f\"Soft targets preview:\")\n",
    "display(soft_targets_df[soft_cols].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7dbb541",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "413e7e05",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BiomassStudentDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Student dataset for distillation.\n",
    "    Returns: image + hard targets + soft targets (from Teacher OOF predictions)\n",
    "    NO tabular features!\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        df: pd.DataFrame,\n",
    "        target_cols: list[str],\n",
    "        soft_target_cols: list[str],\n",
    "        img_dir: str,\n",
    "        transform: transforms.Compose | None = None,\n",
    "        is_test: bool = False,\n",
    "        use_log_target: bool = True\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            df: DataFrame with image_path, hard targets, and soft targets\n",
    "            target_cols: List of hard target column names\n",
    "            soft_target_cols: List of soft target column names (from Teacher)\n",
    "            img_dir: Root directory for images\n",
    "            transform: torchvision transform pipeline\n",
    "            is_test: If True, targets are not expected\n",
    "            use_log_target: If True, apply log1p transform to hard targets\n",
    "        \"\"\"\n",
    "        self.df = df.reset_index(drop=True)\n",
    "        self.target_cols = target_cols\n",
    "        self.soft_target_cols = soft_target_cols\n",
    "        self.img_dir = img_dir\n",
    "        self.transform = transform\n",
    "        self.is_test = is_test\n",
    "        self.use_log_target = use_log_target\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx: int) -> dict:\n",
    "        \"\"\"\n",
    "        Returns:\n",
    "            dict with keys:\n",
    "                - 'left_image': tensor [C, H, W]\n",
    "                - 'right_image': tensor [C, H, W]\n",
    "                - 'hard_targets': tensor [3] - real ground truth\n",
    "                - 'soft_targets': tensor [3] - Teacher predictions\n",
    "                - 'image_id': str\n",
    "        \"\"\"\n",
    "        row = self.df.iloc[idx]\n",
    "\n",
    "        # Load image\n",
    "        img_path = os.path.join(self.img_dir, row['image_path'].replace('train/', '').replace('test/', ''))\n",
    "        image = cv2.imread(img_path)\n",
    "\n",
    "        if image is None:\n",
    "            raise FileNotFoundError(f\"Cannot load image: {img_path}\")\n",
    "\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        # Split into left and right patches\n",
    "        h, w, c = image.shape\n",
    "        mid_w = w // 2\n",
    "\n",
    "        left_patch = image[:, :mid_w, :]\n",
    "        right_patch = image[:, mid_w:, :]\n",
    "\n",
    "        # Convert to PIL\n",
    "        left_pil = Image.fromarray(left_patch)\n",
    "        right_pil = Image.fromarray(right_patch)\n",
    "\n",
    "        # Apply transforms\n",
    "        if self.transform:\n",
    "            left_tensor = self.transform(left_pil)\n",
    "            right_tensor = self.transform(right_pil)\n",
    "        else:\n",
    "            left_tensor = transforms.ToTensor()(left_pil)\n",
    "            right_tensor = transforms.ToTensor()(right_pil)\n",
    "\n",
    "        output = {\n",
    "            'left_image': left_tensor,\n",
    "            'right_image': right_tensor,\n",
    "            'image_id': row['image_path'].split('/')[-1].replace('.jpg', '')\n",
    "        }\n",
    "\n",
    "        # Add targets if not test\n",
    "        if not self.is_test:\n",
    "            # Hard targets (ground truth)\n",
    "            hard_targets = row[self.target_cols].values.astype(np.float32)\n",
    "            if self.use_log_target:\n",
    "                hard_targets = np.log1p(hard_targets)\n",
    "            \n",
    "            # Soft targets (Teacher predictions - already in original scale!)\n",
    "            soft_targets = row[self.soft_target_cols].values.astype(np.float32)\n",
    "            # Apply log if needed to match hard targets space\n",
    "            if self.use_log_target:\n",
    "                soft_targets = np.log1p(soft_targets)\n",
    "\n",
    "            output['hard_targets'] = torch.tensor(hard_targets, dtype=torch.float32)\n",
    "            output['soft_targets'] = torch.tensor(soft_targets, dtype=torch.float32)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14e0673c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_img_data_stat(df: pd.DataFrame):\n",
    "    \"\"\"Calculate mean and std of image data for normalization.\"\"\"\n",
    "    means = []\n",
    "    stds = []\n",
    "\n",
    "    loader = tqdm(df['image_path'], desc=\"Calculating image stats\")\n",
    "\n",
    "    for img_path in loader:\n",
    "        full_path = os.path.join(PATH_TRAIN_IMG, img_path.replace('train/', '').replace('test/', ''))\n",
    "        image = cv2.imread(full_path)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        image = image / 255.0  # Normalize to [0, 1]\n",
    "\n",
    "        means.append(np.mean(image, axis=(0, 1)))\n",
    "        stds.append(np.std(image, axis=(0, 1)))\n",
    "\n",
    "    mean = np.mean(means, axis=0)\n",
    "    std = np.mean(stds, axis=0)\n",
    "\n",
    "    return mean, std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7b4abf49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_mean, train_std = calculate_img_data_stat(tabular_df)\n",
    "# print(f\"Train Image Mean: {train_mean}, Std: {train_std}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f7f4ee23",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'url': 'https://github.com/SwinTransformer/storage/releases/download/v2.0.0/swinv2_tiny_patch4_window8_256.pth',\n",
       " 'hf_hub_id': 'timm/swinv2_tiny_window8_256.ms_in1k',\n",
       " 'architecture': 'swinv2_tiny_window8_256',\n",
       " 'tag': 'ms_in1k',\n",
       " 'custom_load': False,\n",
       " 'input_size': (3, 256, 256),\n",
       " 'fixed_input_size': True,\n",
       " 'interpolation': 'bicubic',\n",
       " 'crop_pct': 0.9,\n",
       " 'crop_mode': 'center',\n",
       " 'mean': (0.485, 0.456, 0.406),\n",
       " 'std': (0.229, 0.224, 0.225),\n",
       " 'num_classes': 1000,\n",
       " 'pool_size': (8, 8),\n",
       " 'first_conv': 'patch_embed.proj',\n",
       " 'classifier': 'head.fc',\n",
       " 'license': 'mit'}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Image backbone (processes each patch independently)\n",
    "temp_backbone = timm.create_model(\n",
    "    MODEL,\n",
    "    pretrained=True,\n",
    "    num_classes=0,  # remove classification head\n",
    "    global_pool='avg'\n",
    ")\n",
    "\n",
    "temp_backbone.default_cfg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d60de83f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Backbone expected input size: (3, 256, 256), using SIZE=256\n",
      "Backbone expected mean: (0.485, 0.456, 0.406), std: (0.229, 0.224, 0.225)\n"
     ]
    }
   ],
   "source": [
    "inputs_size = temp_backbone.default_cfg['input_size']\n",
    "mean = temp_backbone.default_cfg['mean']\n",
    "std = temp_backbone.default_cfg['std']\n",
    "\n",
    "SIZE = int(inputs_size[1]) if inputs_size is not None and inputs_size[1] == inputs_size[2] else 256\n",
    "print(f\"Backbone expected input size: {inputs_size}, using SIZE={SIZE}\")\n",
    "print(f\"Backbone expected mean: {mean}, std: {std}\")\n",
    "\n",
    "# Get backbone output dimension\n",
    "with torch.no_grad():\n",
    "    try:\n",
    "        dummy = torch.randn(1, 3, SIZE, SIZE)\n",
    "        feat_dim = temp_backbone(dummy).shape[1]\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error getting backbone feature dimension: {e}\")\n",
    "        raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06c353b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Student needs STRONGER augmentations (he has no metadata!)\n",
    "student_train_transform = transforms.Compose([\n",
    "    transforms.Resize((SIZE, SIZE)),\n",
    "    \n",
    "    # Geometric augmentations (stronger)\n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "    transforms.RandomVerticalFlip(p=0.5),\n",
    "    transforms.RandomRotation(degrees=90),\n",
    "    transforms.RandomAffine(degrees=15, translate=(0.1, 0.1), scale=(0.9, 1.1)),\n",
    "    \n",
    "    # Color augmentations (stronger)\n",
    "    transforms.RandomApply([\n",
    "        transforms.ColorJitter(brightness=0.4, contrast=0.4, saturation=0.4, hue=0.15)\n",
    "    ], p=0.6),\n",
    "    \n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=mean, std=std),\n",
    "    \n",
    "    # Stronger occlusion simulation\n",
    "    transforms.RandomErasing(p=0.3, scale=(0.02, 0.15), ratio=(0.3, 3.3))\n",
    "])\n",
    "\n",
    "# Validation transform (same as before)\n",
    "student_val_transform = transforms.Compose([\n",
    "    transforms.Resize((SIZE, SIZE)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=mean, std=std)\n",
    "])\n",
    "\n",
    "logger.info(\"Student augmentations are MORE AGGRESSIVE than Teacher!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc5ef120",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Left image shape: torch.Size([3, 256, 256])\n",
      "Right image shape: torch.Size([3, 256, 256])\n",
      "Tabular shape: torch.Size([21])\n",
      "Targets shape: torch.Size([3])\n",
      "Image ID: ID1011485656\n",
      "Target values: tensor([0.0000, 3.4965, 2.8493])\n",
      "\n",
      "Original targets from df: [np.float64(0.0) np.float64(31.9984) np.float64(16.275)]\n",
      "Log-transformed targets: tensor([0.0000, 3.4965, 2.8493])\n",
      "Should be close to: [0.        3.496459  2.8492603]\n"
     ]
    }
   ],
   "source": [
    "# Create dataset instance\n",
    "train_dataset = BiomassStudentDataset(\n",
    "    df=tabular_df,\n",
    "    tabular_features=tabular_data,\n",
    "    target_cols=target_cols,\n",
    "    img_dir=PATH_TRAIN_IMG,\n",
    "    transform=train_transform,\n",
    "    is_test=False\n",
    ")\n",
    "\n",
    "# Test it\n",
    "sample = train_dataset[0]\n",
    "print(f\"Left image shape: {sample['left_image'].shape}\")\n",
    "print(f\"Right image shape: {sample['right_image'].shape}\")\n",
    "print(f\"Tabular shape: {sample['tabular'].shape}\")\n",
    "print(f\"Targets shape: {sample['targets'].shape}\")\n",
    "print(f\"Image ID: {sample['image_id']}\")\n",
    "print(f\"Target values: {sample['targets']}\")\n",
    "print()\n",
    "\n",
    "# Test dataset with log transform\n",
    "print(f\"Original targets from df: {tabular_df.iloc[0][target_cols].values}\")\n",
    "print(f\"Log-transformed targets: {sample['targets']}\")\n",
    "print(f\"Should be close to: {np.log1p(tabular_df.iloc[0][target_cols].values.astype(np.float32))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42f322e7",
   "metadata": {},
   "source": [
    "## Spliting Data (StratifiedKFold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "29b90a93",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_season(date_str: str) -> str:\n",
    "    \"\"\"\n",
    "    Convert date string to season.\n",
    "\n",
    "    Args:\n",
    "        date_str: Date in format 'YYYY/M/D' or 'YYYY/MM/DD'\n",
    "\n",
    "    Returns:\n",
    "        Season name: 'Summer', 'Autumn', 'Winter', 'Spring'\n",
    "    \"\"\"\n",
    "    # Parse month from date string\n",
    "    month = int(date_str.split('/')[1])\n",
    "\n",
    "    # Australian seasons (Southern Hemisphere)\n",
    "    if month in [12, 1, 2]:\n",
    "        return 'Summer'\n",
    "    elif month in [3, 4, 5]:\n",
    "        return 'Autumn'\n",
    "    elif month in [6, 7, 8]:\n",
    "        return 'Winter'\n",
    "    else:  # 9, 10, 11\n",
    "        return 'Spring'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d5e023e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique stratification groups:\n",
      "strat_group\n",
      "Spring_Tas_Ryegrass_Clover                                                41\n",
      "Winter_Vic_Phalaris_Clover                                                32\n",
      "Autumn_Tas_Ryegrass                                                       22\n",
      "Winter_Vic_Ryegrass_Clover                                                21\n",
      "Spring_Tas_Clover                                                         21\n",
      "Winter_WA_Clover                                                          20\n",
      "Winter_Tas_Ryegrass_Clover                                                18\n",
      "Autumn_NSW_Lucerne                                                        15\n",
      "Spring_Vic_Ryegrass_Clover                                                11\n",
      "Spring_Vic_Phalaris_BarleyGrass_SilverGrass_SpearGrass_Clover_Capeweed    11\n",
      "Spring_NSW_Fescue                                                         11\n",
      "Summer_NSW_Fescue_CrumbWeed                                               10\n",
      "Spring_Vic_Phalaris_Clover                                                10\n",
      "Winter_Tas_WhiteClover                                                    10\n",
      "Winter_Vic_Ryegrass                                                       10\n",
      "Winter_Tas_Ryegrass                                                       10\n",
      "Spring_Tas_Ryegrass                                                        9\n",
      "Summer_NSW_Fescue                                                          9\n",
      "Summer_NSW_Phalaris                                                        8\n",
      "Autumn_NSW_Fescue                                                          8\n",
      "Winter_Vic_Phalaris_Ryegrass_Clover                                        8\n",
      "Summer_NSW_Lucerne                                                         7\n",
      "Spring_Vic_Phalaris_Clover_Ryegrass_Barleygrass_Bromegrass                 7\n",
      "Autumn_Tas_Ryegrass_Clover                                                 7\n",
      "Summer_NSW_Ryegrass                                                        7\n",
      "Spring_WA_SubcloverLosa                                                    5\n",
      "Spring_WA_Ryegrass                                                         4\n",
      "Spring_WA_SubcloverDalkeith                                                3\n",
      "Winter_Vic_Mixed                                                           2\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Total groups: 29\n"
     ]
    }
   ],
   "source": [
    "# Add season column\n",
    "tabular_df['Season'] = tabular_df['Sampling_Date'].apply(get_season)\n",
    "\n",
    "# Create stratification column combining Season, State, and Species\n",
    "tabular_df['strat_group'] = (\n",
    "    tabular_df['Season'].astype(str) + '_' +\n",
    "    tabular_df['State'].astype(str) + '_' +\n",
    "    tabular_df['Species'].astype(str)\n",
    ")\n",
    "\n",
    "print(\"Unique stratification groups:\")\n",
    "print(tabular_df['strat_group'].value_counts())\n",
    "print(f\"\\nTotal groups: {tabular_df['strat_group'].nunique()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "7d5f0fd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Fold 1:\n",
      "  Train samples: 285\n",
      "  Val samples: 72\n",
      "\n",
      "Fold 2:\n",
      "  Train samples: 285\n",
      "  Val samples: 72\n",
      "\n",
      "Fold 3:\n",
      "  Train samples: 286\n",
      "  Val samples: 71\n",
      "\n",
      "Fold 4:\n",
      "  Train samples: 286\n",
      "  Val samples: 71\n",
      "\n",
      "Fold 5:\n",
      "  Train samples: 286\n",
      "  Val samples: 71\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\_GitHub\\CSIRO-Image2Biomass-Prediction\\.venv\\Lib\\site-packages\\sklearn\\model_selection\\_split.py:813: UserWarning: The least populated class in y has only 2 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Initialize StratifiedKFold\n",
    "n_folds = 5\n",
    "skf = StratifiedKFold(n_splits=n_folds, shuffle=True, random_state=42)\n",
    "\n",
    "# Get stratification labels\n",
    "strat_labels = tabular_df['strat_group'].values\n",
    "\n",
    "# Create fold assignments\n",
    "tabular_df['fold'] = -1\n",
    "\n",
    "for fold_idx, (train_idx, val_idx) in enumerate(skf.split(tabular_df, strat_labels)):\n",
    "    tabular_df.loc[val_idx, 'fold'] = fold_idx\n",
    "\n",
    "    print(f\"\\nFold {fold_idx + 1}:\")\n",
    "    print(f\"  Train samples: {len(train_idx)}\")\n",
    "    print(f\"  Val samples: {len(val_idx)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "20130d85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stratification verification:\n",
      "\n",
      "Fold 1:\n",
      "  Season distribution:\n",
      "Season\n",
      "Winter    0.361\n",
      "Spring    0.347\n",
      "Autumn    0.167\n",
      "Summer    0.125\n",
      "Name: proportion, dtype: float64\n",
      "  State distribution:\n",
      "State\n",
      "Tas    0.389\n",
      "Vic    0.319\n",
      "NSW    0.222\n",
      "WA     0.069\n",
      "Name: proportion, dtype: float64\n",
      "  Species distribution:\n",
      "Species\n",
      "Ryegrass_Clover                                                0.292\n",
      "Ryegrass                                                       0.167\n",
      "Phalaris_Clover                                                0.111\n",
      "Clover                                                         0.111\n",
      "Fescue                                                         0.083\n",
      "Lucerne                                                        0.056\n",
      "Fescue_CrumbWeed                                               0.028\n",
      "Phalaris                                                       0.028\n",
      "WhiteClover                                                    0.028\n",
      "Phalaris_Clover_Ryegrass_Barleygrass_Bromegrass                0.028\n",
      "Phalaris_BarleyGrass_SilverGrass_SpearGrass_Clover_Capeweed    0.028\n",
      "SubcloverLosa                                                  0.014\n",
      "Phalaris_Ryegrass_Clover                                       0.014\n",
      "Mixed                                                          0.014\n",
      "Name: proportion, dtype: float64\n",
      "\n",
      "Fold 2:\n",
      "  Season distribution:\n",
      "Season\n",
      "Spring    0.389\n",
      "Winter    0.375\n",
      "Autumn    0.125\n",
      "Summer    0.111\n",
      "Name: proportion, dtype: float64\n",
      "  State distribution:\n",
      "State\n",
      "Tas    0.375\n",
      "Vic    0.333\n",
      "NSW    0.194\n",
      "WA     0.097\n",
      "Name: proportion, dtype: float64\n",
      "  Species distribution:\n",
      "Species\n",
      "Ryegrass_Clover                                                0.278\n",
      "Ryegrass                                                       0.167\n",
      "Phalaris_Clover                                                0.125\n",
      "Clover                                                         0.111\n",
      "Fescue                                                         0.069\n",
      "Lucerne                                                        0.056\n",
      "Phalaris_BarleyGrass_SilverGrass_SpearGrass_Clover_Capeweed    0.042\n",
      "Fescue_CrumbWeed                                               0.028\n",
      "Phalaris                                                       0.028\n",
      "WhiteClover                                                    0.028\n",
      "SubcloverDalkeith                                              0.014\n",
      "Phalaris_Clover_Ryegrass_Barleygrass_Bromegrass                0.014\n",
      "Phalaris_Ryegrass_Clover                                       0.014\n",
      "Mixed                                                          0.014\n",
      "SubcloverLosa                                                  0.014\n",
      "Name: proportion, dtype: float64\n",
      "\n",
      "Fold 3:\n",
      "  Season distribution:\n",
      "Season\n",
      "Spring    0.380\n",
      "Winter    0.380\n",
      "Autumn    0.127\n",
      "Summer    0.113\n",
      "Name: proportion, dtype: float64\n",
      "  State distribution:\n",
      "State\n",
      "Tas    0.380\n",
      "Vic    0.310\n",
      "NSW    0.211\n",
      "WA     0.099\n",
      "Name: proportion, dtype: float64\n",
      "  Species distribution:\n",
      "Species\n",
      "Ryegrass_Clover                                                0.268\n",
      "Ryegrass                                                       0.169\n",
      "Phalaris_Clover                                                0.127\n",
      "Clover                                                         0.113\n",
      "Fescue                                                         0.085\n",
      "Lucerne                                                        0.056\n",
      "WhiteClover                                                    0.028\n",
      "Phalaris_BarleyGrass_SilverGrass_SpearGrass_Clover_Capeweed    0.028\n",
      "Phalaris                                                       0.028\n",
      "Fescue_CrumbWeed                                               0.028\n",
      "Phalaris_Ryegrass_Clover                                       0.028\n",
      "Phalaris_Clover_Ryegrass_Barleygrass_Bromegrass                0.014\n",
      "SubcloverDalkeith                                              0.014\n",
      "SubcloverLosa                                                  0.014\n",
      "Name: proportion, dtype: float64\n",
      "\n",
      "Fold 4:\n",
      "  Season distribution:\n",
      "Season\n",
      "Spring    0.380\n",
      "Winter    0.366\n",
      "Autumn    0.141\n",
      "Summer    0.113\n",
      "Name: proportion, dtype: float64\n",
      "  State distribution:\n",
      "State\n",
      "Tas    0.394\n",
      "Vic    0.296\n",
      "NSW    0.211\n",
      "WA     0.099\n",
      "Name: proportion, dtype: float64\n",
      "  Species distribution:\n",
      "Species\n",
      "Ryegrass_Clover                                                0.268\n",
      "Ryegrass                                                       0.169\n",
      "Clover                                                         0.127\n",
      "Phalaris_Clover                                                0.113\n",
      "Fescue                                                         0.085\n",
      "Lucerne                                                        0.070\n",
      "Phalaris_Ryegrass_Clover                                       0.028\n",
      "WhiteClover                                                    0.028\n",
      "Fescue_CrumbWeed                                               0.028\n",
      "Phalaris_BarleyGrass_SilverGrass_SpearGrass_Clover_Capeweed    0.028\n",
      "Phalaris                                                       0.014\n",
      "Phalaris_Clover_Ryegrass_Barleygrass_Bromegrass                0.014\n",
      "SubcloverLosa                                                  0.014\n",
      "SubcloverDalkeith                                              0.014\n",
      "Name: proportion, dtype: float64\n",
      "\n",
      "Fold 5:\n",
      "  Season distribution:\n",
      "Season\n",
      "Spring    0.366\n",
      "Winter    0.352\n",
      "Autumn    0.169\n",
      "Summer    0.113\n",
      "Name: proportion, dtype: float64\n",
      "  State distribution:\n",
      "State\n",
      "Tas    0.394\n",
      "Vic    0.310\n",
      "NSW    0.211\n",
      "WA     0.085\n",
      "Name: proportion, dtype: float64\n",
      "  Species distribution:\n",
      "Species\n",
      "Ryegrass_Clover                                                0.268\n",
      "Ryegrass                                                       0.197\n",
      "Clover                                                         0.113\n",
      "Phalaris_Clover                                                0.113\n",
      "Fescue                                                         0.070\n",
      "Lucerne                                                        0.070\n",
      "WhiteClover                                                    0.028\n",
      "Phalaris_Ryegrass_Clover                                       0.028\n",
      "Fescue_CrumbWeed                                               0.028\n",
      "Phalaris_Clover_Ryegrass_Barleygrass_Bromegrass                0.028\n",
      "Phalaris_BarleyGrass_SilverGrass_SpearGrass_Clover_Capeweed    0.028\n",
      "Phalaris                                                       0.014\n",
      "SubcloverLosa                                                  0.014\n",
      "Name: proportion, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Verify stratification worked\n",
    "print(\"Stratification verification:\")\n",
    "\n",
    "for fold in range(n_folds):\n",
    "    fold_df = tabular_df[tabular_df['fold'] == fold]\n",
    "    print(f\"\\nFold {fold + 1}:\")\n",
    "    print(f\"  Season distribution:\")\n",
    "    print(fold_df['Season'].value_counts(normalize=True).round(3))\n",
    "    print(f\"  State distribution:\")\n",
    "    print(fold_df['State'].value_counts(normalize=True).round(3))\n",
    "    print(f\"  Species distribution:\")\n",
    "    print(fold_df['Species'].value_counts(normalize=True).round(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "7cea12fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_fold_data(df: pd.DataFrame, fold: int):\n",
    "    \"\"\"\n",
    "    Get train/val split for specific fold.\n",
    "\n",
    "    Args:\n",
    "        df: DataFrame with 'fold' column\n",
    "        fold: Fold index to use as validation\n",
    "\n",
    "    Returns:\n",
    "        train_df, val_df\n",
    "    \"\"\"\n",
    "    train_df = df[df['fold'] != fold].reset_index(drop=True)\n",
    "    val_df = df[df['fold'] == fold].reset_index(drop=True)\n",
    "\n",
    "    return train_df, val_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25e99df9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_loaders(fold: int, bs: int, soft_df: pd.DataFrame) -> tuple[DataLoader, DataLoader]:\n",
    "    \"\"\"\n",
    "    Get dataloaders for Student model (no tabular features needed!)\n",
    "    \n",
    "    Args:\n",
    "        fold: Fold index to use as validation\n",
    "        bs: Batch size\n",
    "        soft_df: DataFrame with soft targets from Teacher\n",
    "    \"\"\"\n",
    "    train_df = soft_df[soft_df['fold'] != fold].reset_index(drop=True)\n",
    "    val_df = soft_df[soft_df['fold'] == fold].reset_index(drop=True)\n",
    "\n",
    "    logger.info(f\"Student training fold {fold}:\")\n",
    "    logger.info(f\"  Train size: {len(train_df)}\")\n",
    "    logger.info(f\"  Val size: {len(val_df)}\")\n",
    "\n",
    "    # Soft target columns\n",
    "    soft_target_cols = ['Dry_Clover_g_soft', 'Dry_Dead_g_soft', 'Dry_Green_g_soft']\n",
    "\n",
    "    # Create datasets\n",
    "    train_dataset = BiomassStudentDataset(\n",
    "        df=train_df,\n",
    "        target_cols=target_cols,\n",
    "        soft_target_cols=soft_target_cols,\n",
    "        img_dir=PATH_TRAIN_IMG,\n",
    "        transform=student_train_transform,\n",
    "        is_test=False,\n",
    "        use_log_target=USE_LOG_TARGET\n",
    "    )\n",
    "\n",
    "    val_dataset = BiomassStudentDataset(\n",
    "        df=val_df,\n",
    "        target_cols=target_cols,\n",
    "        soft_target_cols=soft_target_cols,\n",
    "        img_dir=PATH_TRAIN_IMG,\n",
    "        transform=student_val_transform,\n",
    "        is_test=False,\n",
    "        use_log_target=USE_LOG_TARGET\n",
    "    )\n",
    "\n",
    "    # Create dataloaders\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=bs,\n",
    "        shuffle=True,\n",
    "        num_workers=min(NUM_WORKERS, 8),\n",
    "        pin_memory=True if torch.cuda.is_available() else False,\n",
    "        persistent_workers=True if NUM_WORKERS > 0 else False\n",
    "    )\n",
    "\n",
    "    val_loader = DataLoader(\n",
    "        val_dataset,\n",
    "        batch_size=bs * 2,\n",
    "        shuffle=False,\n",
    "        num_workers=min(NUM_WORKERS, 8),\n",
    "        pin_memory=True if torch.cuda.is_available() else False,\n",
    "        persistent_workers=True if NUM_WORKERS > 0 else False\n",
    "    )\n",
    "\n",
    "    logger.info(f\"Train batches: {len(train_loader)}\")\n",
    "    logger.info(f\"Val batches: {len(val_loader)}\")\n",
    "\n",
    "    return train_loader, val_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9205264a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test student dataset\n",
    "logger.info(\"Testing Student Dataset...\")\n",
    "\n",
    "test_train_loader, test_val_loader = get_student_loaders(fold=0, bs=BATCH_SIZE, soft_df=soft_targets_df)\n",
    "\n",
    "sample_batch = next(iter(test_train_loader))\n",
    "\n",
    "print(f\"Left image shape: {sample_batch['left_image'].shape}\")\n",
    "print(f\"Right image shape: {sample_batch['right_image'].shape}\")\n",
    "print(f\"Hard targets shape: {sample_batch['hard_targets'].shape}\")\n",
    "print(f\"Soft targets shape: {sample_batch['soft_targets'].shape}\")\n",
    "\n",
    "logger.success(\"Student dataset works perfectly!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53fd54d9",
   "metadata": {},
   "source": [
    "## Ligtning Module"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b284bcb6",
   "metadata": {},
   "source": [
    "### Custom Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "0f7a48a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = [\n",
    "    \"Dry_Clover_g\",\n",
    "    \"Dry_Dead_g\",\n",
    "    \"Dry_Green_g\",\n",
    "    \"Dry_Total_g\",\n",
    "    \"GDM_g\"\n",
    "]\n",
    "\n",
    "weights = {\n",
    "    'Dry_Green_g': 0.1,\n",
    "    'Dry_Dead_g': 0.1,\n",
    "    'Dry_Clover_g': 0.1,\n",
    "    'GDM_g': 0.2,\n",
    "    'Dry_Total_g': 0.5,\n",
    "}\n",
    "\n",
    "\n",
    "def competition_metric(y_true, y_pred) -> float:\n",
    "    \"\"\"Function to calculate the competition's official evaluation metric (weighted R2 score).\"\"\"\n",
    "    weights_array = np.array([weights[l] for l in labels])\n",
    "\n",
    "    # Align with this calculation method\n",
    "    y_weighted_mean = np.average(y_true, weights=weights_array, axis=1).mean()\n",
    "\n",
    "    # For ss_res and ss_tot, also take the weighted average on axis=1, then the mean of the result\n",
    "    ss_res = np.average((y_true - y_pred)**2,\n",
    "                        weights=weights_array, axis=1).mean()\n",
    "    ss_tot = np.average((y_true - y_weighted_mean)**2,\n",
    "                        weights=weights_array, axis=1).mean()\n",
    "\n",
    "    return 1 - ss_res / ss_tot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e5a9bc56",
   "metadata": {},
   "outputs": [],
   "source": [
    "class StudentDistillationLoss(nn.Module):\n",
    "    \"\"\"\n",
    "    Custom loss for Student model combining:\n",
    "    1. Distillation loss (learn from Teacher)\n",
    "    2. Hard loss (learn from real targets with competition weights)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, alpha: float = 0.5, use_log_space: bool = True):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            alpha: Weight for distillation loss (0.5 = equal weight to Teacher and ground truth)\n",
    "            use_log_space: If True, compute loss in log space\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.alpha = alpha\n",
    "        self.use_log_space = use_log_space\n",
    "        \n",
    "        # Competition weights\n",
    "        self.w_green = 0.1\n",
    "        self.w_clover = 0.1\n",
    "        self.w_dead = 0.1\n",
    "        self.w_gdm = 0.2\n",
    "        self.w_total = 0.5\n",
    "        \n",
    "    def forward(\n",
    "        self, \n",
    "        student_preds: torch.Tensor,  # [B, 3] predictions in log space\n",
    "        hard_targets: torch.Tensor,   # [B, 3] ground truth in log space\n",
    "        soft_targets: torch.Tensor    # [B, 3] Teacher predictions in log space\n",
    "    ) -> tuple[torch.Tensor, dict]:\n",
    "        \"\"\"\n",
    "        Returns:\n",
    "            total_loss: Combined loss\n",
    "            loss_dict: Dictionary with individual loss components\n",
    "        \"\"\"\n",
    "        \n",
    "        # 1. Distillation Loss (MSE with Teacher's soft targets)\n",
    "        loss_distill = F.mse_loss(student_preds, soft_targets)\n",
    "        \n",
    "        # 2. Hard Loss with competition weights\n",
    "        # Individual components\n",
    "        loss_green = F.mse_loss(student_preds[:, 2], hard_targets[:, 2])   # Dry_Green_g\n",
    "        loss_clover = F.mse_loss(student_preds[:, 0], hard_targets[:, 0])  # Dry_Clover_g\n",
    "        loss_dead = F.mse_loss(student_preds[:, 1], hard_targets[:, 1])    # Dry_Dead_g\n",
    "        \n",
    "        # Derived targets (computed from components)\n",
    "        # Dry_Total_g = sum of all 3 components\n",
    "        student_total = student_preds.sum(dim=1)\n",
    "        hard_total = hard_targets.sum(dim=1)\n",
    "        loss_total = F.mse_loss(student_total, hard_total)\n",
    "        \n",
    "        # GDM_g = Clover + Green\n",
    "        student_gdm = student_preds[:, 0] + student_preds[:, 2]  # Clover + Green\n",
    "        hard_gdm = hard_targets[:, 0] + hard_targets[:, 2]\n",
    "        loss_gdm = F.mse_loss(student_gdm, hard_gdm)\n",
    "        \n",
    "        # Weighted hard loss (following competition metric weights)\n",
    "        loss_hard = (\n",
    "            self.w_green * loss_green +\n",
    "            self.w_clover * loss_clover +\n",
    "            self.w_dead * loss_dead +\n",
    "            self.w_gdm * loss_gdm +\n",
    "            self.w_total * loss_total\n",
    "        )\n",
    "        \n",
    "        # 3. Total loss (weighted combination)\n",
    "        total_loss = self.alpha * loss_distill + (1 - self.alpha) * loss_hard\n",
    "        \n",
    "        # Return loss dict for logging\n",
    "        loss_dict = {\n",
    "            'loss_distill': loss_distill.item(),\n",
    "            'loss_hard': loss_hard.item(),\n",
    "            'loss_green': loss_green.item(),\n",
    "            'loss_clover': loss_clover.item(),\n",
    "            'loss_dead': loss_dead.item(),\n",
    "            'loss_total': loss_total.item(),\n",
    "            'loss_gdm': loss_gdm.item(),\n",
    "        }\n",
    "        \n",
    "        return total_loss, loss_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e8b2e58",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BiomassStudentModel(pl.LightningModule):\n",
    "    \"\"\"\n",
    "    Student model for biomass prediction.\n",
    "    Uses ONLY images (dual-patch), NO tabular features.\n",
    "    Learns from Teacher's soft targets + ground truth.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        backbone_name: str = 'swinv2_tiny_window8_256',\n",
    "        num_targets: int = 3,\n",
    "        lr: float = 1e-4,\n",
    "        weight_decay: float = 1e-5,\n",
    "        hidden_ratio: float = 0.5,\n",
    "        dropout: float = 0.3,  # Higher dropout for student\n",
    "        fusion_method: str = 'mean',\n",
    "        distill_alpha: float = 0.5,  # Weight for distillation loss\n",
    "        use_log_target: bool = True\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "\n",
    "        # Image backbone (same as Teacher)\n",
    "        self.backbone = timm.create_model(\n",
    "            backbone_name,\n",
    "            pretrained=True,\n",
    "            num_classes=0,\n",
    "            global_pool='avg'\n",
    "        )\n",
    "\n",
    "        self.lr = lr\n",
    "        self.weight_decay = weight_decay\n",
    "        self.fusion_method = fusion_method\n",
    "        self.use_log_target = use_log_target\n",
    "\n",
    "        # Get backbone output dimension\n",
    "        with torch.no_grad():\n",
    "            dummy = torch.randn(1, 3, SIZE, SIZE)\n",
    "            feat_dim = self.backbone(dummy).shape[1]\n",
    "\n",
    "        self.feat_dim = feat_dim\n",
    "\n",
    "        # NO tabular features - only image features!\n",
    "        if self.fusion_method == 'concat':\n",
    "            self.combined_dim = feat_dim * 2\n",
    "        else:  # mean or max\n",
    "            self.combined_dim = feat_dim\n",
    "\n",
    "        # Regression heads (simpler than Teacher)\n",
    "        hidden_size = max(32, int(self.combined_dim * hidden_ratio))\n",
    "\n",
    "        def make_head():\n",
    "            return nn.Sequential(\n",
    "                nn.Linear(self.combined_dim, hidden_size),\n",
    "                nn.ReLU(inplace=True),\n",
    "                nn.Dropout(dropout),\n",
    "                nn.Linear(hidden_size, 1)\n",
    "            )\n",
    "\n",
    "        self.head_green = make_head()\n",
    "        self.head_clover = make_head()\n",
    "        self.head_dead = make_head()\n",
    "\n",
    "        # Custom distillation loss\n",
    "        self.criterion = StudentDistillationLoss(\n",
    "            alpha=distill_alpha,\n",
    "            use_log_space=use_log_target\n",
    "        )\n",
    "\n",
    "        # Storage for validation\n",
    "        self.validation_step_outputs = []\n",
    "\n",
    "        logger.info(f\"Student model initialized: backbone={backbone_name}, feat_dim={feat_dim}, \"\n",
    "                    f\"combined_dim={self.combined_dim}, fusion={fusion_method}, \"\n",
    "                    f\"distill_alpha={distill_alpha}\")\n",
    "\n",
    "    def forward(self, batch: dict) -> tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            batch: dict with 'left_image', 'right_image'\n",
    "\n",
    "        Returns:\n",
    "            (green, clover, dead) predictions\n",
    "        \"\"\"\n",
    "        # Extract features from each patch\n",
    "        left_feat = self.backbone(batch['left_image'])\n",
    "        right_feat = self.backbone(batch['right_image'])\n",
    "\n",
    "        # Fuse image features\n",
    "        if self.fusion_method == 'concat':\n",
    "            img_feat = torch.cat([left_feat, right_feat], dim=1)\n",
    "        elif self.fusion_method == 'mean':\n",
    "            img_feat = (left_feat + right_feat) / 2\n",
    "        elif self.fusion_method == 'max':\n",
    "            img_feat = torch.maximum(left_feat, right_feat)\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown fusion method: {self.fusion_method}\")\n",
    "\n",
    "        # Predict each target\n",
    "        green = self.head_green(img_feat).squeeze(1)\n",
    "        clover = self.head_clover(img_feat).squeeze(1)\n",
    "        dead = self.head_dead(img_feat).squeeze(1)\n",
    "\n",
    "        return green, clover, dead\n",
    "\n",
    "    def compute_all_targets(self, green: torch.Tensor, clover: torch.Tensor, dead: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Compute all 5 targets from 3 predicted ones\"\"\"\n",
    "        green = torch.clamp(green, min=0.0)\n",
    "        clover = torch.clamp(clover, min=0.0)\n",
    "        dead = torch.clamp(dead, min=0.0)\n",
    "\n",
    "        total = green + dead + clover\n",
    "        gdm = clover + green\n",
    "\n",
    "        all_targets = torch.stack([clover, dead, green, total, gdm], dim=1)\n",
    "        return all_targets\n",
    "\n",
    "    def training_step(self, batch: dict, batch_idx: int) -> torch.Tensor:\n",
    "        green, clover, dead = self(batch)\n",
    "        \n",
    "        # Stack predictions [B, 3] in order: [clover, dead, green]\n",
    "        preds = torch.stack([clover, dead, green], dim=1)\n",
    "        \n",
    "        # Compute distillation loss\n",
    "        loss, loss_dict = self.criterion(\n",
    "            preds,\n",
    "            batch['hard_targets'],\n",
    "            batch['soft_targets']\n",
    "        )\n",
    "\n",
    "        # Log all loss components\n",
    "        self.log('train_loss', loss, on_step=True, on_epoch=True, prog_bar=True,\n",
    "                 batch_size=batch['hard_targets'].size(0))\n",
    "        self.log('train_loss_distill', loss_dict['loss_distill'], on_step=False, on_epoch=True)\n",
    "        self.log('train_loss_hard', loss_dict['loss_hard'], on_step=False, on_epoch=True)\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch: dict, batch_idx: int) -> torch.Tensor:\n",
    "        green_pred, clover_pred, dead_pred = self(batch)\n",
    "        \n",
    "        preds = torch.stack([clover_pred, dead_pred, green_pred], dim=1)\n",
    "        \n",
    "        # Compute loss\n",
    "        loss, loss_dict = self.criterion(\n",
    "            preds,\n",
    "            batch['hard_targets'],\n",
    "            batch['soft_targets']\n",
    "        )\n",
    "\n",
    "        self.log('val_loss', loss, on_step=False, on_epoch=True, prog_bar=True,\n",
    "                 batch_size=batch['hard_targets'].size(0))\n",
    "        self.log('val_loss_distill', loss_dict['loss_distill'], on_step=False, on_epoch=True)\n",
    "        self.log('val_loss_hard', loss_dict['loss_hard'], on_step=False, on_epoch=True)\n",
    "\n",
    "        # Convert to original scale for metric\n",
    "        if self.use_log_target:\n",
    "            green_pred = torch.expm1(green_pred)\n",
    "            clover_pred = torch.expm1(clover_pred)\n",
    "            dead_pred = torch.expm1(dead_pred)\n",
    "            \n",
    "            hard_targets_original = torch.expm1(batch['hard_targets'])\n",
    "        else:\n",
    "            hard_targets_original = batch['hard_targets']\n",
    "\n",
    "        # Compute all 5 targets\n",
    "        preds_all = self.compute_all_targets(green_pred, clover_pred, dead_pred)\n",
    "        \n",
    "        clover_true = hard_targets_original[:, 0]\n",
    "        dead_true = hard_targets_original[:, 1]\n",
    "        green_true = hard_targets_original[:, 2]\n",
    "        targets_all = self.compute_all_targets(green_true, clover_true, dead_true)\n",
    "\n",
    "        self.validation_step_outputs.append({\n",
    "            'preds': preds_all.detach().cpu(),\n",
    "            'targets': targets_all.detach().cpu()\n",
    "        })\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def on_validation_epoch_end(self):\n",
    "        if len(self.validation_step_outputs) == 0:\n",
    "            return\n",
    "\n",
    "        all_preds = torch.cat([x['preds'] for x in self.validation_step_outputs], dim=0).numpy()\n",
    "        all_targets = torch.cat([x['targets'] for x in self.validation_step_outputs], dim=0).numpy()\n",
    "\n",
    "        comp_metric = competition_metric(all_targets, all_preds)\n",
    "        self.log('val_comp_metric', comp_metric, on_epoch=True, prog_bar=True)\n",
    "\n",
    "        self.validation_step_outputs.clear()\n",
    "\n",
    "    def predict_step(self, batch: dict, batch_idx: int) -> torch.Tensor:\n",
    "        green, clover, dead = self(batch)\n",
    "        preds = torch.stack([clover, dead, green], dim=1)\n",
    "\n",
    "        if self.use_log_target:\n",
    "            preds = torch.expm1(preds)\n",
    "\n",
    "        preds = torch.clamp(preds, min=0.0)\n",
    "        return preds\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = AdamW(\n",
    "            self.parameters(),\n",
    "            lr=self.lr,\n",
    "            weight_decay=self.weight_decay\n",
    "        )\n",
    "\n",
    "        scheduler = CosineAnnealingLR(\n",
    "            optimizer,\n",
    "            T_max=self.trainer.max_epochs or 20,\n",
    "            eta_min=self.lr * 0.01\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            'optimizer': optimizer,\n",
    "            'lr_scheduler': {\n",
    "                'scheduler': scheduler,\n",
    "                'interval': 'epoch'\n",
    "            }\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b85bdfb",
   "metadata": {},
   "source": [
    "## Folds Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd59f4b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "student_fold_results = []\n",
    "\n",
    "for fold_id in range(N_FOLDS):\n",
    "    train_loader, val_loader = get_student_loaders(fold=fold_id, bs=BATCH_SIZE, soft_df=soft_targets_df)\n",
    "\n",
    "    model = BiomassStudentModel(\n",
    "        backbone_name=MODEL,\n",
    "        num_targets=len(target_cols),\n",
    "        lr=STUDENT_LR,\n",
    "        weight_decay=WEIGHT_DECAY,\n",
    "        hidden_ratio=HIDDEN_RATIO,\n",
    "        dropout=STUDENT_DROPOUT,\n",
    "        fusion_method=FUSION_METHOD,\n",
    "        distill_alpha=DISTILL_ALPHA,\n",
    "        use_log_target=USE_LOG_TARGET\n",
    "    )\n",
    "\n",
    "    # Callbacks\n",
    "    checkpoint_callback = ModelCheckpoint(\n",
    "        monitor='val_loss',\n",
    "        dirpath=os.path.join(CHECKPOINTS_DIR, f'fold{fold_id}'),\n",
    "        filename=f'{DESCRIPTION_FULL}-student-fold{fold_id}' + '-{epoch:02d}-{val_loss:.4f}',\n",
    "        save_top_k=3,\n",
    "        mode='min'\n",
    "    )\n",
    "\n",
    "    early_stopping_callback = EarlyStopping(\n",
    "        monitor='val_loss',\n",
    "        patience=7,  # More patience for student\n",
    "        mode='min',\n",
    "        verbose=True\n",
    "    )\n",
    "\n",
    "    lr_monitor = LearningRateMonitor(logging_interval='epoch')\n",
    "\n",
    "    # Logger\n",
    "    wandb_logger = WandbLogger(\n",
    "        project=PROJECT_NAME,\n",
    "        name=f'{DESCRIPTION_FULL}-student-fold{fold_id}',\n",
    "        log_model='all'\n",
    "    )\n",
    "\n",
    "    # Trainer\n",
    "    trainer = pl.Trainer(\n",
    "        max_epochs=STUDENT_EPOCHS,\n",
    "        accelerator=DEVICE.type,\n",
    "        precision='16-mixed' if torch.cuda.is_available() else 32,\n",
    "        accumulate_grad_batches=GRAD_ACCUM,\n",
    "        callbacks=[checkpoint_callback, early_stopping_callback, lr_monitor],\n",
    "        logger=wandb_logger,\n",
    "        log_every_n_steps=1,\n",
    "        gradient_clip_val=1.0,\n",
    "        enable_progress_bar=True\n",
    "    )\n",
    "\n",
    "    try:\n",
    "        # Train\n",
    "        logger.info(f\"\\nTraining Student on Fold {fold_id}...\")\n",
    "        trainer.fit(model, train_loader, val_loader)\n",
    "\n",
    "        # Load best checkpoint\n",
    "        best_model_path = checkpoint_callback.best_model_path\n",
    "        logger.info(f\"Loading best Student model from: {best_model_path}\")\n",
    "        best_model = BiomassStudentModel.load_from_checkpoint(best_model_path)\n",
    "\n",
    "        # Evaluate\n",
    "        val_result = trainer.validate(best_model, val_loader, verbose=False)\n",
    "        student_fold_results.append({\n",
    "            'fold': fold_id,\n",
    "            'val_loss': val_result[0]['val_loss'],\n",
    "            'val_comp_metric': val_result[0].get('val_comp_metric', 0.0)\n",
    "        })\n",
    "        \n",
    "        logger.success(f\"Fold {fold_id} completed: val_loss={val_result[0]['val_loss']:.4f}\")\n",
    "\n",
    "    except SystemExit:\n",
    "        logger.warning(f\"Training interrupted during fold {fold_id}. Exiting gracefully.\")\n",
    "        wandb_logger.experiment.finish()\n",
    "        break\n",
    "\n",
    "    finally:\n",
    "        wandb_logger.experiment.finish()\n",
    "\n",
    "logger.success(\"STUDENT TRAINING COMPLETED\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f4c9c8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Summary\n",
      "\n",
      "   fold  val_loss\n",
      "0     0  0.893824\n",
      "1     1  0.811320\n",
      "2     2  0.936835\n",
      "3     3  1.147606\n",
      "4     4  1.055121\n",
      "Mean Val Loss: 0.9689 Â± 0.1331\n"
     ]
    }
   ],
   "source": [
    "print(\"Training Summary\")\n",
    "print()\n",
    "results_df = pd.DataFrame(student_fold_results)\n",
    "print(results_df)\n",
    "print(f\"Mean Val Loss: {results_df['val_loss'].mean():.4f} Â± {results_df['val_loss'].std():.4f}\")\n",
    "print(f\"Mean Comp Metric: {results_df['val_comp_metric'].mean():.4f} Â± {results_df['val_comp_metric'].std():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc5d4c96",
   "metadata": {},
   "source": [
    "## Prepare Data for Student"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "bb0de811",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_teacher_models(checkpoints_dir: str, n_folds: int) -> list:\n",
    "    \"\"\"\n",
    "    Load best checkpoints from all folds.\n",
    "    \n",
    "    Args:\n",
    "        checkpoints_dir: Directory with fold checkpoints\n",
    "        n_folds: Number of folds\n",
    "    \n",
    "    Returns:\n",
    "        List of loaded models\n",
    "    \"\"\"\n",
    "    models = []\n",
    "    \n",
    "    for fold_id in range(n_folds):\n",
    "        fold_dir = os.path.join(checkpoints_dir, f'fold{fold_id}')\n",
    "        \n",
    "        # Find best checkpoint file\n",
    "        ckpt_files = [f for f in os.listdir(fold_dir) if f.endswith('.ckpt')]\n",
    "        if not ckpt_files:\n",
    "            logger.warning(f\"No checkpoint found in {fold_dir}\")\n",
    "            continue\n",
    "\n",
    "        # sort by val_loss in filename (val_loss=xxx.ckpt)\n",
    "        ckpt_files.sort(key=lambda x: float(x.split('=')[-1].replace('.ckpt', '')))\n",
    "        \n",
    "        # Assuming the first one is the best\n",
    "        ckpt_path = os.path.join(fold_dir, ckpt_files[0])\n",
    "        logger.info(f\"Loading checkpoint: {ckpt_path}\")\n",
    "        \n",
    "        model = BiomassTeacherModel.load_from_checkpoint(ckpt_path)\n",
    "        model.eval()\n",
    "        model = model.to(DEVICE)\n",
    "        models.append(model)\n",
    "    \n",
    "    logger.success(f\"Loaded {len(models)} teacher models\")\n",
    "    return models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "da56e402",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;2;161;247;255m\n",
      "[2025-12-11 14:17:08]\n",
      "INFO: Loading checkpoint: ./kaggle/checkpoints/teacher/fold0\\swinv2_tiny_window8_256-local_train[5]Folds_log_fusion-mean_epochs20_bs16_gradacc1_lr0.0001_wd0.05_dr0.2_hr0.5-fold0-epoch=13-val_loss=0.8938.ckpt\u001b[0m\n",
      "\u001b[38;2;161;247;255m\n",
      "[2025-12-11 14:17:09]\n",
      "INFO: Model initialized: backbone=swinv2_tiny_window8_256, feat_dim=768, combined_dim=789, fusion=mean, use_log_target=True\u001b[0m\n",
      "\u001b[38;2;161;247;255m\n",
      "[2025-12-11 14:17:09]\n",
      "INFO: Loading checkpoint: ./kaggle/checkpoints/teacher/fold1\\swinv2_tiny_window8_256-local_train[5]Folds_log_fusion-mean_epochs20_bs16_gradacc1_lr0.0001_wd0.05_dr0.2_hr0.5-fold1-epoch=11-val_loss=0.8113.ckpt\u001b[0m\n",
      "\u001b[38;2;161;247;255m\n",
      "[2025-12-11 14:17:10]\n",
      "INFO: Model initialized: backbone=swinv2_tiny_window8_256, feat_dim=768, combined_dim=789, fusion=mean, use_log_target=True\u001b[0m\n",
      "\u001b[38;2;161;247;255m\n",
      "[2025-12-11 14:17:10]\n",
      "INFO: Loading checkpoint: ./kaggle/checkpoints/teacher/fold2\\swinv2_tiny_window8_256-local_train[5]Folds_log_fusion-mean_epochs20_bs16_gradacc1_lr0.0001_wd0.05_dr0.2_hr0.5-fold2-epoch=19-val_loss=0.9368.ckpt\u001b[0m\n",
      "\u001b[38;2;161;247;255m\n",
      "[2025-12-11 14:17:12]\n",
      "INFO: Model initialized: backbone=swinv2_tiny_window8_256, feat_dim=768, combined_dim=789, fusion=mean, use_log_target=True\u001b[0m\n",
      "\u001b[38;2;161;247;255m\n",
      "[2025-12-11 14:17:12]\n",
      "INFO: Loading checkpoint: ./kaggle/checkpoints/teacher/fold3\\swinv2_tiny_window8_256-local_train[5]Folds_log_fusion-mean_epochs20_bs16_gradacc1_lr0.0001_wd0.05_dr0.2_hr0.5-fold3-epoch=16-val_loss=1.1476.ckpt\u001b[0m\n",
      "\u001b[38;2;161;247;255m\n",
      "[2025-12-11 14:17:13]\n",
      "INFO: Model initialized: backbone=swinv2_tiny_window8_256, feat_dim=768, combined_dim=789, fusion=mean, use_log_target=True\u001b[0m\n",
      "\u001b[38;2;161;247;255m\n",
      "[2025-12-11 14:17:13]\n",
      "INFO: Loading checkpoint: ./kaggle/checkpoints/teacher/fold4\\_swinv2_tiny_window8_256-local_train[5]Folds_log_fusion-mean_epochs20_bs16_gradacc1_lr0.0001_wd0.05_dr0.2_hr0.5-fold4-epoch=19-val_loss=1.0456.ckpt\u001b[0m\n",
      "\u001b[38;2;161;247;255m\n",
      "[2025-12-11 14:17:14]\n",
      "INFO: Model initialized: backbone=swinv2_tiny_window8_256, feat_dim=768, combined_dim=789, fusion=mean, use_log_target=True\u001b[0m\n",
      "\u001b[38;2;105;254;105m\n",
      "[2025-12-11 14:17:14]\n",
      "SUCCESS: Loaded 5 teacher models\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "models = load_teacher_models(CHECKPOINTS_DIR, N_FOLDS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfbd7b4b",
   "metadata": {},
   "source": [
    "### Direct Ensemble Soft Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "510df361",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_soft_targets_dataset():\n",
    "    \"\"\"\n",
    "    Create dataset with soft targets from teacher ensemble.\n",
    "    Predicts on ALL training data (100%).\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame with soft targets appended\n",
    "    \"\"\"\n",
    "    \n",
    "    # Load all teacher models\n",
    "    teacher_models = load_teacher_models(CHECKPOINTS_DIR, N_FOLDS)\n",
    "    \n",
    "    if not teacher_models:\n",
    "        logger.error(\"No teacher models loaded!\")\n",
    "        return None\n",
    "    \n",
    "    logger.info(f\"Creating soft targets using {len(teacher_models)} teacher models...\")\n",
    "    \n",
    "    # Prepare full dataset with NO augmentation\n",
    "    full_dataset = BiomassDataset(\n",
    "        df=tabular_df,\n",
    "        tabular_features=tabular_data,\n",
    "        target_cols=target_cols,\n",
    "        img_dir=PATH_TRAIN_IMG,\n",
    "        transform=val_transform,  # Use validation transform (no augmentation)\n",
    "        is_test=False,\n",
    "        use_log_target=USE_LOG_TARGET\n",
    "    )\n",
    "    \n",
    "    # Create dataloader for inference\n",
    "    full_loader = DataLoader(\n",
    "        full_dataset,\n",
    "        batch_size=BATCH_SIZE * 2,  # Larger batch for inference\n",
    "        shuffle=False,\n",
    "        num_workers=min(NUM_WORKERS, 8),\n",
    "        pin_memory=True if torch.cuda.is_available() else False\n",
    "    )\n",
    "    \n",
    "    logger.info(f\"Total samples for soft targets: {len(full_dataset)}\")\n",
    "    logger.info(f\"Inference batches: {len(full_loader)}\")\n",
    "    \n",
    "    # Store predictions from all models\n",
    "    all_predictions = []\n",
    "    \n",
    "    # For each teacher model\n",
    "    for model_idx, teacher_model in enumerate(teacher_models):\n",
    "        logger.info(f\"Processing teacher model {model_idx + 1}/{len(teacher_models)}...\")\n",
    "        \n",
    "        model_predictions = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch_idx, batch in enumerate(full_loader):\n",
    "                # Move batch to device\n",
    "                batch = {k: v.to(DEVICE) if isinstance(v, torch.Tensor) else v \n",
    "                        for k, v in batch.items()}\n",
    "                \n",
    "                # Get predictions\n",
    "                preds = teacher_model.predict_step(batch, 0)  # [B, 3]\n",
    "                model_predictions.append(preds.cpu().numpy())\n",
    "                \n",
    "                if (batch_idx + 1) % 10 == 0:\n",
    "                    logger.debug(f\"  Batch {batch_idx + 1}/{len(full_loader)}\")\n",
    "        \n",
    "        # Concatenate all batches\n",
    "        model_preds_array = np.concatenate(model_predictions, axis=0)  # [N, 3]\n",
    "        all_predictions.append(model_preds_array)\n",
    "        \n",
    "        logger.success(f\"Model {model_idx + 1} predictions shape: {model_preds_array.shape}\")\n",
    "    \n",
    "    # Average predictions across all models\n",
    "    ensemble_predictions = np.mean(all_predictions, axis=0)  # [N, 3]\n",
    "    \n",
    "    logger.success(f\"Ensemble predictions shape: {ensemble_predictions.shape}\")\n",
    "    logger.info(f\"Ensemble predictions range: min={ensemble_predictions.min():.4f}, max={ensemble_predictions.max():.4f}\")\n",
    "    \n",
    "    # Create DataFrame with soft targets\n",
    "    soft_targets_df = tabular_df.copy()\n",
    "    \n",
    "    # Add soft target columns\n",
    "    soft_targets_df['Dry_Clover_g_soft'] = ensemble_predictions[:, 0]\n",
    "    soft_targets_df['Dry_Dead_g_soft'] = ensemble_predictions[:, 1]\n",
    "    soft_targets_df['Dry_Green_g_soft'] = ensemble_predictions[:, 2]\n",
    "    \n",
    "    # Keep original targets for reference\n",
    "    # (optional: remove them later for student training)\n",
    "    \n",
    "    logger.info(\"\\nSoft targets statistics:\")\n",
    "    logger.info(f\"Dry_Clover_g: mean={soft_targets_df['Dry_Clover_g_soft'].mean():.4f}, \"\n",
    "               f\"std={soft_targets_df['Dry_Clover_g_soft'].std():.4f}\")\n",
    "    logger.info(f\"Dry_Dead_g: mean={soft_targets_df['Dry_Dead_g_soft'].mean():.4f}, \"\n",
    "               f\"std={soft_targets_df['Dry_Dead_g_soft'].std():.4f}\")\n",
    "    logger.info(f\"Dry_Green_g: mean={soft_targets_df['Dry_Green_g_soft'].mean():.4f}, \"\n",
    "               f\"std={soft_targets_df['Dry_Green_g_soft'].std():.4f}\")\n",
    "    \n",
    "    return soft_targets_df, ensemble_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "b67d9c59",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_soft_targets(soft_targets_df: pd.DataFrame, output_path: str = './kaggle/input/'):\n",
    "    \"\"\"\n",
    "    Save soft targets to CSV.\n",
    "    \n",
    "    Args:\n",
    "        soft_targets_df: DataFrame with soft targets\n",
    "        output_path: Path to save CSV\n",
    "    \"\"\"\n",
    "    os.makedirs(output_path, exist_ok=True)\n",
    "    \n",
    "    output_file = os.path.join(output_path, 'train_with_soft_targets.csv')\n",
    "    soft_targets_df.to_csv(output_file, index=False)\n",
    "    \n",
    "    logger.success(f\"Saved soft targets to: {output_file}\")\n",
    "    logger.info(f\"Shape: {soft_targets_df.shape}\")\n",
    "    logger.info(f\"Columns: {soft_targets_df.columns.tolist()}\")\n",
    "    \n",
    "    return output_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "fc1ce7ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_soft_targets(soft_targets_df: pd.DataFrame):\n",
    "    \"\"\"\n",
    "    Validate soft targets quality.\n",
    "    \n",
    "    Args:\n",
    "        soft_targets_df: DataFrame with soft targets\n",
    "    \"\"\"\n",
    "    logger.info(\"\\n=== Soft Targets Validation ===\")\n",
    "    \n",
    "    for col in ['Dry_Clover_g_soft', 'Dry_Dead_g_soft', 'Dry_Green_g_soft']:\n",
    "        original_col = col.replace('_soft', '')\n",
    "        \n",
    "        # Check for NaN\n",
    "        nan_count = soft_targets_df[col].isna().sum()\n",
    "        if nan_count > 0:\n",
    "            logger.warning(f\"{col}: {nan_count} NaN values\")\n",
    "        \n",
    "        # Compare soft vs hard targets\n",
    "        corr = soft_targets_df[original_col].corr(soft_targets_df[col])\n",
    "        mse = np.mean((soft_targets_df[original_col].values - soft_targets_df[col].values)**2)\n",
    "\n",
    "        # logger.info(f\"{col}:\")\n",
    "        # logger.info(f\"  Correlation with hard target: {corr:.4f}\")\n",
    "        # logger.info(f\"  MSE vs hard target: {mse:.4f}\")\n",
    "        # logger.info(f\"  Range: [{soft_targets_df[col].min():.4f}, {soft_targets_df[col].max():.4f}]\")\n",
    "        msg = [f\"{col}:\", f\"Correlation with hard target: {corr:.4f}\", f\"MSE vs hard target: {mse:.4f}\", f\"Range: [{soft_targets_df[col].min():.4f}, {soft_targets_df[col].max():.4f}]\"]\n",
    "        msg = \"\\n\".join(msg)\n",
    "        logger.info(msg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bfb0a17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;2;161;247;255m\n",
      "[2025-12-11 14:20:17]\n",
      "INFO: Loading checkpoint: ./kaggle/checkpoints/teacher/fold0\\swinv2_tiny_window8_256-local_train[5]Folds_log_fusion-mean_epochs20_bs16_gradacc1_lr0.0001_wd0.05_dr0.2_hr0.5-fold0-epoch=13-val_loss=0.8938.ckpt\u001b[0m\n",
      "\u001b[38;2;161;247;255m\n",
      "[2025-12-11 14:20:18]\n",
      "INFO: Model initialized: backbone=swinv2_tiny_window8_256, feat_dim=768, combined_dim=789, fusion=mean, use_log_target=True\u001b[0m\n",
      "\u001b[38;2;161;247;255m\n",
      "[2025-12-11 14:20:18]\n",
      "INFO: Loading checkpoint: ./kaggle/checkpoints/teacher/fold1\\swinv2_tiny_window8_256-local_train[5]Folds_log_fusion-mean_epochs20_bs16_gradacc1_lr0.0001_wd0.05_dr0.2_hr0.5-fold1-epoch=11-val_loss=0.8113.ckpt\u001b[0m\n",
      "\u001b[38;2;161;247;255m\n",
      "[2025-12-11 14:20:19]\n",
      "INFO: Model initialized: backbone=swinv2_tiny_window8_256, feat_dim=768, combined_dim=789, fusion=mean, use_log_target=True\u001b[0m\n",
      "\u001b[38;2;161;247;255m\n",
      "[2025-12-11 14:20:19]\n",
      "INFO: Loading checkpoint: ./kaggle/checkpoints/teacher/fold2\\swinv2_tiny_window8_256-local_train[5]Folds_log_fusion-mean_epochs20_bs16_gradacc1_lr0.0001_wd0.05_dr0.2_hr0.5-fold2-epoch=19-val_loss=0.9368.ckpt\u001b[0m\n",
      "\u001b[38;2;161;247;255m\n",
      "[2025-12-11 14:20:20]\n",
      "INFO: Model initialized: backbone=swinv2_tiny_window8_256, feat_dim=768, combined_dim=789, fusion=mean, use_log_target=True\u001b[0m\n",
      "\u001b[38;2;161;247;255m\n",
      "[2025-12-11 14:20:20]\n",
      "INFO: Loading checkpoint: ./kaggle/checkpoints/teacher/fold3\\swinv2_tiny_window8_256-local_train[5]Folds_log_fusion-mean_epochs20_bs16_gradacc1_lr0.0001_wd0.05_dr0.2_hr0.5-fold3-epoch=16-val_loss=1.1476.ckpt\u001b[0m\n",
      "\u001b[38;2;161;247;255m\n",
      "[2025-12-11 14:20:22]\n",
      "INFO: Model initialized: backbone=swinv2_tiny_window8_256, feat_dim=768, combined_dim=789, fusion=mean, use_log_target=True\u001b[0m\n",
      "\u001b[38;2;161;247;255m\n",
      "[2025-12-11 14:20:22]\n",
      "INFO: Loading checkpoint: ./kaggle/checkpoints/teacher/fold4\\_swinv2_tiny_window8_256-local_train[5]Folds_log_fusion-mean_epochs20_bs16_gradacc1_lr0.0001_wd0.05_dr0.2_hr0.5-fold4-epoch=19-val_loss=1.0456.ckpt\u001b[0m\n",
      "\u001b[38;2;161;247;255m\n",
      "[2025-12-11 14:20:23]\n",
      "INFO: Model initialized: backbone=swinv2_tiny_window8_256, feat_dim=768, combined_dim=789, fusion=mean, use_log_target=True\u001b[0m\n",
      "\u001b[38;2;105;254;105m\n",
      "[2025-12-11 14:20:23]\n",
      "SUCCESS: Loaded 5 teacher models\u001b[0m\n",
      "\u001b[38;2;161;247;255m\n",
      "[2025-12-11 14:20:23]\n",
      "INFO: Creating soft targets using 5 teacher models...\u001b[0m\n",
      "\u001b[38;2;161;247;255m\n",
      "[2025-12-11 14:20:23]\n",
      "INFO: Total samples for soft targets: 357\u001b[0m\n",
      "\u001b[38;2;161;247;255m\n",
      "[2025-12-11 14:20:23]\n",
      "INFO: Inference batches: 12\u001b[0m\n",
      "\u001b[38;2;161;247;255m\n",
      "[2025-12-11 14:20:23]\n",
      "INFO: \n",
      "Processing teacher model 1/5...\u001b[0m\n",
      "\u001b[38;2;58;206;255m\n",
      "[2025-12-11 14:20:55]\n",
      "DEBUG:   Batch 10/12\u001b[0m\n",
      "\u001b[38;2;105;254;105m\n",
      "[2025-12-11 14:20:59]\n",
      "SUCCESS: Model 1 predictions shape: (357, 3)\u001b[0m\n",
      "\u001b[38;2;161;247;255m\n",
      "[2025-12-11 14:20:59]\n",
      "INFO: \n",
      "Processing teacher model 2/5...\u001b[0m\n",
      "\u001b[38;2;58;206;255m\n",
      "[2025-12-11 14:21:34]\n",
      "DEBUG:   Batch 10/12\u001b[0m\n",
      "\u001b[38;2;105;254;105m\n",
      "[2025-12-11 14:21:39]\n",
      "SUCCESS: Model 2 predictions shape: (357, 3)\u001b[0m\n",
      "\u001b[38;2;161;247;255m\n",
      "[2025-12-11 14:21:39]\n",
      "INFO: \n",
      "Processing teacher model 3/5...\u001b[0m\n",
      "\u001b[38;2;58;206;255m\n",
      "[2025-12-11 14:22:16]\n",
      "DEBUG:   Batch 10/12\u001b[0m\n",
      "\u001b[38;2;105;254;105m\n",
      "[2025-12-11 14:22:20]\n",
      "SUCCESS: Model 3 predictions shape: (357, 3)\u001b[0m\n",
      "\u001b[38;2;161;247;255m\n",
      "[2025-12-11 14:22:20]\n",
      "INFO: \n",
      "Processing teacher model 4/5...\u001b[0m\n",
      "\u001b[38;2;58;206;255m\n",
      "[2025-12-11 14:22:53]\n",
      "DEBUG:   Batch 10/12\u001b[0m\n",
      "\u001b[38;2;105;254;105m\n",
      "[2025-12-11 14:22:58]\n",
      "SUCCESS: Model 4 predictions shape: (357, 3)\u001b[0m\n",
      "\u001b[38;2;161;247;255m\n",
      "[2025-12-11 14:22:58]\n",
      "INFO: \n",
      "Processing teacher model 5/5...\u001b[0m\n",
      "\u001b[38;2;58;206;255m\n",
      "[2025-12-11 14:23:32]\n",
      "DEBUG:   Batch 10/12\u001b[0m\n",
      "\u001b[38;2;105;254;105m\n",
      "[2025-12-11 14:23:36]\n",
      "SUCCESS: Model 5 predictions shape: (357, 3)\u001b[0m\n",
      "\u001b[38;2;105;254;105m\n",
      "[2025-12-11 14:23:36]\n",
      "SUCCESS: Ensemble predictions shape: (357, 3)\u001b[0m\n",
      "\u001b[38;2;161;247;255m\n",
      "[2025-12-11 14:23:36]\n",
      "INFO: Ensemble predictions range: min=0.0000, max=142.3275\u001b[0m\n",
      "\u001b[38;2;161;247;255m\n",
      "[2025-12-11 14:23:36]\n",
      "INFO: \n",
      "Soft targets statistics:\u001b[0m\n",
      "\u001b[38;2;161;247;255m\n",
      "[2025-12-11 14:23:36]\n",
      "INFO: Dry_Clover_g: mean=7.0558, std=14.6372\u001b[0m\n",
      "\u001b[38;2;161;247;255m\n",
      "[2025-12-11 14:23:36]\n",
      "INFO: Dry_Dead_g: mean=11.9407, std=10.4754\u001b[0m\n",
      "\u001b[38;2;161;247;255m\n",
      "[2025-12-11 14:23:36]\n",
      "INFO: Dry_Green_g: mean=26.5914, std=25.4467\u001b[0m\n",
      "\u001b[38;2;161;247;255m\n",
      "[2025-12-11 14:23:36]\n",
      "INFO: \n",
      "=== Soft Targets Validation ===\u001b[0m\n",
      "\u001b[38;2;161;247;255m\n",
      "[2025-12-11 14:23:36]\n",
      "INFO: Dry_Clover_g_soft:\u001b[0m\n",
      "\u001b[38;2;161;247;255m\n",
      "[2025-12-11 14:23:36]\n",
      "INFO:   Correlation with hard target: 0.9426\u001b[0m\n",
      "\u001b[38;2;161;247;255m\n",
      "[2025-12-11 14:23:36]\n",
      "INFO:   MSE vs hard target: 26.7841\u001b[0m\n",
      "\u001b[38;2;161;247;255m\n",
      "[2025-12-11 14:23:36]\n",
      "INFO:   Range: [0.0000, 97.1455]\u001b[0m\n",
      "\u001b[38;2;161;247;255m\n",
      "[2025-12-11 14:23:36]\n",
      "INFO: Dry_Dead_g_soft:\u001b[0m\n",
      "\u001b[38;2;161;247;255m\n",
      "[2025-12-11 14:23:36]\n",
      "INFO:   Correlation with hard target: 0.8593\u001b[0m\n",
      "\u001b[38;2;161;247;255m\n",
      "[2025-12-11 14:23:36]\n",
      "INFO:   MSE vs hard target: 40.1800\u001b[0m\n",
      "\u001b[38;2;161;247;255m\n",
      "[2025-12-11 14:23:36]\n",
      "INFO:   Range: [0.0000, 56.2889]\u001b[0m\n",
      "\u001b[38;2;161;247;255m\n",
      "[2025-12-11 14:23:36]\n",
      "INFO: Dry_Green_g_soft:\u001b[0m\n",
      "\u001b[38;2;161;247;255m\n",
      "[2025-12-11 14:23:36]\n",
      "INFO:   Correlation with hard target: 0.9462\u001b[0m\n",
      "\u001b[38;2;161;247;255m\n",
      "[2025-12-11 14:23:36]\n",
      "INFO:   MSE vs hard target: 69.3651\u001b[0m\n",
      "\u001b[38;2;161;247;255m\n",
      "[2025-12-11 14:23:36]\n",
      "INFO:   Range: [0.0000, 142.3275]\u001b[0m\n",
      "\u001b[38;2;105;254;105m\n",
      "[2025-12-11 14:23:36]\n",
      "SUCCESS: Saved soft targets to: ./kaggle/input/train_with_soft_targets.csv\u001b[0m\n",
      "\u001b[38;2;161;247;255m\n",
      "[2025-12-11 14:23:36]\n",
      "INFO: Shape: (357, 15)\u001b[0m\n",
      "\u001b[38;2;161;247;255m\n",
      "[2025-12-11 14:23:36]\n",
      "INFO: Columns: ['image_path', 'Sampling_Date', 'State', 'Species', 'Height_Ave_cm', 'Pre_GSHH_NDVI', 'Dry_Clover_g', 'Dry_Dead_g', 'Dry_Green_g', 'Season', 'strat_group', 'fold', 'Dry_Clover_g_soft', 'Dry_Dead_g_soft', 'Dry_Green_g_soft']\u001b[0m\n",
      "File: ./kaggle/input/train_with_soft_targets.csv\n",
      "Samples: 357\n",
      "Soft target columns: ['Dry_Clover_g_soft', 'Dry_Dead_g_soft', 'Dry_Green_g_soft']\n"
     ]
    }
   ],
   "source": [
    "# Create soft targets\n",
    "soft_targets_df, ensemble_preds = create_soft_targets_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "52bac444",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;2;161;247;255m\n",
      "[2025-12-11 14:33:55]\n",
      "INFO: \n",
      "=== Soft Targets Validation ===\u001b[0m\n",
      "\u001b[38;2;161;247;255m\n",
      "[2025-12-11 14:33:55]\n",
      "INFO: Dry_Clover_g_soft:\n",
      "Correlation with hard target: 0.9426\n",
      "MSE vs hard target: 26.7841\n",
      "Range: [0.0000, 97.1455]\u001b[0m\n",
      "\u001b[38;2;161;247;255m\n",
      "[2025-12-11 14:33:55]\n",
      "INFO: Dry_Dead_g_soft:\n",
      "Correlation with hard target: 0.8593\n",
      "MSE vs hard target: 40.1800\n",
      "Range: [0.0000, 56.2889]\u001b[0m\n",
      "\u001b[38;2;161;247;255m\n",
      "[2025-12-11 14:33:55]\n",
      "INFO: Dry_Green_g_soft:\n",
      "Correlation with hard target: 0.9462\n",
      "MSE vs hard target: 69.3651\n",
      "Range: [0.0000, 142.3275]\u001b[0m\n",
      "\u001b[38;2;105;254;105m\n",
      "[2025-12-11 14:33:55]\n",
      "SUCCESS: Saved soft targets to: ./kaggle/input/train_with_soft_targets.csv\u001b[0m\n",
      "\u001b[38;2;161;247;255m\n",
      "[2025-12-11 14:33:55]\n",
      "INFO: Shape: (357, 15)\u001b[0m\n",
      "\u001b[38;2;161;247;255m\n",
      "[2025-12-11 14:33:55]\n",
      "INFO: Columns: ['image_path', 'Sampling_Date', 'State', 'Species', 'Height_Ave_cm', 'Pre_GSHH_NDVI', 'Dry_Clover_g', 'Dry_Dead_g', 'Dry_Green_g', 'Season', 'strat_group', 'fold', 'Dry_Clover_g_soft', 'Dry_Dead_g_soft', 'Dry_Green_g_soft']\u001b[0m\n",
      "File: ./kaggle/input/train_with_soft_targets.csv\n",
      "Samples: 357\n",
      "Soft target columns: ['Dry_Clover_g_soft', 'Dry_Dead_g_soft', 'Dry_Green_g_soft']\n"
     ]
    }
   ],
   "source": [
    "# Validate soft targets\n",
    "validate_soft_targets(soft_targets_df)\n",
    "\n",
    "# Save to CSV\n",
    "output_file = save_soft_targets(soft_targets_df)\n",
    "\n",
    "print(f\"File: {output_file}\")\n",
    "print(f\"Samples: {len(soft_targets_df)}\")\n",
    "print(f\"Soft target columns: {[c for c in soft_targets_df.columns if '_soft' in c]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2fd7f00",
   "metadata": {},
   "source": [
    "### Out of Fold Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "468d9e3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_oof_soft_targets():\n",
    "    \"\"\"\n",
    "    Create Out-Of-Fold soft targets from teacher models.\n",
    "    Each model predicts ONLY on its validation fold (unseen data).\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame with OOF soft targets\n",
    "    \"\"\"\n",
    "    logger.info(\"CREATING OUT-OF-FOLD (OOF) SOFT TARGETS\")\n",
    "    \n",
    "    # Initialize array to store OOF predictions\n",
    "    # Shape: [num_samples, 3] for 3 targets\n",
    "    oof_predictions = np.zeros((len(tabular_df), 3))\n",
    "    oof_indices = np.zeros(len(tabular_df), dtype=bool)  # Track which samples got predictions\n",
    "    \n",
    "    # For each fold\n",
    "    for fold_id in range(N_FOLDS):\n",
    "        logger.info(f\"\\nProcessing Fold {fold_id}...\")\n",
    "        \n",
    "        # Load model for this fold\n",
    "        fold_dir = os.path.join(CHECKPOINTS_DIR, f'fold{fold_id}')\n",
    "        ckpt_files = [f for f in os.listdir(fold_dir) if f.endswith('.ckpt')]\n",
    "        \n",
    "        if not ckpt_files:\n",
    "            logger.error(f\"No checkpoint found for fold {fold_id}\")\n",
    "            continue\n",
    "        \n",
    "        # Sort by val_loss to get best checkpoint\n",
    "        ckpt_files.sort(key=lambda x: float(x.split('=')[-1].replace('.ckpt', '')))\n",
    "        ckpt_path = os.path.join(fold_dir, ckpt_files[0])\n",
    "        \n",
    "        logger.info(f\"Loading checkpoint: {ckpt_path}\")\n",
    "        teacher_model = BiomassTeacherModel.load_from_checkpoint(ckpt_path)\n",
    "        teacher_model.eval()\n",
    "        teacher_model = teacher_model.to(DEVICE)\n",
    "        \n",
    "        # Get validation data for this fold\n",
    "        val_df = tabular_df[tabular_df['fold'] == fold_id].reset_index(drop=True)\n",
    "        val_indices = tabular_df[tabular_df['fold'] == fold_id].index.values\n",
    "        \n",
    "        logger.info(f\"Validation samples for fold {fold_id}: {len(val_df)}\")\n",
    "        \n",
    "        # Prepare tabular features for validation fold\n",
    "        fold_preprocessor = ColumnTransformer(\n",
    "            transformers=[\n",
    "                ('num', StandardScaler(), num_features),\n",
    "                ('cat', OneHotEncoder(sparse_output=False, handle_unknown='ignore'), cat_features)\n",
    "            ]\n",
    "        )\n",
    "        \n",
    "        # Fit on train, transform on val\n",
    "        train_df = tabular_df[tabular_df['fold'] != fold_id]\n",
    "        fold_preprocessor.fit(train_df)\n",
    "        val_tabular = fold_preprocessor.transform(val_df)\n",
    "        \n",
    "        # Create validation dataset\n",
    "        val_dataset = BiomassDataset(\n",
    "            df=val_df,\n",
    "            tabular_features=val_tabular,\n",
    "            target_cols=target_cols,\n",
    "            img_dir=PATH_TRAIN_IMG,\n",
    "            transform=val_transform,  # No augmentation\n",
    "            is_test=False,\n",
    "            use_log_target=USE_LOG_TARGET\n",
    "        )\n",
    "        \n",
    "        # Create validation loader\n",
    "        val_loader = DataLoader(\n",
    "            val_dataset,\n",
    "            batch_size=BATCH_SIZE * 2,\n",
    "            shuffle=False,  # IMPORTANT: keep order!\n",
    "            num_workers=min(NUM_WORKERS, 8),\n",
    "            pin_memory=True if torch.cuda.is_available() else False\n",
    "        )\n",
    "        \n",
    "        # Make predictions\n",
    "        fold_predictions = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch_idx, batch in enumerate(val_loader):\n",
    "                # Move batch to device\n",
    "                batch = {k: v.to(DEVICE) if isinstance(v, torch.Tensor) else v \n",
    "                        for k, v in batch.items()}\n",
    "                \n",
    "                # Get predictions\n",
    "                preds = teacher_model.predict_step(batch, 0)  # [B, 3]\n",
    "                fold_predictions.append(preds.cpu().numpy())\n",
    "        \n",
    "        # Concatenate batch predictions\n",
    "        fold_preds_array = np.concatenate(fold_predictions, axis=0)  # [N_val, 3]\n",
    "        \n",
    "        logger.success(f\"Fold {fold_id} predictions shape: {fold_preds_array.shape}\")\n",
    "        \n",
    "        # Verify indices match\n",
    "        assert len(fold_preds_array) == len(val_indices), \\\n",
    "            f\"Predictions length {len(fold_preds_array)} != indices length {len(val_indices)}\"\n",
    "        \n",
    "        # Store predictions at correct indices\n",
    "        oof_predictions[val_indices] = fold_preds_array\n",
    "        oof_indices[val_indices] = True\n",
    "        \n",
    "        logger.info(f\"Stored predictions for indices: {val_indices[:5]}... (showing first 5)\")\n",
    "        \n",
    "        # Clean up\n",
    "        del teacher_model\n",
    "        torch.cuda.empty_cache() if torch.cuda.is_available() else None\n",
    "    \n",
    "    # Verify all samples got predictions\n",
    "    if not oof_indices.all():\n",
    "        missing_count = (~oof_indices).sum()\n",
    "        logger.warning(f\"Missing predictions for {missing_count} samples!\")\n",
    "    else:\n",
    "        logger.success(f\"All {len(oof_predictions)} samples have OOF predictions!\")\n",
    "    \n",
    "    # Create DataFrame with OOF soft targets\n",
    "    soft_targets_df = tabular_df.copy()\n",
    "    \n",
    "    # Add OOF soft target columns\n",
    "    soft_targets_df['Dry_Clover_g_soft'] = oof_predictions[:, 0]\n",
    "    soft_targets_df['Dry_Dead_g_soft'] = oof_predictions[:, 1]\n",
    "    soft_targets_df['Dry_Green_g_soft'] = oof_predictions[:, 2]\n",
    "    \n",
    "    logger.info(\"\\n=== OOF Soft Targets Statistics ===\")\n",
    "    logger.info(f\"Dry_Clover_g: mean={soft_targets_df['Dry_Clover_g_soft'].mean():.4f}, \"\n",
    "               f\"std={soft_targets_df['Dry_Clover_g_soft'].std():.4f}\")\n",
    "    logger.info(f\"Dry_Dead_g: mean={soft_targets_df['Dry_Dead_g_soft'].mean():.4f}, \"\n",
    "               f\"std={soft_targets_df['Dry_Dead_g_soft'].std():.4f}\")\n",
    "    logger.info(f\"Dry_Green_g: mean={soft_targets_df['Dry_Green_g_soft'].mean():.4f}, \"\n",
    "               f\"std={soft_targets_df['Dry_Green_g_soft'].std():.4f}\")\n",
    "    \n",
    "    return soft_targets_df, oof_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "7067bfc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_oof_soft_targets(soft_targets_df: pd.DataFrame, output_path: str = './kaggle/input/'):\n",
    "    \"\"\"\n",
    "    Save OOF soft targets to CSV.\n",
    "    \n",
    "    Args:\n",
    "        soft_targets_df: DataFrame with OOF soft targets\n",
    "        output_path: Path to save CSV\n",
    "    \"\"\"\n",
    "    os.makedirs(output_path, exist_ok=True)\n",
    "    \n",
    "    output_file = os.path.join(output_path, 'train_with_oof_soft_targets.csv')\n",
    "    soft_targets_df.to_csv(output_file, index=False)\n",
    "    \n",
    "    logger.success(f\"Saved OOF soft targets to: {output_file}\")\n",
    "    logger.info(f\"Shape: {soft_targets_df.shape}\")\n",
    "    logger.info(f\"Columns: {soft_targets_df.columns.tolist()}\")\n",
    "    \n",
    "    return output_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "92a2fd88",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_oof_vs_ensemble(oof_df: pd.DataFrame, ensemble_df: pd.DataFrame):\n",
    "    \"\"\"\n",
    "    Compare OOF predictions vs direct ensemble predictions.\n",
    "\n",
    "    Args:\n",
    "        oof_df: DataFrame with OOF soft targets\n",
    "        ensemble_df: DataFrame with ensemble soft targets\n",
    "    \"\"\"\n",
    "    logger.info(\"COMPARING OOF vs ENSEMBLE PREDICTIONS\")\n",
    "\n",
    "    for col in ['Dry_Clover_g_soft', 'Dry_Dead_g_soft', 'Dry_Green_g_soft']:\n",
    "        oof_vals = oof_df[col].values\n",
    "        ens_vals = ensemble_df[col].values\n",
    "\n",
    "        # Correlation\n",
    "        corr = np.corrcoef(oof_vals, ens_vals)[0, 1]\n",
    "\n",
    "        # Mean Absolute Difference\n",
    "        mad = np.mean(np.abs(oof_vals - ens_vals))\n",
    "\n",
    "        # RMSE\n",
    "        rmse = np.sqrt(np.mean((oof_vals - ens_vals)**2))\n",
    "\n",
    "        msgs = [\n",
    "            f\"\\n{col}:\",\n",
    "            f\"  Correlation: {corr:.4f}\",\n",
    "            f\"  Mean Absolute Diff: {mad:.4f}\",\n",
    "            f\"  RMSE: {rmse:.4f}\",\n",
    "            f\"  OOF  range: [{oof_vals.min():.2f}, {oof_vals.max():.2f}]\",\n",
    "            f\"  Ensemble range: [{ens_vals.min():.2f}, {ens_vals.max():.2f}]\"\n",
    "        ]\n",
    "        logger.info(\"\\n\".join(msgs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "98a17903",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;2;161;247;255m\n",
      "[2025-12-11 14:42:18]\n",
      "INFO: CREATING OUT-OF-FOLD (OOF) SOFT TARGETS\u001b[0m\n",
      "\u001b[38;2;161;247;255m\n",
      "[2025-12-11 14:42:18]\n",
      "INFO: \n",
      "Processing Fold 0...\u001b[0m\n",
      "\u001b[38;2;161;247;255m\n",
      "[2025-12-11 14:42:18]\n",
      "INFO: Loading checkpoint: ./kaggle/checkpoints/teacher/fold0\\swinv2_tiny_window8_256-local_train[5]Folds_log_fusion-mean_epochs20_bs16_gradacc1_lr0.0001_wd0.05_dr0.2_hr0.5-fold0-epoch=13-val_loss=0.8938.ckpt\u001b[0m\n",
      "\u001b[38;2;161;247;255m\n",
      "[2025-12-11 14:42:19]\n",
      "INFO: Model initialized: backbone=swinv2_tiny_window8_256, feat_dim=768, combined_dim=789, fusion=mean, use_log_target=True\u001b[0m\n",
      "\u001b[38;2;161;247;255m\n",
      "[2025-12-11 14:42:19]\n",
      "INFO: Validation samples for fold 0: 72\u001b[0m\n",
      "\u001b[38;2;105;254;105m\n",
      "[2025-12-11 14:42:28]\n",
      "SUCCESS: Fold 0 predictions shape: (72, 3)\u001b[0m\n",
      "\u001b[38;2;161;247;255m\n",
      "[2025-12-11 14:42:28]\n",
      "INFO: Stored predictions for indices: [ 1  3  6 11 12]... (showing first 5)\u001b[0m\n",
      "\u001b[38;2;161;247;255m\n",
      "[2025-12-11 14:42:28]\n",
      "INFO: \n",
      "Processing Fold 1...\u001b[0m\n",
      "\u001b[38;2;161;247;255m\n",
      "[2025-12-11 14:42:28]\n",
      "INFO: Loading checkpoint: ./kaggle/checkpoints/teacher/fold1\\swinv2_tiny_window8_256-local_train[5]Folds_log_fusion-mean_epochs20_bs16_gradacc1_lr0.0001_wd0.05_dr0.2_hr0.5-fold1-epoch=11-val_loss=0.8113.ckpt\u001b[0m\n",
      "\u001b[38;2;161;247;255m\n",
      "[2025-12-11 14:42:30]\n",
      "INFO: Model initialized: backbone=swinv2_tiny_window8_256, feat_dim=768, combined_dim=789, fusion=mean, use_log_target=True\u001b[0m\n",
      "\u001b[38;2;161;247;255m\n",
      "[2025-12-11 14:42:30]\n",
      "INFO: Validation samples for fold 1: 72\u001b[0m\n",
      "\u001b[38;2;105;254;105m\n",
      "[2025-12-11 14:42:39]\n",
      "SUCCESS: Fold 1 predictions shape: (72, 3)\u001b[0m\n",
      "\u001b[38;2;161;247;255m\n",
      "[2025-12-11 14:42:39]\n",
      "INFO: Stored predictions for indices: [ 2 10 16 18 20]... (showing first 5)\u001b[0m\n",
      "\u001b[38;2;161;247;255m\n",
      "[2025-12-11 14:42:39]\n",
      "INFO: \n",
      "Processing Fold 2...\u001b[0m\n",
      "\u001b[38;2;161;247;255m\n",
      "[2025-12-11 14:42:39]\n",
      "INFO: Loading checkpoint: ./kaggle/checkpoints/teacher/fold2\\swinv2_tiny_window8_256-local_train[5]Folds_log_fusion-mean_epochs20_bs16_gradacc1_lr0.0001_wd0.05_dr0.2_hr0.5-fold2-epoch=19-val_loss=0.9368.ckpt\u001b[0m\n",
      "\u001b[38;2;161;247;255m\n",
      "[2025-12-11 14:42:41]\n",
      "INFO: Model initialized: backbone=swinv2_tiny_window8_256, feat_dim=768, combined_dim=789, fusion=mean, use_log_target=True\u001b[0m\n",
      "\u001b[38;2;161;247;255m\n",
      "[2025-12-11 14:42:41]\n",
      "INFO: Validation samples for fold 2: 71\u001b[0m\n",
      "\u001b[38;2;105;254;105m\n",
      "[2025-12-11 14:42:50]\n",
      "SUCCESS: Fold 2 predictions shape: (71, 3)\u001b[0m\n",
      "\u001b[38;2;161;247;255m\n",
      "[2025-12-11 14:42:50]\n",
      "INFO: Stored predictions for indices: [ 0  4  5 15 27]... (showing first 5)\u001b[0m\n",
      "\u001b[38;2;161;247;255m\n",
      "[2025-12-11 14:42:50]\n",
      "INFO: \n",
      "Processing Fold 3...\u001b[0m\n",
      "\u001b[38;2;161;247;255m\n",
      "[2025-12-11 14:42:50]\n",
      "INFO: Loading checkpoint: ./kaggle/checkpoints/teacher/fold3\\swinv2_tiny_window8_256-local_train[5]Folds_log_fusion-mean_epochs20_bs16_gradacc1_lr0.0001_wd0.05_dr0.2_hr0.5-fold3-epoch=16-val_loss=1.1476.ckpt\u001b[0m\n",
      "\u001b[38;2;161;247;255m\n",
      "[2025-12-11 14:42:52]\n",
      "INFO: Model initialized: backbone=swinv2_tiny_window8_256, feat_dim=768, combined_dim=789, fusion=mean, use_log_target=True\u001b[0m\n",
      "\u001b[38;2;161;247;255m\n",
      "[2025-12-11 14:42:52]\n",
      "INFO: Validation samples for fold 3: 71\u001b[0m\n",
      "\u001b[38;2;105;254;105m\n",
      "[2025-12-11 14:43:01]\n",
      "SUCCESS: Fold 3 predictions shape: (71, 3)\u001b[0m\n",
      "\u001b[38;2;161;247;255m\n",
      "[2025-12-11 14:43:01]\n",
      "INFO: Stored predictions for indices: [ 9 13 14 21 23]... (showing first 5)\u001b[0m\n",
      "\u001b[38;2;161;247;255m\n",
      "[2025-12-11 14:43:01]\n",
      "INFO: \n",
      "Processing Fold 4...\u001b[0m\n",
      "\u001b[38;2;161;247;255m\n",
      "[2025-12-11 14:43:01]\n",
      "INFO: Loading checkpoint: ./kaggle/checkpoints/teacher/fold4\\_swinv2_tiny_window8_256-local_train[5]Folds_log_fusion-mean_epochs20_bs16_gradacc1_lr0.0001_wd0.05_dr0.2_hr0.5-fold4-epoch=19-val_loss=1.0456.ckpt\u001b[0m\n",
      "\u001b[38;2;161;247;255m\n",
      "[2025-12-11 14:43:02]\n",
      "INFO: Model initialized: backbone=swinv2_tiny_window8_256, feat_dim=768, combined_dim=789, fusion=mean, use_log_target=True\u001b[0m\n",
      "\u001b[38;2;161;247;255m\n",
      "[2025-12-11 14:43:03]\n",
      "INFO: Validation samples for fold 4: 71\u001b[0m\n",
      "\u001b[38;2;105;254;105m\n",
      "[2025-12-11 14:43:11]\n",
      "SUCCESS: Fold 4 predictions shape: (71, 3)\u001b[0m\n",
      "\u001b[38;2;161;247;255m\n",
      "[2025-12-11 14:43:11]\n",
      "INFO: Stored predictions for indices: [ 7  8 19 24 28]... (showing first 5)\u001b[0m\n",
      "\u001b[38;2;105;254;105m\n",
      "[2025-12-11 14:43:11]\n",
      "SUCCESS: All 357 samples have OOF predictions!\u001b[0m\n",
      "\u001b[38;2;161;247;255m\n",
      "[2025-12-11 14:43:11]\n",
      "INFO: \n",
      "=== OOF Soft Targets Statistics ===\u001b[0m\n",
      "\u001b[38;2;161;247;255m\n",
      "[2025-12-11 14:43:11]\n",
      "INFO: Dry_Clover_g: mean=6.6410, std=13.9213\u001b[0m\n",
      "\u001b[38;2;161;247;255m\n",
      "[2025-12-11 14:43:11]\n",
      "INFO: Dry_Dead_g: mean=11.5125, std=10.1113\u001b[0m\n",
      "\u001b[38;2;161;247;255m\n",
      "[2025-12-11 14:43:11]\n",
      "INFO: Dry_Green_g: mean=26.0409, std=24.9338\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Create OOF soft targets\n",
    "oof_soft_targets_df, oof_preds = create_oof_soft_targets()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "4ea8ecac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;2;161;247;255m\n",
      "[2025-12-11 14:43:11]\n",
      "INFO: \n",
      "=== Soft Targets Validation ===\u001b[0m\n",
      "\u001b[38;2;161;247;255m\n",
      "[2025-12-11 14:43:11]\n",
      "INFO: Dry_Clover_g_soft:\n",
      "Correlation with hard target: 0.8748\n",
      "MSE vs hard target: 45.3763\n",
      "Range: [0.0000, 83.9534]\u001b[0m\n",
      "\u001b[38;2;161;247;255m\n",
      "[2025-12-11 14:43:11]\n",
      "INFO: Dry_Dead_g_soft:\n",
      "Correlation with hard target: 0.7059\n",
      "MSE vs hard target: 79.0653\n",
      "Range: [0.0000, 54.3228]\u001b[0m\n",
      "\u001b[38;2;161;247;255m\n",
      "[2025-12-11 14:43:11]\n",
      "INFO: Dry_Green_g_soft:\n",
      "Correlation with hard target: 0.8542\n",
      "MSE vs hard target: 184.7793\n",
      "Range: [0.0000, 140.0474]\u001b[0m\n",
      "\u001b[38;2;105;254;105m\n",
      "[2025-12-11 14:43:11]\n",
      "SUCCESS: Saved OOF soft targets to: ./kaggle/input/train_with_oof_soft_targets.csv\u001b[0m\n",
      "\u001b[38;2;161;247;255m\n",
      "[2025-12-11 14:43:11]\n",
      "INFO: Shape: (357, 15)\u001b[0m\n",
      "\u001b[38;2;161;247;255m\n",
      "[2025-12-11 14:43:11]\n",
      "INFO: Columns: ['image_path', 'Sampling_Date', 'State', 'Species', 'Height_Ave_cm', 'Pre_GSHH_NDVI', 'Dry_Clover_g', 'Dry_Dead_g', 'Dry_Green_g', 'Season', 'strat_group', 'fold', 'Dry_Clover_g_soft', 'Dry_Dead_g_soft', 'Dry_Green_g_soft']\u001b[0m\n",
      "File: ./kaggle/input/train_with_oof_soft_targets.csv\n",
      "Samples: 357\n"
     ]
    }
   ],
   "source": [
    "# Validate OOF soft targets\n",
    "validate_soft_targets(oof_soft_targets_df)\n",
    "\n",
    "# Save to CSV\n",
    "oof_output_file = save_oof_soft_targets(oof_soft_targets_df)\n",
    "print(f\"File: {oof_output_file}\")\n",
    "print(f\"Samples: {len(oof_soft_targets_df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "7cda9e9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;2;161;247;255m\n",
      "[2025-12-11 14:43:11]\n",
      "INFO: COMPARING OOF vs ENSEMBLE PREDICTIONS\u001b[0m\n",
      "\u001b[38;2;161;247;255m\n",
      "[2025-12-11 14:43:11]\n",
      "INFO: \n",
      "Dry_Clover_g_soft:\n",
      "  Correlation: 0.9606\n",
      "  Mean Absolute Diff: 1.5858\n",
      "  RMSE: 4.0862\n",
      "  OOF  range: [0.00, 83.95]\n",
      "  Ensemble range: [0.00, 97.15]\u001b[0m\n",
      "\u001b[38;2;161;247;255m\n",
      "[2025-12-11 14:43:11]\n",
      "INFO: \n",
      "Dry_Dead_g_soft:\n",
      "  Correlation: 0.9250\n",
      "  Mean Absolute Diff: 2.5321\n",
      "  RMSE: 4.0186\n",
      "  OOF  range: [0.00, 54.32]\n",
      "  Ensemble range: [0.00, 56.29]\u001b[0m\n",
      "\u001b[38;2;161;247;255m\n",
      "[2025-12-11 14:43:11]\n",
      "INFO: \n",
      "Dry_Green_g_soft:\n",
      "  Correlation: 0.9481\n",
      "  Mean Absolute Diff: 4.5954\n",
      "  RMSE: 8.1427\n",
      "  OOF  range: [0.00, 140.05]\n",
      "  Ensemble range: [0.00, 142.33]\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Compare with ensemble predictions (if available)\n",
    "if 'soft_targets_df' in locals():\n",
    "    compare_oof_vs_ensemble(oof_soft_targets_df, soft_targets_df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "image2biomass",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
