{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9dafc480",
   "metadata": {},
   "source": [
    "# Distillation. Student"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bba164c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: Currently logged in as: dmykhailov (dmykhailov-kyiv-school-of-economics) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin\n"
     ]
    }
   ],
   "source": [
    "machine = \"local\"\n",
    "!wandb login"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34bdd7d3",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ff175635",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available variables: ['BASE_DIR', 'DATA_DIR', 'Path', 'directory', 'find_project_root', 'project_root', 'sys']\n",
      "PyTorch: 2.9.1+cu128\n",
      "Device: NVIDIA GeForce RTX 5050 Laptop GPU\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import gc\n",
    "import logging\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "from typing import cast\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "\n",
    "import cv2\n",
    "import timm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import pytorch_lightning as pl\n",
    "from tqdm import tqdm\n",
    "from torch.optim import AdamW\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint, EarlyStopping, LearningRateMonitor\n",
    "from pytorch_lightning.loggers import WandbLogger\n",
    "\n",
    "import warnings\n",
    "from notebooks_config import setup_logging, CustomLogger\n",
    "\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "print(f\"PyTorch: {torch.__version__}\")\n",
    "print(f\"Device: {torch.cuda.get_device_name(0) if torch.cuda.is_available() else 'CPU'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ba1c03c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;2;105;254;105m\n",
      "[2025-12-11 15:28:51]\n",
      "SUCCESS: Logging configured successfully âœ…\u001b[0m\n",
      "\u001b[38;2;105;254;105m\n",
      "[2025-12-11 15:28:51]\n",
      "SUCCESS: Logging configuration test completed.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "logger = setup_logging(level=logging.DEBUG, full_color=True, include_function=False)\n",
    "logger = cast(CustomLogger, logger)  # Type hinting\n",
    "logger.success(\"Logging configuration test completed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e452128a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 1488\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DESCRIPTION_FULL: swinv2_tiny_window8_256-local_train[5]Folds_log_fusion-mean_epochs25_bs16_gradacc1_lr0.0001_wd0.05_dr0.3_hr0.5\n",
      "Effective batch size: 16\n"
     ]
    }
   ],
   "source": [
    "cpu_count = os.cpu_count()\n",
    "NUM_WORKERS = 0 if machine == \"local\" else cpu_count // 2 if cpu_count else 0\n",
    "\n",
    "LR = 1e-4\n",
    "EPOCHS = 25\n",
    "N_FOLDS = 5\n",
    "GRAD_ACCUM = 1\n",
    "BATCH_SIZE = 16\n",
    "DROPOUT_RATE = 0.3\n",
    "DISTILL_ALPHA = 0.5  # Weight for distillation loss\n",
    "WEIGHT_DECAY = 0.05\n",
    "HIDDEN_RATIO = 0.5\n",
    "TRAIN_SPLIT_RATIO = 0.02 # Used if N_FOLDS = 0\n",
    "\n",
    "MODEL = \"swinv2_tiny_window8_256\"\n",
    "MODEL_STAGE = \"student\"  # 'teacher' or 'student'\n",
    "PROJECT_NAME = \"csiro-image2biomass-prediction\"\n",
    "CHECKPOINTS_DIR = f\"./kaggle/checkpoints/{MODEL_STAGE}/\"\n",
    "# Whether to use OOF soft targets or 100% ensemble soft targets\n",
    "USE_OOF_SOFT_TARGETS = False\n",
    "\n",
    "# Each patch is 1000x1000, resize to 768x768 for vision transformers\n",
    "SIZE = 768\n",
    "USE_LOG_TARGET = True   # Whether to use log1p transformation on target variable\n",
    "FUSION_METHOD = 'mean'  # ('concat', 'mean', 'max')\n",
    "\n",
    "DESCRIPTION = machine + \\\n",
    "    (f\"_train{TRAIN_SPLIT_RATIO}\" if N_FOLDS == 0 else f\"_train[{N_FOLDS}]Folds\") + (\n",
    "        f\"_log\" if USE_LOG_TARGET else \"\") + f\"_fusion-{FUSION_METHOD}\"\n",
    "DESCRIPTION_FULL = MODEL + \"-\" + DESCRIPTION + \\\n",
    "    f\"_epochs{EPOCHS}_bs{BATCH_SIZE}_gradacc{GRAD_ACCUM}_lr{LR}_wd{WEIGHT_DECAY}_dr{DROPOUT_RATE}_hr{HIDDEN_RATIO}\"\n",
    "SUBMISSION_NAME = f\"{DESCRIPTION_FULL}_submission.csv\"\n",
    "SUBMISSION_ENSEMBLE_NAME = f\"{DESCRIPTION_FULL}_ensemble_submission.csv\"\n",
    "SUBMISSION_MSG = DESCRIPTION_FULL.replace(\"_\", \" \")\n",
    "\n",
    "SEED = 1488\n",
    "torch.manual_seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "pl.seed_everything(SEED)\n",
    "\n",
    "print(\"DESCRIPTION_FULL:\", DESCRIPTION_FULL)\n",
    "print(f\"Effective batch size: {BATCH_SIZE * GRAD_ACCUM}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1804fc71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "NUM_WORKERS: 0\n",
      "\n",
      "NVIDIA GeForce RTX 5050 Laptop GPU\n",
      "Memory Usage:\n",
      "Allocated: 0.0 GB\n",
      "Cached:    0.0 GB\n"
     ]
    }
   ],
   "source": [
    "# setting device on GPU if available, else CPU\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Using device:', DEVICE)\n",
    "print('NUM_WORKERS:', NUM_WORKERS)\n",
    "print()\n",
    "\n",
    "# Additional Info when using cuda\n",
    "if DEVICE.type == 'cuda':\n",
    "    # clean GPU memory\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "\n",
    "    torch.set_float32_matmul_precision('high') if machine == \"local\" else None\n",
    "\n",
    "    print(torch.cuda.get_device_name(0))\n",
    "    print('Memory Usage:')\n",
    "    print('Allocated:', round(torch.cuda.memory_allocated(0)/1024**3, 1), 'GB')\n",
    "    print('Cached:   ', round(torch.cuda.memory_reserved(0)/1024**3, 1), 'GB')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b8b9504",
   "metadata": {},
   "source": [
    "## Data Loading and Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e402a80c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset size: (1071, 9)\n"
     ]
    },
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "sample_id",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "image_path",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "Sampling_Date",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "State",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "Species",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "Pre_GSHH_NDVI",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Height_Ave_cm",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "target_name",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "target",
         "rawType": "float64",
         "type": "float"
        }
       ],
       "ref": "ed94e982-50f5-4a09-a4c7-5ecfb8f580ee",
       "rows": [
        [
         "0",
         "ID1011485656__Dry_Clover_g",
         "train/ID1011485656.jpg",
         "2015/9/4",
         "Tas",
         "Ryegrass_Clover",
         "0.62",
         "4.6667",
         "Dry_Clover_g",
         "0.0"
        ],
        [
         "1",
         "ID1011485656__Dry_Dead_g",
         "train/ID1011485656.jpg",
         "2015/9/4",
         "Tas",
         "Ryegrass_Clover",
         "0.62",
         "4.6667",
         "Dry_Dead_g",
         "31.9984"
        ],
        [
         "2",
         "ID1011485656__Dry_Green_g",
         "train/ID1011485656.jpg",
         "2015/9/4",
         "Tas",
         "Ryegrass_Clover",
         "0.62",
         "4.6667",
         "Dry_Green_g",
         "16.275"
        ],
        [
         "5",
         "ID1012260530__Dry_Clover_g",
         "train/ID1012260530.jpg",
         "2015/4/1",
         "NSW",
         "Lucerne",
         "0.55",
         "16.0",
         "Dry_Clover_g",
         "0.0"
        ],
        [
         "6",
         "ID1012260530__Dry_Dead_g",
         "train/ID1012260530.jpg",
         "2015/4/1",
         "NSW",
         "Lucerne",
         "0.55",
         "16.0",
         "Dry_Dead_g",
         "0.0"
        ]
       ],
       "shape": {
        "columns": 9,
        "rows": 5
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sample_id</th>\n",
       "      <th>image_path</th>\n",
       "      <th>Sampling_Date</th>\n",
       "      <th>State</th>\n",
       "      <th>Species</th>\n",
       "      <th>Pre_GSHH_NDVI</th>\n",
       "      <th>Height_Ave_cm</th>\n",
       "      <th>target_name</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ID1011485656__Dry_Clover_g</td>\n",
       "      <td>train/ID1011485656.jpg</td>\n",
       "      <td>2015/9/4</td>\n",
       "      <td>Tas</td>\n",
       "      <td>Ryegrass_Clover</td>\n",
       "      <td>0.62</td>\n",
       "      <td>4.6667</td>\n",
       "      <td>Dry_Clover_g</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ID1011485656__Dry_Dead_g</td>\n",
       "      <td>train/ID1011485656.jpg</td>\n",
       "      <td>2015/9/4</td>\n",
       "      <td>Tas</td>\n",
       "      <td>Ryegrass_Clover</td>\n",
       "      <td>0.62</td>\n",
       "      <td>4.6667</td>\n",
       "      <td>Dry_Dead_g</td>\n",
       "      <td>31.9984</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ID1011485656__Dry_Green_g</td>\n",
       "      <td>train/ID1011485656.jpg</td>\n",
       "      <td>2015/9/4</td>\n",
       "      <td>Tas</td>\n",
       "      <td>Ryegrass_Clover</td>\n",
       "      <td>0.62</td>\n",
       "      <td>4.6667</td>\n",
       "      <td>Dry_Green_g</td>\n",
       "      <td>16.2750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>ID1012260530__Dry_Clover_g</td>\n",
       "      <td>train/ID1012260530.jpg</td>\n",
       "      <td>2015/4/1</td>\n",
       "      <td>NSW</td>\n",
       "      <td>Lucerne</td>\n",
       "      <td>0.55</td>\n",
       "      <td>16.0000</td>\n",
       "      <td>Dry_Clover_g</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>ID1012260530__Dry_Dead_g</td>\n",
       "      <td>train/ID1012260530.jpg</td>\n",
       "      <td>2015/4/1</td>\n",
       "      <td>NSW</td>\n",
       "      <td>Lucerne</td>\n",
       "      <td>0.55</td>\n",
       "      <td>16.0000</td>\n",
       "      <td>Dry_Dead_g</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    sample_id              image_path Sampling_Date State  \\\n",
       "0  ID1011485656__Dry_Clover_g  train/ID1011485656.jpg      2015/9/4   Tas   \n",
       "1    ID1011485656__Dry_Dead_g  train/ID1011485656.jpg      2015/9/4   Tas   \n",
       "2   ID1011485656__Dry_Green_g  train/ID1011485656.jpg      2015/9/4   Tas   \n",
       "5  ID1012260530__Dry_Clover_g  train/ID1012260530.jpg      2015/4/1   NSW   \n",
       "6    ID1012260530__Dry_Dead_g  train/ID1012260530.jpg      2015/4/1   NSW   \n",
       "\n",
       "           Species  Pre_GSHH_NDVI  Height_Ave_cm   target_name   target  \n",
       "0  Ryegrass_Clover           0.62         4.6667  Dry_Clover_g   0.0000  \n",
       "1  Ryegrass_Clover           0.62         4.6667    Dry_Dead_g  31.9984  \n",
       "2  Ryegrass_Clover           0.62         4.6667   Dry_Green_g  16.2750  \n",
       "5          Lucerne           0.55        16.0000  Dry_Clover_g   0.0000  \n",
       "6          Lucerne           0.55        16.0000    Dry_Dead_g   0.0000  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "PATH_DATA = './kaggle/input/csiro-biomass'\n",
    "PATH_TRAIN_CSV = os.path.join(PATH_DATA, 'train.csv')\n",
    "PATH_TEST_CSV = os.path.join(PATH_DATA, 'test.csv')\n",
    "PATH_TRAIN_IMG = os.path.join(PATH_DATA, 'train')\n",
    "PATH_TEST_IMG = os.path.join(PATH_DATA, 'test')\n",
    "\n",
    "df = pd.read_csv(PATH_TRAIN_CSV)\n",
    "# Remove unneeded targets\n",
    "df = df[~df['target_name'].isin(['Dry_Total_g', 'GDM_g'])]\n",
    "print(f\"Dataset size: {df.shape}\")\n",
    "display(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e1285873",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(357, 9)\n",
      "               image_path Sampling_Date State            Species  \\\n",
      "0  train/ID1011485656.jpg      2015/9/4   Tas    Ryegrass_Clover   \n",
      "1  train/ID1012260530.jpg      2015/4/1   NSW            Lucerne   \n",
      "2  train/ID1025234388.jpg      2015/9/1    WA  SubcloverDalkeith   \n",
      "3  train/ID1028611175.jpg     2015/5/18   Tas           Ryegrass   \n",
      "4  train/ID1035947949.jpg     2015/9/11   Tas           Ryegrass   \n",
      "\n",
      "   Height_Ave_cm  Pre_GSHH_NDVI  Dry_Clover_g  Dry_Dead_g  Dry_Green_g  \n",
      "0         4.6667           0.62        0.0000     31.9984      16.2750  \n",
      "1        16.0000           0.55        0.0000      0.0000       7.6000  \n",
      "2         1.0000           0.38        6.0500      0.0000       0.0000  \n",
      "3         5.0000           0.66        0.0000     30.9703      24.2376  \n",
      "4         3.5000           0.54        0.4343     23.2239      10.5261  \n"
     ]
    }
   ],
   "source": [
    "# pivot the dataframe to have one row per image with multiple target columns\n",
    "tabular_df = df.pivot_table(index=['image_path', 'Sampling_Date', 'State', 'Species', 'Height_Ave_cm', 'Pre_GSHH_NDVI'],\n",
    "                            columns='target_name', values='target', aggfunc='first').reset_index()\n",
    "tabular_df.columns.name = None  # remove the aggregation name\n",
    "print(tabular_df.shape)\n",
    "print(tabular_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0c96a1db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['image_path', 'Sampling_Date', 'State', 'Species', 'Height_Ave_cm', 'Pre_GSHH_NDVI', 'Dry_Clover_g', 'Dry_Dead_g', 'Dry_Green_g']\n"
     ]
    }
   ],
   "source": [
    "print(tabular_df.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0220a26c",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_cols  = ['Dry_Clover_g', 'Dry_Dead_g', 'Dry_Green_g']\n",
    "num_features = ['Height_Ave_cm', 'Pre_GSHH_NDVI']\n",
    "cat_features = ['Species', 'State']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b44b2e92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BUG: data leakage, will be fixed later in this code\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', StandardScaler(), num_features), # normalizing numeric features\n",
    "        ('cat', OneHotEncoder(sparse_output=False, handle_unknown='ignore'), cat_features) # OHE for categorical features\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cdb7c40c",
   "metadata": {},
   "outputs": [],
   "source": [
    "tabular_data = preprocessor.fit_transform(tabular_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6bd47682",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(357, 21)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[-0.28520388, -0.24631873,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  1.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  1.        ,  0.        ,\n",
       "         0.        ],\n",
       "       [ 0.81823967, -0.70706013,  0.        ,  0.        ,  0.        ,\n",
       "         1.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  1.        ,  0.        ,  0.        ,\n",
       "         0.        ],\n",
       "       [-0.64220462, -1.82600352,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  1.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         1.        ],\n",
       "       [-0.25275281,  0.01696207,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  1.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  1.        ,  0.        ,\n",
       "         0.        ],\n",
       "       [-0.39879723, -0.77288032,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  1.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  1.        ,  0.        ,\n",
       "         0.        ]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(tabular_data.shape)\n",
    "display(tabular_data[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2b5017a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;2;105;254;105m\n",
      "[2025-12-11 15:28:52]\n",
      "SUCCESS: Loaded soft targets: (357, 15)\u001b[0m\n",
      "\u001b[38;2;161;247;255m\n",
      "[2025-12-11 15:28:52]\n",
      "INFO: Columns: ['image_path', 'Sampling_Date', 'State', 'Species', 'Height_Ave_cm', 'Pre_GSHH_NDVI', 'Dry_Clover_g', 'Dry_Dead_g', 'Dry_Green_g', 'Season', 'strat_group', 'fold', 'Dry_Clover_g_soft', 'Dry_Dead_g_soft', 'Dry_Green_g_soft']\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Load OOF soft targets from Teacher\n",
    "path_soft_targets = './kaggle/input/{type}.csv'\n",
    "path_soft_targets = path_soft_targets.format(type='train_with_oof_soft_targets' if USE_OOF_SOFT_TARGETS else 'train_with_soft_targets')\n",
    "\n",
    "soft_targets_df = pd.read_csv(path_soft_targets)\n",
    "\n",
    "logger.success(f\"Loaded soft targets: {soft_targets_df.shape}\")\n",
    "logger.info(f\"Columns: {soft_targets_df.columns.tolist()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "be47d480",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;2;161;247;255m\n",
      "[2025-12-11 15:28:52]\n",
      "INFO: Soft targets preview:\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "Dry_Clover_g_soft",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Dry_Dead_g_soft",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Dry_Green_g_soft",
         "rawType": "float64",
         "type": "float"
        }
       ],
       "ref": "7226f2e6-1fd4-4c4e-b401-544e35465843",
       "rows": [
        [
         "0",
         "0.36956924",
         "43.185364",
         "20.15381"
        ],
        [
         "1",
         "0.051065993",
         "0.5456077",
         "7.336673"
        ],
        [
         "2",
         "6.852712",
         "0.061617006",
         "0.0"
        ],
        [
         "3",
         "0.5613593",
         "32.825825",
         "19.57612"
        ],
        [
         "4",
         "1.1006649",
         "16.536076",
         "8.233045"
        ]
       ],
       "shape": {
        "columns": 3,
        "rows": 5
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Dry_Clover_g_soft</th>\n",
       "      <th>Dry_Dead_g_soft</th>\n",
       "      <th>Dry_Green_g_soft</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.369569</td>\n",
       "      <td>43.185364</td>\n",
       "      <td>20.153810</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.051066</td>\n",
       "      <td>0.545608</td>\n",
       "      <td>7.336673</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6.852712</td>\n",
       "      <td>0.061617</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.561359</td>\n",
       "      <td>32.825825</td>\n",
       "      <td>19.576120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.100665</td>\n",
       "      <td>16.536076</td>\n",
       "      <td>8.233045</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Dry_Clover_g_soft  Dry_Dead_g_soft  Dry_Green_g_soft\n",
       "0           0.369569        43.185364         20.153810\n",
       "1           0.051066         0.545608          7.336673\n",
       "2           6.852712         0.061617          0.000000\n",
       "3           0.561359        32.825825         19.576120\n",
       "4           1.100665        16.536076          8.233045"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Verify we have soft target columns\n",
    "soft_cols = ['Dry_Clover_g_soft', 'Dry_Dead_g_soft', 'Dry_Green_g_soft']\n",
    "assert all(col in soft_targets_df.columns for col in soft_cols), \"Missing soft target columns!\"\n",
    "\n",
    "logger.info(f\"Soft targets preview:\")\n",
    "display(soft_targets_df[soft_cols].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7dbb541",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "413e7e05",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BiomassStudentDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Student dataset for distillation.\n",
    "    Returns: image + hard targets + soft targets (from Teacher OOF predictions)\n",
    "    NO tabular features!\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        df: pd.DataFrame,\n",
    "        target_cols: list[str],\n",
    "        soft_target_cols: list[str],\n",
    "        img_dir: str,\n",
    "        transform: transforms.Compose | None = None,\n",
    "        is_test: bool = False,\n",
    "        use_log_target: bool = True\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            df: DataFrame with image_path, hard targets, and soft targets\n",
    "            target_cols: List of hard target column names\n",
    "            soft_target_cols: List of soft target column names (from Teacher)\n",
    "            img_dir: Root directory for images\n",
    "            transform: torchvision transform pipeline\n",
    "            is_test: If True, targets are not expected\n",
    "            use_log_target: If True, apply log1p transform to hard targets\n",
    "        \"\"\"\n",
    "        self.df = df.reset_index(drop=True)\n",
    "        self.target_cols = target_cols\n",
    "        self.soft_target_cols = soft_target_cols\n",
    "        self.img_dir = img_dir\n",
    "        self.transform = transform\n",
    "        self.is_test = is_test\n",
    "        self.use_log_target = use_log_target\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx: int) -> dict:\n",
    "        \"\"\"\n",
    "        Returns:\n",
    "            dict with keys:\n",
    "                - 'left_image': tensor [C, H, W]\n",
    "                - 'right_image': tensor [C, H, W]\n",
    "                - 'hard_targets': tensor [3] - real ground truth\n",
    "                - 'soft_targets': tensor [3] - Teacher predictions\n",
    "                - 'image_id': str\n",
    "        \"\"\"\n",
    "        row = self.df.iloc[idx]\n",
    "\n",
    "        # Load image\n",
    "        img_path = os.path.join(self.img_dir, row['image_path'].replace('train/', '').replace('test/', ''))\n",
    "        image = cv2.imread(img_path)\n",
    "\n",
    "        if image is None:\n",
    "            raise FileNotFoundError(f\"Cannot load image: {img_path}\")\n",
    "\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        # Split into left and right patches\n",
    "        h, w, c = image.shape\n",
    "        mid_w = w // 2\n",
    "\n",
    "        left_patch = image[:, :mid_w, :]\n",
    "        right_patch = image[:, mid_w:, :]\n",
    "\n",
    "        # Convert to PIL\n",
    "        left_pil = Image.fromarray(left_patch)\n",
    "        right_pil = Image.fromarray(right_patch)\n",
    "\n",
    "        # Apply transforms\n",
    "        if self.transform:\n",
    "            left_tensor = self.transform(left_pil)\n",
    "            right_tensor = self.transform(right_pil)\n",
    "        else:\n",
    "            left_tensor = transforms.ToTensor()(left_pil)\n",
    "            right_tensor = transforms.ToTensor()(right_pil)\n",
    "\n",
    "        output = {\n",
    "            'left_image': left_tensor,\n",
    "            'right_image': right_tensor,\n",
    "            'image_id': row['image_path'].split('/')[-1].replace('.jpg', '')\n",
    "        }\n",
    "\n",
    "        # Add targets if not test\n",
    "        if not self.is_test:\n",
    "            # Hard targets (ground truth)\n",
    "            hard_targets = row[self.target_cols].values.astype(np.float32)\n",
    "            if self.use_log_target:\n",
    "                hard_targets = np.log1p(hard_targets)\n",
    "\n",
    "            # Soft targets (Teacher predictions - already in original scale!)\n",
    "            soft_targets = row[self.soft_target_cols].values.astype(np.float32)\n",
    "            # Apply log if needed to match hard targets space\n",
    "            if self.use_log_target:\n",
    "                soft_targets = np.log1p(soft_targets)\n",
    "\n",
    "            output['hard_targets'] = torch.tensor(\n",
    "                hard_targets, dtype=torch.float32)\n",
    "            output['soft_targets'] = torch.tensor(\n",
    "                soft_targets, dtype=torch.float32)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "14e0673c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_img_data_stat(df: pd.DataFrame):\n",
    "    \"\"\"Calculate mean and std of image data for normalization.\"\"\"\n",
    "    means = []\n",
    "    stds = []\n",
    "\n",
    "    loader = tqdm(df['image_path'], desc=\"Calculating image stats\")\n",
    "\n",
    "    for img_path in loader:\n",
    "        full_path = os.path.join(PATH_TRAIN_IMG, img_path.replace('train/', '').replace('test/', ''))\n",
    "        image = cv2.imread(full_path)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        image = image / 255.0  # Normalize to [0, 1]\n",
    "\n",
    "        means.append(np.mean(image, axis=(0, 1)))\n",
    "        stds.append(np.std(image, axis=(0, 1)))\n",
    "\n",
    "    mean = np.mean(means, axis=0)\n",
    "    std = np.mean(stds, axis=0)\n",
    "\n",
    "    return mean, std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7b4abf49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_mean, train_std = calculate_img_data_stat(tabular_df)\n",
    "# print(f\"Train Image Mean: {train_mean}, Std: {train_std}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f7f4ee23",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'url': 'https://github.com/SwinTransformer/storage/releases/download/v2.0.0/swinv2_tiny_patch4_window8_256.pth',\n",
       " 'hf_hub_id': 'timm/swinv2_tiny_window8_256.ms_in1k',\n",
       " 'architecture': 'swinv2_tiny_window8_256',\n",
       " 'tag': 'ms_in1k',\n",
       " 'custom_load': False,\n",
       " 'input_size': (3, 256, 256),\n",
       " 'fixed_input_size': True,\n",
       " 'interpolation': 'bicubic',\n",
       " 'crop_pct': 0.9,\n",
       " 'crop_mode': 'center',\n",
       " 'mean': (0.485, 0.456, 0.406),\n",
       " 'std': (0.229, 0.224, 0.225),\n",
       " 'num_classes': 1000,\n",
       " 'pool_size': (8, 8),\n",
       " 'first_conv': 'patch_embed.proj',\n",
       " 'classifier': 'head.fc',\n",
       " 'license': 'mit'}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Image backbone (processes each patch independently)\n",
    "temp_backbone = timm.create_model(\n",
    "    MODEL,\n",
    "    pretrained=True,\n",
    "    num_classes=0,  # remove classification head\n",
    "    global_pool='avg'\n",
    ")\n",
    "\n",
    "temp_backbone.default_cfg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d60de83f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Backbone expected input size: (3, 256, 256), using SIZE=256\n",
      "Backbone expected mean: (0.485, 0.456, 0.406), std: (0.229, 0.224, 0.225)\n"
     ]
    }
   ],
   "source": [
    "inputs_size = temp_backbone.default_cfg['input_size']\n",
    "mean = temp_backbone.default_cfg['mean']\n",
    "std = temp_backbone.default_cfg['std']\n",
    "\n",
    "SIZE = int(inputs_size[1]) if inputs_size is not None and inputs_size[1] == inputs_size[2] else 256\n",
    "print(f\"Backbone expected input size: {inputs_size}, using SIZE={SIZE}\")\n",
    "print(f\"Backbone expected mean: {mean}, std: {std}\")\n",
    "\n",
    "# Get backbone output dimension\n",
    "with torch.no_grad():\n",
    "    try:\n",
    "        dummy = torch.randn(1, 3, SIZE, SIZE)\n",
    "        feat_dim = temp_backbone(dummy).shape[1]\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error getting backbone feature dimension: {e}\")\n",
    "        raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "06c353b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;2;161;247;255m\n",
      "[2025-12-11 15:28:54]\n",
      "INFO: Student augmentations are MORE AGGRESSIVE than Teacher!\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Student needs STRONGER augmentations (he has no metadata!)\n",
    "student_train_transform = transforms.Compose([\n",
    "    transforms.Resize((SIZE, SIZE)),\n",
    "\n",
    "    # Geometric augmentations (stronger)\n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "    transforms.RandomVerticalFlip(p=0.5),\n",
    "    transforms.RandomRotation(degrees=90),\n",
    "    transforms.RandomAffine(degrees=15, translate=(0.1, 0.1), scale=(0.9, 1.1)),\n",
    "\n",
    "    # Color augmentations (stronger)\n",
    "    transforms.RandomApply([\n",
    "        transforms.ColorJitter(brightness=0.4, contrast=0.4, saturation=0.4, hue=0.15)\n",
    "    ], p=0.6),\n",
    "\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=mean, std=std),\n",
    "\n",
    "    # Stronger occlusion simulation\n",
    "    transforms.RandomErasing(p=0.3, scale=(0.02, 0.15), ratio=(0.3, 3.3))\n",
    "])\n",
    "\n",
    "# Validation transform (same as before)\n",
    "student_val_transform = transforms.Compose([\n",
    "    transforms.Resize((SIZE, SIZE)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=mean, std=std)\n",
    "])\n",
    "\n",
    "logger.info(\"Student augmentations are MORE AGGRESSIVE than Teacher!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "cc5ef120",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;2;161;247;255m\n",
      "[2025-12-11 15:28:54]\n",
      "INFO: Testing dataset loading...\u001b[0m\n",
      "Dataset size: 357\n",
      "Sample keys: dict_keys(['left_image', 'right_image', 'image_id', 'hard_targets', 'soft_targets'])\n",
      "Left image: torch.Size([3, 256, 256])\n",
      "Right image: torch.Size([3, 256, 256])\n",
      "Hard targets: tensor([0.0000, 3.4965, 2.8493])\n",
      "Soft targets: tensor([0.3145, 3.7884, 3.0518])\n"
     ]
    }
   ],
   "source": [
    "# Create dataset instance\n",
    "train_dataset = BiomassStudentDataset(\n",
    "    df=soft_targets_df,\n",
    "    target_cols=target_cols,\n",
    "    soft_target_cols=soft_cols,\n",
    "    img_dir=PATH_TRAIN_IMG,\n",
    "    transform=student_train_transform,\n",
    "    use_log_target=USE_LOG_TARGET\n",
    ")\n",
    "\n",
    "# Test loading one sample\n",
    "logger.info(f\"Testing dataset loading...\")\n",
    "sample = train_dataset[0]\n",
    "\n",
    "print(f\"Dataset size: {len(train_dataset)}\")\n",
    "print(f\"Sample keys: {sample.keys()}\")\n",
    "print(f\"Left image: {sample['left_image'].shape}\")\n",
    "print(f\"Right image: {sample['right_image'].shape}\")\n",
    "print(f\"Hard targets: {sample['hard_targets']}\")\n",
    "print(f\"Soft targets: {sample['soft_targets']}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42f322e7",
   "metadata": {},
   "source": [
    "## Spliting Data (StratifiedKFold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "29b90a93",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_season(date_str: str) -> str:\n",
    "    \"\"\"\n",
    "    Convert date string to season.\n",
    "\n",
    "    Args:\n",
    "        date_str: Date in format 'YYYY/M/D' or 'YYYY/MM/DD'\n",
    "\n",
    "    Returns:\n",
    "        Season name: 'Summer', 'Autumn', 'Winter', 'Spring'\n",
    "    \"\"\"\n",
    "    # Parse month from date string\n",
    "    month = int(date_str.split('/')[1])\n",
    "\n",
    "    # Australian seasons (Southern Hemisphere)\n",
    "    if month in [12, 1, 2]:\n",
    "        return 'Summer'\n",
    "    elif month in [3, 4, 5]:\n",
    "        return 'Autumn'\n",
    "    elif month in [6, 7, 8]:\n",
    "        return 'Winter'\n",
    "    else:  # 9, 10, 11\n",
    "        return 'Spring'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d5e023e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique stratification groups:\n",
      "strat_group\n",
      "Spring_Tas_Ryegrass_Clover                                                41\n",
      "Winter_Vic_Phalaris_Clover                                                32\n",
      "Autumn_Tas_Ryegrass                                                       22\n",
      "Winter_Vic_Ryegrass_Clover                                                21\n",
      "Spring_Tas_Clover                                                         21\n",
      "Winter_WA_Clover                                                          20\n",
      "Winter_Tas_Ryegrass_Clover                                                18\n",
      "Autumn_NSW_Lucerne                                                        15\n",
      "Spring_Vic_Ryegrass_Clover                                                11\n",
      "Spring_Vic_Phalaris_BarleyGrass_SilverGrass_SpearGrass_Clover_Capeweed    11\n",
      "Spring_NSW_Fescue                                                         11\n",
      "Summer_NSW_Fescue_CrumbWeed                                               10\n",
      "Spring_Vic_Phalaris_Clover                                                10\n",
      "Winter_Tas_WhiteClover                                                    10\n",
      "Winter_Vic_Ryegrass                                                       10\n",
      "Winter_Tas_Ryegrass                                                       10\n",
      "Spring_Tas_Ryegrass                                                        9\n",
      "Summer_NSW_Fescue                                                          9\n",
      "Summer_NSW_Phalaris                                                        8\n",
      "Autumn_NSW_Fescue                                                          8\n",
      "Winter_Vic_Phalaris_Ryegrass_Clover                                        8\n",
      "Summer_NSW_Lucerne                                                         7\n",
      "Spring_Vic_Phalaris_Clover_Ryegrass_Barleygrass_Bromegrass                 7\n",
      "Autumn_Tas_Ryegrass_Clover                                                 7\n",
      "Summer_NSW_Ryegrass                                                        7\n",
      "Spring_WA_SubcloverLosa                                                    5\n",
      "Spring_WA_Ryegrass                                                         4\n",
      "Spring_WA_SubcloverDalkeith                                                3\n",
      "Winter_Vic_Mixed                                                           2\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Total groups: 29\n"
     ]
    }
   ],
   "source": [
    "# Add season column\n",
    "tabular_df['Season'] = tabular_df['Sampling_Date'].apply(get_season)\n",
    "\n",
    "# Create stratification column combining Season, State, and Species\n",
    "tabular_df['strat_group'] = (\n",
    "    tabular_df['Season'].astype(str) + '_' +\n",
    "    tabular_df['State'].astype(str) + '_' +\n",
    "    tabular_df['Species'].astype(str)\n",
    ")\n",
    "\n",
    "print(\"Unique stratification groups:\")\n",
    "print(tabular_df['strat_group'].value_counts())\n",
    "print(f\"\\nTotal groups: {tabular_df['strat_group'].nunique()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7d5f0fd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Fold 1:\n",
      "  Train samples: 285\n",
      "  Val samples: 72\n",
      "\n",
      "Fold 2:\n",
      "  Train samples: 285\n",
      "  Val samples: 72\n",
      "\n",
      "Fold 3:\n",
      "  Train samples: 286\n",
      "  Val samples: 71\n",
      "\n",
      "Fold 4:\n",
      "  Train samples: 286\n",
      "  Val samples: 71\n",
      "\n",
      "Fold 5:\n",
      "  Train samples: 286\n",
      "  Val samples: 71\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\_GitHub\\CSIRO-Image2Biomass-Prediction\\.venv\\Lib\\site-packages\\sklearn\\model_selection\\_split.py:813: UserWarning: The least populated class in y has only 2 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Initialize StratifiedKFold\n",
    "n_folds = 5\n",
    "skf = StratifiedKFold(n_splits=n_folds, shuffle=True, random_state=42)\n",
    "\n",
    "# Get stratification labels\n",
    "strat_labels = tabular_df['strat_group'].values\n",
    "\n",
    "# Create fold assignments\n",
    "tabular_df['fold'] = -1\n",
    "\n",
    "for fold_idx, (train_idx, val_idx) in enumerate(skf.split(tabular_df, strat_labels)):\n",
    "    tabular_df.loc[val_idx, 'fold'] = fold_idx\n",
    "\n",
    "    print(f\"\\nFold {fold_idx + 1}:\")\n",
    "    print(f\"  Train samples: {len(train_idx)}\")\n",
    "    print(f\"  Val samples: {len(val_idx)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "20130d85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stratification verification:\n",
      "\n",
      "Fold 1:\n",
      "  Season distribution:\n",
      "Season\n",
      "Winter    0.361\n",
      "Spring    0.347\n",
      "Autumn    0.167\n",
      "Summer    0.125\n",
      "Name: proportion, dtype: float64\n",
      "  State distribution:\n",
      "State\n",
      "Tas    0.389\n",
      "Vic    0.319\n",
      "NSW    0.222\n",
      "WA     0.069\n",
      "Name: proportion, dtype: float64\n",
      "  Species distribution:\n",
      "Species\n",
      "Ryegrass_Clover                                                0.292\n",
      "Ryegrass                                                       0.167\n",
      "Phalaris_Clover                                                0.111\n",
      "Clover                                                         0.111\n",
      "Fescue                                                         0.083\n",
      "Lucerne                                                        0.056\n",
      "Fescue_CrumbWeed                                               0.028\n",
      "Phalaris                                                       0.028\n",
      "WhiteClover                                                    0.028\n",
      "Phalaris_Clover_Ryegrass_Barleygrass_Bromegrass                0.028\n",
      "Phalaris_BarleyGrass_SilverGrass_SpearGrass_Clover_Capeweed    0.028\n",
      "SubcloverLosa                                                  0.014\n",
      "Phalaris_Ryegrass_Clover                                       0.014\n",
      "Mixed                                                          0.014\n",
      "Name: proportion, dtype: float64\n",
      "\n",
      "Fold 2:\n",
      "  Season distribution:\n",
      "Season\n",
      "Spring    0.389\n",
      "Winter    0.375\n",
      "Autumn    0.125\n",
      "Summer    0.111\n",
      "Name: proportion, dtype: float64\n",
      "  State distribution:\n",
      "State\n",
      "Tas    0.375\n",
      "Vic    0.333\n",
      "NSW    0.194\n",
      "WA     0.097\n",
      "Name: proportion, dtype: float64\n",
      "  Species distribution:\n",
      "Species\n",
      "Ryegrass_Clover                                                0.278\n",
      "Ryegrass                                                       0.167\n",
      "Phalaris_Clover                                                0.125\n",
      "Clover                                                         0.111\n",
      "Fescue                                                         0.069\n",
      "Lucerne                                                        0.056\n",
      "Phalaris_BarleyGrass_SilverGrass_SpearGrass_Clover_Capeweed    0.042\n",
      "Fescue_CrumbWeed                                               0.028\n",
      "Phalaris                                                       0.028\n",
      "WhiteClover                                                    0.028\n",
      "SubcloverDalkeith                                              0.014\n",
      "Phalaris_Clover_Ryegrass_Barleygrass_Bromegrass                0.014\n",
      "Phalaris_Ryegrass_Clover                                       0.014\n",
      "Mixed                                                          0.014\n",
      "SubcloverLosa                                                  0.014\n",
      "Name: proportion, dtype: float64\n",
      "\n",
      "Fold 3:\n",
      "  Season distribution:\n",
      "Season\n",
      "Spring    0.380\n",
      "Winter    0.380\n",
      "Autumn    0.127\n",
      "Summer    0.113\n",
      "Name: proportion, dtype: float64\n",
      "  State distribution:\n",
      "State\n",
      "Tas    0.380\n",
      "Vic    0.310\n",
      "NSW    0.211\n",
      "WA     0.099\n",
      "Name: proportion, dtype: float64\n",
      "  Species distribution:\n",
      "Species\n",
      "Ryegrass_Clover                                                0.268\n",
      "Ryegrass                                                       0.169\n",
      "Phalaris_Clover                                                0.127\n",
      "Clover                                                         0.113\n",
      "Fescue                                                         0.085\n",
      "Lucerne                                                        0.056\n",
      "WhiteClover                                                    0.028\n",
      "Phalaris_BarleyGrass_SilverGrass_SpearGrass_Clover_Capeweed    0.028\n",
      "Phalaris                                                       0.028\n",
      "Fescue_CrumbWeed                                               0.028\n",
      "Phalaris_Ryegrass_Clover                                       0.028\n",
      "Phalaris_Clover_Ryegrass_Barleygrass_Bromegrass                0.014\n",
      "SubcloverDalkeith                                              0.014\n",
      "SubcloverLosa                                                  0.014\n",
      "Name: proportion, dtype: float64\n",
      "\n",
      "Fold 4:\n",
      "  Season distribution:\n",
      "Season\n",
      "Spring    0.380\n",
      "Winter    0.366\n",
      "Autumn    0.141\n",
      "Summer    0.113\n",
      "Name: proportion, dtype: float64\n",
      "  State distribution:\n",
      "State\n",
      "Tas    0.394\n",
      "Vic    0.296\n",
      "NSW    0.211\n",
      "WA     0.099\n",
      "Name: proportion, dtype: float64\n",
      "  Species distribution:\n",
      "Species\n",
      "Ryegrass_Clover                                                0.268\n",
      "Ryegrass                                                       0.169\n",
      "Clover                                                         0.127\n",
      "Phalaris_Clover                                                0.113\n",
      "Fescue                                                         0.085\n",
      "Lucerne                                                        0.070\n",
      "Phalaris_Ryegrass_Clover                                       0.028\n",
      "WhiteClover                                                    0.028\n",
      "Fescue_CrumbWeed                                               0.028\n",
      "Phalaris_BarleyGrass_SilverGrass_SpearGrass_Clover_Capeweed    0.028\n",
      "Phalaris                                                       0.014\n",
      "Phalaris_Clover_Ryegrass_Barleygrass_Bromegrass                0.014\n",
      "SubcloverLosa                                                  0.014\n",
      "SubcloverDalkeith                                              0.014\n",
      "Name: proportion, dtype: float64\n",
      "\n",
      "Fold 5:\n",
      "  Season distribution:\n",
      "Season\n",
      "Spring    0.366\n",
      "Winter    0.352\n",
      "Autumn    0.169\n",
      "Summer    0.113\n",
      "Name: proportion, dtype: float64\n",
      "  State distribution:\n",
      "State\n",
      "Tas    0.394\n",
      "Vic    0.310\n",
      "NSW    0.211\n",
      "WA     0.085\n",
      "Name: proportion, dtype: float64\n",
      "  Species distribution:\n",
      "Species\n",
      "Ryegrass_Clover                                                0.268\n",
      "Ryegrass                                                       0.197\n",
      "Clover                                                         0.113\n",
      "Phalaris_Clover                                                0.113\n",
      "Fescue                                                         0.070\n",
      "Lucerne                                                        0.070\n",
      "WhiteClover                                                    0.028\n",
      "Phalaris_Ryegrass_Clover                                       0.028\n",
      "Fescue_CrumbWeed                                               0.028\n",
      "Phalaris_Clover_Ryegrass_Barleygrass_Bromegrass                0.028\n",
      "Phalaris_BarleyGrass_SilverGrass_SpearGrass_Clover_Capeweed    0.028\n",
      "Phalaris                                                       0.014\n",
      "SubcloverLosa                                                  0.014\n",
      "Name: proportion, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Verify stratification worked\n",
    "print(\"Stratification verification:\")\n",
    "\n",
    "for fold in range(n_folds):\n",
    "    fold_df = tabular_df[tabular_df['fold'] == fold]\n",
    "    print(f\"\\nFold {fold + 1}:\")\n",
    "    print(f\"  Season distribution:\")\n",
    "    print(fold_df['Season'].value_counts(normalize=True).round(3))\n",
    "    print(f\"  State distribution:\")\n",
    "    print(fold_df['State'].value_counts(normalize=True).round(3))\n",
    "    print(f\"  Species distribution:\")\n",
    "    print(fold_df['Species'].value_counts(normalize=True).round(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7cea12fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_fold_data(df: pd.DataFrame, fold: int):\n",
    "    \"\"\"\n",
    "    Get train/val split for specific fold.\n",
    "\n",
    "    Args:\n",
    "        df: DataFrame with 'fold' column\n",
    "        fold: Fold index to use as validation\n",
    "\n",
    "    Returns:\n",
    "        train_df, val_df\n",
    "    \"\"\"\n",
    "    train_df = df[df['fold'] != fold].reset_index(drop=True)\n",
    "    val_df = df[df['fold'] == fold].reset_index(drop=True)\n",
    "\n",
    "    return train_df, val_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "25e99df9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_loaders(fold: int, bs: int, soft_df: pd.DataFrame) -> tuple[DataLoader, DataLoader]:\n",
    "    \"\"\"\n",
    "    Get dataloaders for Student model (no tabular features needed!)\n",
    "\n",
    "    Args:\n",
    "        fold: Fold index to use as validation\n",
    "        bs: Batch size\n",
    "        soft_df: DataFrame with soft targets from Teacher\n",
    "    \"\"\"\n",
    "    train_df = soft_df[soft_df['fold'] != fold].reset_index(drop=True)\n",
    "    val_df = soft_df[soft_df['fold'] == fold].reset_index(drop=True)\n",
    "\n",
    "    logger.info(f\"Student training fold {fold}:\")\n",
    "    logger.info(f\"  Train size: {len(train_df)}\")\n",
    "    logger.info(f\"  Val size: {len(val_df)}\")\n",
    "\n",
    "    # Soft target columns\n",
    "    soft_target_cols = ['Dry_Clover_g_soft', 'Dry_Dead_g_soft', 'Dry_Green_g_soft']\n",
    "\n",
    "    # Create datasets\n",
    "    train_dataset = BiomassStudentDataset(\n",
    "        df=train_df,\n",
    "        target_cols=target_cols,\n",
    "        soft_target_cols=soft_target_cols,\n",
    "        img_dir=PATH_TRAIN_IMG,\n",
    "        transform=student_train_transform,\n",
    "        is_test=False,\n",
    "        use_log_target=USE_LOG_TARGET\n",
    "    )\n",
    "\n",
    "    val_dataset = BiomassStudentDataset(\n",
    "        df=val_df,\n",
    "        target_cols=target_cols,\n",
    "        soft_target_cols=soft_target_cols,\n",
    "        img_dir=PATH_TRAIN_IMG,\n",
    "        transform=student_val_transform,\n",
    "        is_test=False,\n",
    "        use_log_target=USE_LOG_TARGET\n",
    "    )\n",
    "\n",
    "    # Create dataloaders\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=bs,\n",
    "        shuffle=True,\n",
    "        num_workers=min(NUM_WORKERS, 8),\n",
    "        pin_memory=True if torch.cuda.is_available() else False,\n",
    "        persistent_workers=True if NUM_WORKERS > 0 else False\n",
    "    )\n",
    "\n",
    "    val_loader = DataLoader(\n",
    "        val_dataset,\n",
    "        batch_size=bs * 2,\n",
    "        shuffle=False,\n",
    "        num_workers=min(NUM_WORKERS, 8),\n",
    "        pin_memory=True if torch.cuda.is_available() else False,\n",
    "        persistent_workers=True if NUM_WORKERS > 0 else False\n",
    "    )\n",
    "\n",
    "    logger.info(f\"Train batches: {len(train_loader)}\")\n",
    "    logger.info(f\"Val batches: {len(val_loader)}\")\n",
    "\n",
    "    return train_loader, val_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "9205264a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;2;161;247;255m\n",
      "[2025-12-11 15:32:14]\n",
      "INFO: Testing Student Dataset...\u001b[0m\n",
      "\u001b[38;2;161;247;255m\n",
      "[2025-12-11 15:32:14]\n",
      "INFO: Student training fold 0:\u001b[0m\n",
      "\u001b[38;2;161;247;255m\n",
      "[2025-12-11 15:32:14]\n",
      "INFO:   Train size: 285\u001b[0m\n",
      "\u001b[38;2;161;247;255m\n",
      "[2025-12-11 15:32:14]\n",
      "INFO:   Val size: 72\u001b[0m\n",
      "\u001b[38;2;161;247;255m\n",
      "[2025-12-11 15:32:14]\n",
      "INFO: Train batches: 18\u001b[0m\n",
      "\u001b[38;2;161;247;255m\n",
      "[2025-12-11 15:32:14]\n",
      "INFO: Val batches: 3\u001b[0m\n",
      "Left image shape: torch.Size([16, 3, 256, 256])\n",
      "Right image shape: torch.Size([16, 3, 256, 256])\n",
      "Hard targets shape: torch.Size([16, 3])\n",
      "Soft targets shape: torch.Size([16, 3])\n",
      "\u001b[38;2;105;254;105m\n",
      "[2025-12-11 15:32:15]\n",
      "SUCCESS: Student dataset works perfectly!\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Test student dataset\n",
    "logger.info(\"Testing Student Dataset...\")\n",
    "\n",
    "test_train_loader, test_val_loader = get_loaders(fold=0, bs=BATCH_SIZE, soft_df=soft_targets_df)\n",
    "\n",
    "sample_batch = next(iter(test_train_loader))\n",
    "\n",
    "print(f\"Left image shape: {sample_batch['left_image'].shape}\")\n",
    "print(f\"Right image shape: {sample_batch['right_image'].shape}\")\n",
    "print(f\"Hard targets shape: {sample_batch['hard_targets'].shape}\")\n",
    "print(f\"Soft targets shape: {sample_batch['soft_targets'].shape}\")\n",
    "\n",
    "logger.success(\"Student dataset works perfectly!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53fd54d9",
   "metadata": {},
   "source": [
    "## Ligtning Module"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b284bcb6",
   "metadata": {},
   "source": [
    "### Custom Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "0f7a48a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = [\n",
    "    \"Dry_Clover_g\",\n",
    "    \"Dry_Dead_g\",\n",
    "    \"Dry_Green_g\",\n",
    "    \"Dry_Total_g\",\n",
    "    \"GDM_g\"\n",
    "]\n",
    "\n",
    "weights = {\n",
    "    'Dry_Green_g': 0.1,\n",
    "    'Dry_Dead_g': 0.1,\n",
    "    'Dry_Clover_g': 0.1,\n",
    "    'GDM_g': 0.2,\n",
    "    'Dry_Total_g': 0.5,\n",
    "}\n",
    "\n",
    "\n",
    "def competition_metric(y_true, y_pred) -> float:\n",
    "    \"\"\"Function to calculate the competition's official evaluation metric (weighted R2 score).\"\"\"\n",
    "    weights_array = np.array([weights[l] for l in labels])\n",
    "\n",
    "    # Align with this calculation method\n",
    "    y_weighted_mean = np.average(y_true, weights=weights_array, axis=1).mean()\n",
    "\n",
    "    # For ss_res and ss_tot, also take the weighted average on axis=1, then the mean of the result\n",
    "    ss_res = np.average((y_true - y_pred)**2,\n",
    "                        weights=weights_array, axis=1).mean()\n",
    "    ss_tot = np.average((y_true - y_weighted_mean)**2,\n",
    "                        weights=weights_array, axis=1).mean()\n",
    "\n",
    "    return 1 - ss_res / ss_tot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e5a9bc56",
   "metadata": {},
   "outputs": [],
   "source": [
    "class StudentDistillationLoss(nn.Module):\n",
    "    \"\"\"\n",
    "    Custom loss for Student model combining:\n",
    "    1. Distillation loss (learn from Teacher)\n",
    "    2. Hard loss (learn from real targets with competition weights)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, alpha: float = 0.5, use_log_space: bool = True):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            alpha: Weight for distillation loss (0.5 = equal weight to Teacher and ground truth)\n",
    "            use_log_space: If True, compute loss in log space\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.alpha = alpha\n",
    "        self.use_log_space = use_log_space\n",
    "\n",
    "        # Competition weights\n",
    "        self.w_green = 0.1\n",
    "        self.w_clover = 0.1\n",
    "        self.w_dead = 0.1\n",
    "        self.w_gdm = 0.2\n",
    "        self.w_total = 0.5\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        student_preds: torch.Tensor,  # [B, 3] predictions in log space\n",
    "        hard_targets: torch.Tensor,   # [B, 3] ground truth in log space\n",
    "        soft_targets: torch.Tensor    # [B, 3] Teacher predictions in log space\n",
    "    ) -> tuple[torch.Tensor, dict]:\n",
    "        \"\"\"\n",
    "        Returns:\n",
    "            total_loss: Combined loss\n",
    "            loss_dict: Dictionary with individual loss components\n",
    "        \"\"\"\n",
    "\n",
    "        # 1. Distillation Loss (MSE with Teacher's soft targets)\n",
    "        loss_distill = F.mse_loss(student_preds, soft_targets)\n",
    "\n",
    "        # 2. Hard Loss with competition weights\n",
    "        # Individual components\n",
    "        loss_green = F.mse_loss(\n",
    "            student_preds[:, 2], hard_targets[:, 2])   # Dry_Green_g\n",
    "        loss_clover = F.mse_loss(\n",
    "            student_preds[:, 0], hard_targets[:, 0])  # Dry_Clover_g\n",
    "        loss_dead = F.mse_loss(\n",
    "            student_preds[:, 1], hard_targets[:, 1])    # Dry_Dead_g\n",
    "\n",
    "        # Derived targets (computed from components)\n",
    "        # Dry_Total_g = sum of all 3 components\n",
    "        student_total = student_preds.sum(dim=1)\n",
    "        hard_total = hard_targets.sum(dim=1)\n",
    "        loss_total = F.mse_loss(student_total, hard_total)\n",
    "\n",
    "        # GDM_g = Clover + Green\n",
    "        student_gdm = student_preds[:, 0] + \\\n",
    "            student_preds[:, 2]  # Clover + Green\n",
    "        hard_gdm = hard_targets[:, 0] + hard_targets[:, 2]\n",
    "        loss_gdm = F.mse_loss(student_gdm, hard_gdm)\n",
    "\n",
    "        # Weighted hard loss (following competition metric weights)\n",
    "        loss_hard = (\n",
    "            self.w_green * loss_green +\n",
    "            self.w_clover * loss_clover +\n",
    "            self.w_dead * loss_dead +\n",
    "            self.w_gdm * loss_gdm +\n",
    "            self.w_total * loss_total\n",
    "        )\n",
    "\n",
    "        # 3. Total loss (weighted combination)\n",
    "        total_loss = self.alpha * loss_distill + (1 - self.alpha) * loss_hard\n",
    "\n",
    "        # Return loss dict for logging\n",
    "        loss_dict = {\n",
    "            'loss_distill': loss_distill.item(),\n",
    "            'loss_hard': loss_hard.item(),\n",
    "            'loss_green': loss_green.item(),\n",
    "            'loss_clover': loss_clover.item(),\n",
    "            'loss_dead': loss_dead.item(),\n",
    "            'loss_total': loss_total.item(),\n",
    "            'loss_gdm': loss_gdm.item(),\n",
    "        }\n",
    "\n",
    "        return total_loss, loss_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "9e8b2e58",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BiomassStudentModel(pl.LightningModule):\n",
    "    \"\"\"\n",
    "    Student model for biomass prediction.\n",
    "    Uses ONLY images (dual-patch), NO tabular features.\n",
    "    Learns from Teacher's soft targets + ground truth.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        backbone_name: str = 'swinv2_tiny_window8_256',\n",
    "        num_targets: int = 3,\n",
    "        lr: float = 1e-4,\n",
    "        weight_decay: float = 1e-5,\n",
    "        hidden_ratio: float = 0.5,\n",
    "        dropout: float = 0.3,  # Higher dropout for student\n",
    "        fusion_method: str = 'mean',\n",
    "        distill_alpha: float = 0.5,  # Weight for distillation loss\n",
    "        use_log_target: bool = True\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "\n",
    "        # Image backbone (same as Teacher)\n",
    "        self.backbone = timm.create_model(\n",
    "            backbone_name,\n",
    "            pretrained=True,\n",
    "            num_classes=0,\n",
    "            global_pool='avg'\n",
    "        )\n",
    "\n",
    "        self.lr = lr\n",
    "        self.weight_decay = weight_decay\n",
    "        self.fusion_method = fusion_method\n",
    "        self.use_log_target = use_log_target\n",
    "\n",
    "        # Get backbone output dimension\n",
    "        with torch.no_grad():\n",
    "            dummy = torch.randn(1, 3, SIZE, SIZE)\n",
    "            feat_dim = self.backbone(dummy).shape[1]\n",
    "\n",
    "        self.feat_dim = feat_dim\n",
    "\n",
    "        # NO tabular features - only image features!\n",
    "        if self.fusion_method == 'concat':\n",
    "            self.combined_dim = feat_dim * 2\n",
    "        else:  # mean or max\n",
    "            self.combined_dim = feat_dim\n",
    "\n",
    "        # Regression heads (simpler than Teacher)\n",
    "        hidden_size = max(32, int(self.combined_dim * hidden_ratio))\n",
    "\n",
    "        def make_head():\n",
    "            return nn.Sequential(\n",
    "                nn.Linear(self.combined_dim, hidden_size),\n",
    "                nn.ReLU(inplace=True),\n",
    "                nn.Dropout(dropout),\n",
    "                nn.Linear(hidden_size, 1)\n",
    "            )\n",
    "\n",
    "        self.head_green = make_head()\n",
    "        self.head_clover = make_head()\n",
    "        self.head_dead = make_head()\n",
    "\n",
    "        # Custom distillation loss\n",
    "        self.criterion = StudentDistillationLoss(\n",
    "            alpha=distill_alpha,\n",
    "            use_log_space=use_log_target\n",
    "        )\n",
    "\n",
    "        # Storage for validation\n",
    "        self.validation_step_outputs = []\n",
    "\n",
    "        logger.info(f\"Student model initialized: backbone={backbone_name}, feat_dim={feat_dim}, \"\n",
    "                    f\"combined_dim={self.combined_dim}, fusion={fusion_method}, \"\n",
    "                    f\"distill_alpha={distill_alpha}\")\n",
    "\n",
    "    def forward(self, batch: dict) -> tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            batch: dict with 'left_image', 'right_image'\n",
    "\n",
    "        Returns:\n",
    "            (green, clover, dead) predictions\n",
    "        \"\"\"\n",
    "        # Extract features from each patch\n",
    "        left_feat = self.backbone(batch['left_image'])\n",
    "        right_feat = self.backbone(batch['right_image'])\n",
    "\n",
    "        # Fuse image features\n",
    "        if self.fusion_method == 'concat':\n",
    "            img_feat = torch.cat([left_feat, right_feat], dim=1)\n",
    "        elif self.fusion_method == 'mean':\n",
    "            img_feat = (left_feat + right_feat) / 2\n",
    "        elif self.fusion_method == 'max':\n",
    "            img_feat = torch.maximum(left_feat, right_feat)\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown fusion method: {self.fusion_method}\")\n",
    "\n",
    "        # Predict each target\n",
    "        green = self.head_green(img_feat).squeeze(1)\n",
    "        clover = self.head_clover(img_feat).squeeze(1)\n",
    "        dead = self.head_dead(img_feat).squeeze(1)\n",
    "\n",
    "        return green, clover, dead\n",
    "\n",
    "    def compute_all_targets(self, green: torch.Tensor, clover: torch.Tensor, dead: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Compute all 5 targets from 3 predicted ones\"\"\"\n",
    "        green = torch.clamp(green, min=0.0)\n",
    "        clover = torch.clamp(clover, min=0.0)\n",
    "        dead = torch.clamp(dead, min=0.0)\n",
    "\n",
    "        total = green + dead + clover\n",
    "        gdm = clover + green\n",
    "\n",
    "        all_targets = torch.stack([clover, dead, green, total, gdm], dim=1)\n",
    "        return all_targets\n",
    "\n",
    "    def training_step(self, batch: dict, batch_idx: int) -> torch.Tensor:\n",
    "        green, clover, dead = self(batch)\n",
    "\n",
    "        # Stack predictions [B, 3] in order: [clover, dead, green]\n",
    "        preds = torch.stack([clover, dead, green], dim=1)\n",
    "\n",
    "        # Compute distillation loss\n",
    "        loss, loss_dict = self.criterion(\n",
    "            preds,\n",
    "            batch['hard_targets'],\n",
    "            batch['soft_targets']\n",
    "        )\n",
    "\n",
    "        # Log all loss components\n",
    "        self.log('train_loss', loss, on_step=True, on_epoch=True, prog_bar=True,\n",
    "                 batch_size=batch['hard_targets'].size(0))\n",
    "        self.log('train_loss_distill',\n",
    "                 loss_dict['loss_distill'], on_step=True, on_epoch=True)\n",
    "        self.log('train_loss_hard',\n",
    "                 loss_dict['loss_hard'], on_step=True, on_epoch=True)\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch: dict, batch_idx: int) -> torch.Tensor:\n",
    "        green_pred, clover_pred, dead_pred = self(batch)\n",
    "\n",
    "        preds = torch.stack([clover_pred, dead_pred, green_pred], dim=1)\n",
    "\n",
    "        # Compute loss\n",
    "        loss, loss_dict = self.criterion(\n",
    "            preds,\n",
    "            batch['hard_targets'],\n",
    "            batch['soft_targets']\n",
    "        )\n",
    "\n",
    "        self.log('val_loss', loss, on_step=False, on_epoch=True, prog_bar=True,\n",
    "                 batch_size=batch['hard_targets'].size(0))\n",
    "        self.log('val_loss_distill',\n",
    "                 loss_dict['loss_distill'], on_step=False, on_epoch=True)\n",
    "        self.log('val_loss_hard',\n",
    "                 loss_dict['loss_hard'], on_step=False, on_epoch=True)\n",
    "\n",
    "        # Convert to original scale for metric\n",
    "        if self.use_log_target:\n",
    "            green_pred = torch.expm1(green_pred)\n",
    "            clover_pred = torch.expm1(clover_pred)\n",
    "            dead_pred = torch.expm1(dead_pred)\n",
    "\n",
    "            hard_targets_original = torch.expm1(batch['hard_targets'])\n",
    "        else:\n",
    "            hard_targets_original = batch['hard_targets']\n",
    "\n",
    "        # Compute all 5 targets\n",
    "        preds_all = self.compute_all_targets(\n",
    "            green_pred, clover_pred, dead_pred)\n",
    "\n",
    "        clover_true = hard_targets_original[:, 0]\n",
    "        dead_true = hard_targets_original[:, 1]\n",
    "        green_true = hard_targets_original[:, 2]\n",
    "        targets_all = self.compute_all_targets(\n",
    "            green_true, clover_true, dead_true)\n",
    "\n",
    "        self.validation_step_outputs.append({\n",
    "            'preds': preds_all.detach().cpu(),\n",
    "            'targets': targets_all.detach().cpu()\n",
    "        })\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def on_validation_epoch_end(self):\n",
    "        if len(self.validation_step_outputs) == 0:\n",
    "            return\n",
    "\n",
    "        all_preds = torch.cat(\n",
    "            [x['preds'] for x in self.validation_step_outputs], dim=0).numpy()\n",
    "        all_targets = torch.cat(\n",
    "            [x['targets'] for x in self.validation_step_outputs], dim=0).numpy()\n",
    "\n",
    "        comp_metric = competition_metric(all_targets, all_preds)\n",
    "        self.log('val_comp_metric', comp_metric, on_epoch=True, prog_bar=True)\n",
    "\n",
    "        self.validation_step_outputs.clear()\n",
    "\n",
    "    def predict_step(self, batch: dict, batch_idx: int) -> torch.Tensor:\n",
    "        green, clover, dead = self(batch)\n",
    "        preds = torch.stack([clover, dead, green], dim=1)\n",
    "\n",
    "        if self.use_log_target:\n",
    "            preds = torch.expm1(preds)\n",
    "\n",
    "        preds = torch.clamp(preds, min=0.0)\n",
    "        return preds\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = AdamW(\n",
    "            self.parameters(),\n",
    "            lr=self.lr,\n",
    "            weight_decay=self.weight_decay\n",
    "        )\n",
    "\n",
    "        scheduler = CosineAnnealingLR(\n",
    "            optimizer,\n",
    "            T_max=self.trainer.max_epochs or 20,\n",
    "            eta_min=self.lr * 0.01\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            'optimizer': optimizer,\n",
    "            'lr_scheduler': {\n",
    "                'scheduler': scheduler,\n",
    "                'interval': 'epoch'\n",
    "            }\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b85bdfb",
   "metadata": {},
   "source": [
    "## Folds Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd59f4b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;2;161;247;255m\n",
      "[2025-12-11 15:33:44]\n",
      "INFO: Student training fold 0:\u001b[0m\n",
      "\u001b[38;2;161;247;255m\n",
      "[2025-12-11 15:33:44]\n",
      "INFO:   Train size: 285\u001b[0m\n",
      "\u001b[38;2;161;247;255m\n",
      "[2025-12-11 15:33:44]\n",
      "INFO:   Val size: 72\u001b[0m\n",
      "\u001b[38;2;161;247;255m\n",
      "[2025-12-11 15:33:44]\n",
      "INFO: Train batches: 18\u001b[0m\n",
      "\u001b[38;2;161;247;255m\n",
      "[2025-12-11 15:33:44]\n",
      "INFO: Val batches: 3\u001b[0m\n",
      "\u001b[38;2;161;247;255m\n",
      "[2025-12-11 15:33:45]\n",
      "INFO: Student model initialized: backbone=swinv2_tiny_window8_256, feat_dim=768, combined_dim=768, fusion=mean, distill_alpha=0.5\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using 16bit Automatic Mixed Precision (AMP)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;2;161;247;255m\n",
      "[2025-12-11 15:33:45]\n",
      "INFO: \n",
      "Training Student on Fold 0...\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The anonymous setting has no effect and will be removed in a future version.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mdmykhailov\u001b[0m (\u001b[33mdmykhailov-kyiv-school-of-economics\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.23.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>wandb\\run-20251211_153346-govsnwhb</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/dmykhailov-kyiv-school-of-economics/csiro-image2biomass-prediction/runs/govsnwhb' target=\"_blank\">swinv2_tiny_window8_256-local_train[5]Folds_log_fusion-mean_epochs25_bs16_gradacc1_lr0.0001_wd0.05_dr0.3_hr0.5-student-fold0</a></strong> to <a href='https://wandb.ai/dmykhailov-kyiv-school-of-economics/csiro-image2biomass-prediction' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/dmykhailov-kyiv-school-of-economics/csiro-image2biomass-prediction' target=\"_blank\">https://wandb.ai/dmykhailov-kyiv-school-of-economics/csiro-image2biomass-prediction</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/dmykhailov-kyiv-school-of-economics/csiro-image2biomass-prediction/runs/govsnwhb' target=\"_blank\">https://wandb.ai/dmykhailov-kyiv-school-of-economics/csiro-image2biomass-prediction/runs/govsnwhb</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "c:\\_GitHub\\CSIRO-Image2Biomass-Prediction\\.venv\\Lib\\site-packages\\pytorch_lightning\\utilities\\model_summary\\model_summary.py:242: Precision 16-mixed is not supported by the model summary.  Estimated model size in MB will not be accurate. Using 32 bits instead.\n",
      "\n",
      "  | Name        | Type                    | Params | Mode  | FLOPs\n",
      "------------------------------------------------------------------------\n",
      "0 | backbone    | SwinTransformerV2       | 27.6 M | train | 0    \n",
      "1 | head_green  | Sequential              | 295 K  | train | 0    \n",
      "2 | head_clover | Sequential              | 295 K  | train | 0    \n",
      "3 | head_dead   | Sequential              | 295 K  | train | 0    \n",
      "4 | criterion   | StudentDistillationLoss | 0      | train | 0    \n",
      "------------------------------------------------------------------------\n",
      "28.5 M    Trainable params\n",
      "0         Non-trainable params\n",
      "28.5 M    Total params\n",
      "113.861   Total estimated model params size (MB)\n",
      "311       Modules in train mode\n",
      "0         Modules in eval mode\n",
      "0         Total Flops\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e1c782bde596449d926baa9cca6394b7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\_GitHub\\CSIRO-Image2Biomass-Prediction\\.venv\\Lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:434: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=19` in the `DataLoader` to improve performance.\n",
      "c:\\_GitHub\\CSIRO-Image2Biomass-Prediction\\.venv\\Lib\\site-packages\\pytorch_lightning\\utilities\\data.py:79: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 32. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.\n",
      "c:\\_GitHub\\CSIRO-Image2Biomass-Prediction\\.venv\\Lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:434: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=19` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4fede073130446e2be763e2ff15344f1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\_GitHub\\CSIRO-Image2Biomass-Prediction\\.venv\\Lib\\site-packages\\pytorch_lightning\\utilities\\data.py:79: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 16. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.\n",
      "c:\\_GitHub\\CSIRO-Image2Biomass-Prediction\\.venv\\Lib\\site-packages\\pytorch_lightning\\utilities\\data.py:79: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 13. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e7b7bba7566c4faeb375c6b54b59ad1f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\_GitHub\\CSIRO-Image2Biomass-Prediction\\.venv\\Lib\\site-packages\\pytorch_lightning\\utilities\\data.py:79: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 8. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.\n",
      "Metric val_loss improved. New best score: 1.546\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2b8dcc3513854dc3a362dad94252e2c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.459 >= min_delta = 0.0. New best score: 1.087\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3ad6581691a144fca43dbd3c1e588640",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aec3fa4dc8de4549a4108fa7bf2ac145",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.139 >= min_delta = 0.0. New best score: 0.948\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e5cf20e9582048d4ad9f4b41e509e5c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.277 >= min_delta = 0.0. New best score: 0.671\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "32cf7e63bc0749c08d8e5991a565608a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f8bd8ad8a4eb4c13846ca46d65f77c3c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b835de5cd69543f3a6cfa509c739ed77",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.012 >= min_delta = 0.0. New best score: 0.659\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "58902495c7294fb39aebeafd9fb64356",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9865f8ee45844622bef850e8b8a105ea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.068 >= min_delta = 0.0. New best score: 0.592\n"
     ]
    }
   ],
   "source": [
    "student_fold_results = []\n",
    "\n",
    "for fold_id in range(N_FOLDS):\n",
    "    train_loader, val_loader = get_loaders(\n",
    "        fold=fold_id, bs=BATCH_SIZE, soft_df=soft_targets_df)\n",
    "\n",
    "    model = BiomassStudentModel(\n",
    "        backbone_name=MODEL,\n",
    "        num_targets=len(target_cols),\n",
    "        lr=LR,\n",
    "        weight_decay=WEIGHT_DECAY,\n",
    "        hidden_ratio=HIDDEN_RATIO,\n",
    "        dropout=DROPOUT_RATE,\n",
    "        fusion_method=FUSION_METHOD,\n",
    "        distill_alpha=DISTILL_ALPHA,\n",
    "        use_log_target=USE_LOG_TARGET\n",
    "    )\n",
    "\n",
    "    # Callbacks\n",
    "    checkpoint_callback = ModelCheckpoint(\n",
    "        monitor='val_loss',\n",
    "        dirpath=os.path.join(CHECKPOINTS_DIR, f'fold{fold_id}'),\n",
    "        filename=f'{DESCRIPTION_FULL}-student-fold{fold_id}' +\n",
    "        '-{epoch:02d}-{val_loss:.4f}',\n",
    "        save_top_k=3,\n",
    "        mode='min'\n",
    "    )\n",
    "\n",
    "    early_stopping_callback = EarlyStopping(\n",
    "        monitor='val_loss',\n",
    "        patience=7,  # More patience for student\n",
    "        mode='min',\n",
    "        verbose=True\n",
    "    )\n",
    "\n",
    "    lr_monitor = LearningRateMonitor(logging_interval='epoch')\n",
    "\n",
    "    # Logger\n",
    "    wandb_logger = WandbLogger(\n",
    "        project=PROJECT_NAME,\n",
    "        name=f'{DESCRIPTION_FULL}-student-fold{fold_id}',\n",
    "        log_model='all'\n",
    "    )\n",
    "\n",
    "    # Trainer\n",
    "    trainer = pl.Trainer(\n",
    "        max_epochs=EPOCHS,\n",
    "        accelerator=DEVICE.type,\n",
    "        precision='16-mixed' if torch.cuda.is_available() else 32,\n",
    "        accumulate_grad_batches=GRAD_ACCUM,\n",
    "        callbacks=[checkpoint_callback, early_stopping_callback, lr_monitor],\n",
    "        logger=wandb_logger,\n",
    "        log_every_n_steps=1,\n",
    "        gradient_clip_val=1.0,\n",
    "        enable_progress_bar=True\n",
    "    )\n",
    "\n",
    "    try:\n",
    "        # Train\n",
    "        logger.info(f\"\\nTraining Student on Fold {fold_id}...\")\n",
    "        trainer.fit(model, train_loader, val_loader)\n",
    "\n",
    "        # Load best checkpoint\n",
    "        best_model_path = checkpoint_callback.best_model_path\n",
    "        logger.info(f\"Loading best Student model from: {best_model_path}\")\n",
    "        best_model = BiomassStudentModel.load_from_checkpoint(best_model_path)\n",
    "\n",
    "        # Evaluate\n",
    "        val_result = trainer.validate(best_model, val_loader, verbose=False)\n",
    "        student_fold_results.append({\n",
    "            'fold': fold_id,\n",
    "            'val_loss': val_result[0]['val_loss'],\n",
    "            'val_comp_metric': val_result[0].get('val_comp_metric', 0.0)\n",
    "        })\n",
    "\n",
    "        logger.success(\n",
    "            f\"Fold {fold_id} completed: val_loss={val_result[0]['val_loss']:.4f}\")\n",
    "\n",
    "    except SystemExit:\n",
    "        logger.warning(\n",
    "            f\"Training interrupted during fold {fold_id}. Exiting gracefully.\")\n",
    "        wandb_logger.experiment.finish()\n",
    "        break\n",
    "\n",
    "    finally:\n",
    "        wandb_logger.experiment.finish()\n",
    "\n",
    "logger.success(\"STUDENT TRAINING COMPLETED\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f4c9c8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Summary\n",
      "\n",
      "   fold  val_loss\n",
      "0     0  0.893824\n",
      "1     1  0.811320\n",
      "2     2  0.936835\n",
      "3     3  1.147606\n",
      "4     4  1.055121\n",
      "Mean Val Loss: 0.9689 Â± 0.1331\n"
     ]
    }
   ],
   "source": [
    "print(\"Training Summary\")\n",
    "print()\n",
    "results_df = pd.DataFrame(student_fold_results)\n",
    "print(results_df)\n",
    "print(f\"Mean Val Loss: {results_df['val_loss'].mean():.4f} Â± {results_df['val_loss'].std():.4f}\")\n",
    "print(f\"Mean Comp Metric: {results_df['val_comp_metric'].mean():.4f} Â± {results_df['val_comp_metric'].std():.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "image2biomass",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
