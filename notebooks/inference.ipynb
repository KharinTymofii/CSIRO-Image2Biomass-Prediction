{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "865c1bde",
   "metadata": {
    "papermill": {
     "duration": 0.004115,
     "end_time": "2025-12-11T19:12:20.041103",
     "exception": false,
     "start_time": "2025-12-11T19:12:20.036988",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Distillation. Student Inference\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c4c0636",
   "metadata": {
    "papermill": {
     "duration": 0.003092,
     "end_time": "2025-12-11T19:12:20.047506",
     "exception": false,
     "start_time": "2025-12-11T19:12:20.044414",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Imports\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e299f95a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-11T19:12:20.054508Z",
     "iopub.status.busy": "2025-12-11T19:12:20.054261Z",
     "iopub.status.idle": "2025-12-11T19:12:38.383713Z",
     "shell.execute_reply": "2025-12-11T19:12:38.382820Z"
    },
    "papermill": {
     "duration": 18.334462,
     "end_time": "2025-12-11T19:12:38.385055",
     "exception": false,
     "start_time": "2025-12-11T19:12:20.050593",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch: 2.9.1+cu128\n",
      "Device: NVIDIA GeForce RTX 5050 Laptop GPU\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import gc\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "\n",
    "import cv2\n",
    "import timm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import pytorch_lightning as pl\n",
    "from tqdm import tqdm\n",
    "from torch.optim import AdamW\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "from transformers import Dinov2Model, Dinov2Config\n",
    "\n",
    "import warnings\n",
    "\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "print(f\"PyTorch: {torch.__version__}\")\n",
    "print(\n",
    "    f\"Device: {torch.cuda.get_device_name(0) if torch.cuda.is_available() else 'CPU'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "077a1548",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-11T19:12:38.393412Z",
     "iopub.status.busy": "2025-12-11T19:12:38.392973Z",
     "iopub.status.idle": "2025-12-11T19:12:38.652255Z",
     "shell.execute_reply": "2025-12-11T19:12:38.651736Z"
    },
    "papermill": {
     "duration": 0.264771,
     "end_time": "2025-12-11T19:12:38.653430",
     "exception": false,
     "start_time": "2025-12-11T19:12:38.388659",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        # print(os.path.join(dirname, filename))\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "89d6f31a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-11T19:12:38.661069Z",
     "iopub.status.busy": "2025-12-11T19:12:38.660833Z",
     "iopub.status.idle": "2025-12-11T19:12:38.674456Z",
     "shell.execute_reply": "2025-12-11T19:12:38.673819Z"
    },
    "papermill": {
     "duration": 0.018698,
     "end_time": "2025-12-11T19:12:38.675573",
     "exception": false,
     "start_time": "2025-12-11T19:12:38.656875",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 1488\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DESCRIPTION_FULL: facebook_dinov2-base-kaggle_train[5]Folds_log_fusion-gating_epochs25_bs16_gradacc1_lr0.0001_wd0.05_dr0.3_hr0.5\n",
      "Effective batch size: 16\n"
     ]
    }
   ],
   "source": [
    "cpu_count = os.cpu_count()\n",
    "NUM_WORKERS = 0\n",
    "\n",
    "IS_ENSEMBLE = False\n",
    "\n",
    "LR = 1e-4\n",
    "EPOCHS = 25\n",
    "N_FOLDS = 5\n",
    "GRAD_ACCUM = 1\n",
    "BATCH_SIZE = 16\n",
    "DROPOUT_RATE = 0.3\n",
    "# Weight for distillation loss\n",
    "# Loss = DISTILL_ALPHA * Distillation_Loss + (1 - DISTILL_ALPHA) * Hard_Loss\n",
    "DISTILL_ALPHA = 0.5\n",
    "WEIGHT_DECAY = 0.05\n",
    "HIDDEN_RATIO = 0.5\n",
    "TRAIN_SPLIT_RATIO = 0.02  # Used if N_FOLDS = 0\n",
    "\n",
    "MODEL = 'facebook/dinov2-base'\n",
    "CHECKPOINTS_DIR = f\"./kaggle/input/2head/\"\n",
    "WEIGHTS_PATH = f\"{CHECKPOINTS_DIR}{MODEL.replace('/', '_')}.pth\"\n",
    "PROJECT_NAME = \"csiro-image2biomass-prediction\"\n",
    "# Whether to use OOF soft targets or 100% ensemble soft targets\n",
    "USE_OOF_SOFT_TARGETS = False\n",
    "\n",
    "# Each patch is 1000x1000, resize to 768x768 for vision transformers\n",
    "SIZE = 768\n",
    "USE_LOG_TARGET = True     # Whether to use log1p transformation on target variable\n",
    "FUSION_METHOD = 'gating'  # ('concat', 'mean', 'max') OR 'gating'\n",
    "\n",
    "DESCRIPTION = \"kaggle\" + \\\n",
    "    (f\"_train{TRAIN_SPLIT_RATIO}\" if N_FOLDS == 0 else f\"_train[{N_FOLDS}]Folds\") + (\n",
    "        f\"_log\" if USE_LOG_TARGET else \"\") + f\"_fusion-{FUSION_METHOD}\"\n",
    "DESCRIPTION_FULL = MODEL.replace('/', '_') + \"-\" + DESCRIPTION + \\\n",
    "    f\"_epochs{EPOCHS}_bs{BATCH_SIZE}_gradacc{GRAD_ACCUM}_lr{LR}_wd{WEIGHT_DECAY}_dr{DROPOUT_RATE}_hr{HIDDEN_RATIO}\"\n",
    "SUBMISSION_NAME = f\"{DESCRIPTION_FULL}_submission.csv\"\n",
    "SUBMISSION_ENSEMBLE_NAME = f\"{DESCRIPTION_FULL}_ensemble_submission.csv\"\n",
    "SUBMISSION_MSG = DESCRIPTION_FULL.replace(\"_\", \" \")\n",
    "\n",
    "SEED = 1488\n",
    "torch.manual_seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "pl.seed_everything(SEED)\n",
    "\n",
    "print(\"DESCRIPTION_FULL:\", DESCRIPTION_FULL)\n",
    "print(f\"Effective batch size: {BATCH_SIZE * GRAD_ACCUM}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "13369e6c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-11T19:12:38.683004Z",
     "iopub.status.busy": "2025-12-11T19:12:38.682783Z",
     "iopub.status.idle": "2025-12-11T19:12:38.924229Z",
     "shell.execute_reply": "2025-12-11T19:12:38.923358Z"
    },
    "papermill": {
     "duration": 0.246526,
     "end_time": "2025-12-11T19:12:38.925434",
     "exception": false,
     "start_time": "2025-12-11T19:12:38.678908",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "NUM_WORKERS: 0\n",
      "\n",
      "NVIDIA GeForce RTX 5050 Laptop GPU\n",
      "Memory Usage:\n",
      "Allocated: 0.0 GB\n",
      "Cached:    0.0 GB\n"
     ]
    }
   ],
   "source": [
    "# setting device on GPU if available, else CPU\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Using device:', DEVICE)\n",
    "print('NUM_WORKERS:', NUM_WORKERS)\n",
    "print()\n",
    "\n",
    "# Additional Info when using cuda\n",
    "if DEVICE.type == 'cuda':\n",
    "    # clean GPU memory\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "\n",
    "    # torch.set_float32_matmul_precision('high')\n",
    "\n",
    "    print(torch.cuda.get_device_name(0))\n",
    "    print('Memory Usage:')\n",
    "    print('Allocated:', round(torch.cuda.memory_allocated(0)/1024**3, 1), 'GB')\n",
    "    print('Cached:   ', round(torch.cuda.memory_reserved(0)/1024**3, 1), 'GB')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee0baaac",
   "metadata": {
    "papermill": {
     "duration": 0.003521,
     "end_time": "2025-12-11T19:12:38.932551",
     "exception": false,
     "start_time": "2025-12-11T19:12:38.929030",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Model Architecture\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6d676db0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-11T19:12:38.940125Z",
     "iopub.status.busy": "2025-12-11T19:12:38.939895Z",
     "iopub.status.idle": "2025-12-11T19:12:38.945085Z",
     "shell.execute_reply": "2025-12-11T19:12:38.944346Z"
    },
    "papermill": {
     "duration": 0.010274,
     "end_time": "2025-12-11T19:12:38.946120",
     "exception": false,
     "start_time": "2025-12-11T19:12:38.935846",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "labels = [\n",
    "    \"Dry_Clover_g\",\n",
    "    \"Dry_Dead_g\",\n",
    "    \"Dry_Green_g\",\n",
    "    \"Dry_Total_g\",\n",
    "    \"GDM_g\"\n",
    "]\n",
    "\n",
    "weights = {\n",
    "    'Dry_Green_g': 0.1,\n",
    "    'Dry_Dead_g': 0.1,\n",
    "    'Dry_Clover_g': 0.1,\n",
    "    'GDM_g': 0.2,\n",
    "    'Dry_Total_g': 0.5,\n",
    "}\n",
    "\n",
    "\n",
    "def competition_metric(y_true, y_pred) -> float:\n",
    "    \"\"\"Function to calculate the competition's official evaluation metric (weighted R2 score).\"\"\"\n",
    "    weights_array = np.array([weights[l] for l in labels])\n",
    "\n",
    "    # Align with this calculation method\n",
    "    y_weighted_mean = np.average(y_true, weights=weights_array, axis=1).mean()\n",
    "\n",
    "    # For ss_res and ss_tot, also take the weighted average on axis=1, then the mean of the result\n",
    "    ss_res = np.average((y_true - y_pred)**2,\n",
    "                        weights=weights_array, axis=1).mean()\n",
    "    ss_tot = np.average((y_true - y_weighted_mean)**2,\n",
    "                        weights=weights_array, axis=1).mean()\n",
    "\n",
    "    return 1 - ss_res / ss_tot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "da81bbf2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-11T19:12:38.953854Z",
     "iopub.status.busy": "2025-12-11T19:12:38.953631Z",
     "iopub.status.idle": "2025-12-11T19:12:38.961408Z",
     "shell.execute_reply": "2025-12-11T19:12:38.960794Z"
    },
    "papermill": {
     "duration": 0.012976,
     "end_time": "2025-12-11T19:12:38.962524",
     "exception": false,
     "start_time": "2025-12-11T19:12:38.949548",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class BiomassTeacherModelPatches(pl.LightningModule):\n",
    "    \"\"\"Dual-head teacher model (inference-focused: uses image-only head).\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        backbone_name: str = 'facebook/dinov2-base',\n",
    "        tabular_dim: int = 10,\n",
    "        num_targets: int = 3,\n",
    "        lr: float = 1e-4,\n",
    "        weight_decay: float = 1e-5,\n",
    "        hidden_ratio: float = 0.5,\n",
    "        dropout: float = 0.2,\n",
    "        fusion_method: str = 'gating',\n",
    "        use_log_target: bool = True,\n",
    "        tabular_dropout_prob: float = 0.3,\n",
    "        lambda_cons: float = 0.5,\n",
    "        pretrained_backbone: bool = True,\n",
    "        backbone_weights_path: str | None = None,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "\n",
    "        # Backbone (offline friendly)\n",
    "        if pretrained_backbone:\n",
    "            self.backbone = Dinov2Model.from_pretrained(backbone_name)\n",
    "        else:\n",
    "            try:\n",
    "                config = Dinov2Config.from_pretrained(\n",
    "                    backbone_name, local_files_only=True)\n",
    "            except Exception:\n",
    "                config = Dinov2Config()\n",
    "            self.backbone = Dinov2Model(config)\n",
    "            if backbone_weights_path and os.path.exists(backbone_weights_path):\n",
    "                state_dict = torch.load(\n",
    "                    backbone_weights_path, map_location='cpu')\n",
    "                self.backbone.load_state_dict(state_dict, strict=False)\n",
    "\n",
    "        self.hidden_dim = self.backbone.config.hidden_size\n",
    "        self.patch_size = getattr(self.backbone.config, 'patch_size', None)\n",
    "\n",
    "        self.lr = lr\n",
    "        self.weight_decay = weight_decay\n",
    "        self.fusion_method = fusion_method\n",
    "        self.use_log_target = use_log_target\n",
    "        self.tabular_dropout_prob = tabular_dropout_prob\n",
    "        self.lambda_cons = lambda_cons\n",
    "\n",
    "        self.prediction_mode = 'img'\n",
    "\n",
    "        hidden_size = max(32, int(self.hidden_dim * hidden_ratio))\n",
    "\n",
    "        def make_patch_head():\n",
    "            return nn.Sequential(\n",
    "                nn.Linear(self.hidden_dim, hidden_size),\n",
    "                nn.LayerNorm(hidden_size),\n",
    "                nn.ReLU(inplace=True),\n",
    "                nn.Dropout(dropout),\n",
    "                nn.Linear(hidden_size, 1)\n",
    "            )\n",
    "\n",
    "        # Image-only heads\n",
    "        self.img_head_green = make_patch_head()\n",
    "        self.img_head_clover = make_patch_head()\n",
    "        self.img_head_dead = make_patch_head()\n",
    "\n",
    "        # Privileged heads (kept for checkpoint compatibility)\n",
    "        self.priv_head_green = make_patch_head()\n",
    "        self.priv_head_clover = make_patch_head()\n",
    "        self.priv_head_dead = make_patch_head()\n",
    "\n",
    "        # Tabular fusion modules (kept for checkpoint compatibility)\n",
    "        if self.fusion_method == 'gating':\n",
    "            self.tabular_gate = nn.Sequential(\n",
    "                nn.Linear(tabular_dim, hidden_size),\n",
    "                nn.ReLU(inplace=True),\n",
    "                nn.Linear(hidden_size, 1),\n",
    "                nn.Sigmoid()\n",
    "            )\n",
    "        elif self.fusion_method == 'concat':\n",
    "            self.fusion_layer = nn.Sequential(\n",
    "                nn.Linear(3 + tabular_dim, hidden_size),\n",
    "                nn.ReLU(inplace=True),\n",
    "                nn.Dropout(dropout),\n",
    "                nn.Linear(hidden_size, 3)\n",
    "            )\n",
    "\n",
    "    def forward(self, batch: dict):\n",
    "        # Forward through DINOv2 for both patches\n",
    "        left_outputs = self.backbone(batch['left_image'])\n",
    "        left_patches = left_outputs.last_hidden_state[:, 1:, :]\n",
    "\n",
    "        right_outputs = self.backbone(batch['right_image'])\n",
    "        right_patches = right_outputs.last_hidden_state[:, 1:, :]\n",
    "\n",
    "        all_patches = torch.cat([left_patches, right_patches], dim=1)\n",
    "\n",
    "        # Image-only head\n",
    "        img_green = self.img_head_green(all_patches).mean(dim=1).squeeze(1)\n",
    "        img_clover = self.img_head_clover(all_patches).mean(dim=1).squeeze(1)\n",
    "        img_dead = self.img_head_dead(all_patches).mean(dim=1).squeeze(1)\n",
    "\n",
    "        # Privileged head (ignored in inference, kept for compatibility)\n",
    "        priv_green = self.priv_head_green(all_patches).mean(dim=1).squeeze(1)\n",
    "        priv_clover = self.priv_head_clover(all_patches).mean(dim=1).squeeze(1)\n",
    "        priv_dead = self.priv_head_dead(all_patches).mean(dim=1).squeeze(1)\n",
    "\n",
    "        # Tabular (if present) for compatibility; safe when missing\n",
    "        tabular = batch.get('tabular', None)\n",
    "        if tabular is None:\n",
    "            tabular = torch.zeros(img_green.size(0),\n",
    "                                  self.tabular_gate[0].in_features if hasattr(\n",
    "                                      self, 'tabular_gate') else 0,\n",
    "                                  device=img_green.device)\n",
    "\n",
    "        if self.fusion_method == 'gating' and hasattr(self, 'tabular_gate') and tabular.numel() > 0:\n",
    "            gate = self.tabular_gate(tabular).squeeze(1)\n",
    "            priv_green = priv_green * gate\n",
    "            priv_clover = priv_clover * gate\n",
    "            priv_dead = priv_dead * gate\n",
    "        elif self.fusion_method == 'concat' and hasattr(self, 'fusion_layer') and tabular.numel() > 0:\n",
    "            combined = torch.cat([\n",
    "                priv_green.unsqueeze(1),\n",
    "                priv_clover.unsqueeze(1),\n",
    "                priv_dead.unsqueeze(1),\n",
    "                tabular\n",
    "            ], dim=1)\n",
    "            output = self.fusion_layer(combined)\n",
    "            priv_green, priv_clover, priv_dead = output[:,\n",
    "                                                        0], output[:, 1], output[:, 2]\n",
    "\n",
    "        return (img_green, img_clover, img_dead), (priv_green, priv_clover, priv_dead)\n",
    "\n",
    "    def predict_step(self, batch: dict, batch_idx: int = 0) -> torch.Tensor:\n",
    "        img_pred, priv_pred = self(batch)\n",
    "        if self.prediction_mode == 'img':\n",
    "            green, clover, dead = img_pred\n",
    "        else:\n",
    "            green, clover, dead = priv_pred\n",
    "\n",
    "        preds = torch.stack([clover, dead, green], dim=1)\n",
    "        if self.use_log_target:\n",
    "            preds = torch.expm1(preds)\n",
    "        preds = torch.clamp(preds, min=0.0)\n",
    "        return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc59dd50",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-11T19:12:38.996769Z",
     "iopub.status.busy": "2025-12-11T19:12:38.996566Z",
     "iopub.status.idle": "2025-12-11T19:12:40.693006Z",
     "shell.execute_reply": "2025-12-11T19:12:40.692116Z"
    },
    "papermill": {
     "duration": 1.701985,
     "end_time": "2025-12-11T19:12:40.694624",
     "exception": false,
     "start_time": "2025-12-11T19:12:38.992639",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(f\"Loading backbone weights from: {WEIGHTS_PATH}\")\n",
    "config = Dinov2Config()\n",
    "temp_backbone = Dinov2Model(config=config)\n",
    "state_dict = torch.load(WEIGHTS_PATH, map_location='cpu')\n",
    "temp_backbone.load_state_dict(state_dict, strict=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "20603376",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # save model weights to file\n",
    "# torch.save(temp_backbone.state_dict(), WEIGHTS_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "67bea8db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "518"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SIZE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6be3cb04",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs_size = config.image_size\n",
    "mean = [0.485, 0.456, 0.406]\n",
    "std = [0.229, 0.224, 0.225]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "1586a378",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-11T19:12:40.704683Z",
     "iopub.status.busy": "2025-12-11T19:12:40.704457Z",
     "iopub.status.idle": "2025-12-11T19:12:40.978567Z",
     "shell.execute_reply": "2025-12-11T19:12:40.977920Z"
    },
    "papermill": {
     "duration": 0.280794,
     "end_time": "2025-12-11T19:12:40.979953",
     "exception": false,
     "start_time": "2025-12-11T19:12:40.699159",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Backbone expected input size: 518, using SIZE=518\n",
      "Backbone expected mean: [0.485, 0.456, 0.406], std: [0.229, 0.224, 0.225]\n",
      "768\n"
     ]
    }
   ],
   "source": [
    "SIZE = inputs_size\n",
    "print(f\"Backbone expected input size: {inputs_size}, using SIZE={SIZE}\")\n",
    "print(f\"Backbone expected mean: {mean}, std: {std}\")\n",
    "\n",
    "# Get backbone output dimension\n",
    "with torch.no_grad():\n",
    "    dummy = torch.randn(1, 3, SIZE, SIZE)\n",
    "    outputs = temp_backbone(dummy)\n",
    "    feat_dim = outputs.last_hidden_state.sum(\n",
    "        dim=1).shape[1]  # Average pooling\n",
    "    print(feat_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7ad7904b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-11T19:12:40.988400Z",
     "iopub.status.busy": "2025-12-11T19:12:40.988135Z",
     "iopub.status.idle": "2025-12-11T19:12:40.992065Z",
     "shell.execute_reply": "2025-12-11T19:12:40.991493Z"
    },
    "papermill": {
     "duration": 0.009317,
     "end_time": "2025-12-11T19:12:40.993095",
     "exception": false,
     "start_time": "2025-12-11T19:12:40.983778",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "student_val_transform = transforms.Compose([\n",
    "    transforms.Resize((SIZE, SIZE)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=mean, std=std)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "34f57baa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TTA helpers\n",
    "TTA_TYPES = ['id', 'hflip', 'vflip', 'hvflip']\n",
    "\n",
    "\n",
    "def apply_tta(left: torch.Tensor, right: torch.Tensor, tta: str) -> tuple[torch.Tensor, torch.Tensor]:\n",
    "    \"\"\"Apply simple flip-based TTA to both patches.\"\"\"\n",
    "    if tta == 'hflip':\n",
    "        return torch.flip(left, dims=[2]), torch.flip(right, dims=[2])\n",
    "    if tta == 'vflip':\n",
    "        return torch.flip(left, dims=[1]), torch.flip(right, dims=[1])\n",
    "    if tta == 'hvflip':\n",
    "        return torch.flip(left, dims=[1, 2]), torch.flip(right, dims=[1, 2])\n",
    "    return left, right"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5a5a5df0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stack_targets_from_preds(preds_3: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"Given [B,3] (clover, dead, green) produce [B,5] ordered targets.\"\"\"\n",
    "    clover = preds_3[:, 0]\n",
    "    dead = preds_3[:, 1]\n",
    "    green = preds_3[:, 2]\n",
    "    total = green + dead + clover\n",
    "    gdm = clover + green\n",
    "    return torch.stack([clover, dead, green, total, gdm], dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ac2c6e1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_model_batch(model: BiomassTeacherModelPatches, batch: dict, tta_types: list[str]) -> torch.Tensor:\n",
    "    \"\"\"Run model over TTA variants and average. Returns [B,5].\"\"\"\n",
    "    model_preds = []\n",
    "    for tta in tta_types:\n",
    "        left_t, right_t = apply_tta(\n",
    "            batch['left_image'], batch['right_image'], tta)\n",
    "        tta_batch = {\n",
    "            'left_image': left_t,\n",
    "            'right_image': right_t,\n",
    "            'tabular': batch['tabular'],\n",
    "        }\n",
    "        preds_3 = model.predict_step(tta_batch, 0)  # [B,3]\n",
    "        model_preds.append(stack_targets_from_preds(preds_3))\n",
    "    return torch.stack(model_preds, dim=0).mean(dim=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b494fd8",
   "metadata": {
    "papermill": {
     "duration": 0.003342,
     "end_time": "2025-12-11T19:12:40.999896",
     "exception": false,
     "start_time": "2025-12-11T19:12:40.996554",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Inference on Test Set\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c97fc78c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-11T19:12:41.007903Z",
     "iopub.status.busy": "2025-12-11T19:12:41.007699Z",
     "iopub.status.idle": "2025-12-11T19:12:41.010988Z",
     "shell.execute_reply": "2025-12-11T19:12:41.010419Z"
    },
    "papermill": {
     "duration": 0.008328,
     "end_time": "2025-12-11T19:12:41.012015",
     "exception": false,
     "start_time": "2025-12-11T19:12:41.003687",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "PATH_DATA = './kaggle/input/csiro-biomass'\n",
    "STUDENT_MODELS_DIR = './kaggle/input/2head'\n",
    "PATH_TEST_CSV = os.path.join(PATH_DATA, 'test.csv')\n",
    "PATH_TEST_IMG = os.path.join(PATH_DATA, 'test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "14d08303",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-11T19:12:41.019779Z",
     "iopub.status.busy": "2025-12-11T19:12:41.019584Z",
     "iopub.status.idle": "2025-12-11T19:12:41.056030Z",
     "shell.execute_reply": "2025-12-11T19:12:41.055336Z"
    },
    "papermill": {
     "duration": 0.041627,
     "end_time": "2025-12-11T19:12:41.057105",
     "exception": false,
     "start_time": "2025-12-11T19:12:41.015478",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set size: 1\n",
      "              image_path                   sample_id   target_name\n",
      "0  test/ID1001187975.jpg  ID1001187975__Dry_Clover_g  Dry_Clover_g\n"
     ]
    }
   ],
   "source": [
    "# Load test CSV\n",
    "test_df = pd.read_csv(PATH_TEST_CSV)\n",
    "test_df = test_df[~test_df['target_name'].isin(['Dry_Total_g', 'GDM_g'])]\n",
    "\n",
    "# Pivot to one row per image\n",
    "test_pivot = test_df.pivot_table(\n",
    "    index='image_path',\n",
    "    aggfunc='first'\n",
    ").reset_index()\n",
    "\n",
    "print(f\"Test set size: {len(test_pivot)}\")\n",
    "print(test_pivot.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a254b7d0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-11T19:12:41.065121Z",
     "iopub.status.busy": "2025-12-11T19:12:41.064924Z",
     "iopub.status.idle": "2025-12-11T19:12:45.737686Z",
     "shell.execute_reply": "2025-12-11T19:12:45.736663Z"
    },
    "papermill": {
     "duration": 4.678651,
     "end_time": "2025-12-11T19:12:45.739256",
     "exception": false,
     "start_time": "2025-12-11T19:12:41.060605",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Checkpoint discovery and loading\n",
    "def parse_metric_from_filename(filename: str) -> float:\n",
    "    \"\"\"Extract val_comp_metric_img from filename like ...val_comp_metric_img=0.7129.ckpt.\"\"\"\n",
    "    try:\n",
    "        metric_part = filename.split('val_comp_metric_img=')[-1]\n",
    "        return float(metric_part.replace('.ckpt', ''))\n",
    "    except Exception:\n",
    "        return -float('inf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "795f1792",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_student_model(ckpt_path: str, backbone_weights_path: str | None = None) -> BiomassTeacherModelPatches:\n",
    "    \"\"\"Load model from checkpoint without internet (uses saved weights).\"\"\"\n",
    "    checkpoint = torch.load(ckpt_path, map_location='cpu')\n",
    "    hparams = checkpoint['hyper_parameters']\n",
    "\n",
    "    model = BiomassTeacherModelPatches(\n",
    "        backbone_name=hparams['backbone_name'],\n",
    "        tabular_dim=hparams.get('tabular_dim', 0),\n",
    "        num_targets=hparams['num_targets'],\n",
    "        lr=hparams['lr'],\n",
    "        weight_decay=hparams['weight_decay'],\n",
    "        hidden_ratio=hparams['hidden_ratio'],\n",
    "        dropout=hparams['dropout'],\n",
    "        fusion_method=hparams['fusion_method'],\n",
    "        use_log_target=hparams['use_log_target'],\n",
    "        tabular_dropout_prob=hparams.get('tabular_dropout_prob', 0.0),\n",
    "        lambda_cons=hparams.get('lambda_cons', 0.0),\n",
    "        pretrained_backbone=False,\n",
    "        backbone_weights_path=backbone_weights_path,\n",
    "    )\n",
    "    model.prediction_mode = 'img'  # image-only branch for inference    model.eval()\n",
    "    model.to(DEVICE)\n",
    "\n",
    "    # Load state_dict with strict=False to handle size mismatches\n",
    "    missing_keys, unexpected_keys = model.load_state_dict(checkpoint['state_dict'], strict=False)\n",
    "    if missing_keys:\n",
    "        print(f\"  Missing keys: {len(missing_keys)} (expected for new modules)\")\n",
    "    if unexpected_keys:\n",
    "        print(f\"  Unexpected keys: {len(unexpected_keys)} (safe to ignore)\")    \n",
    "        \n",
    "    return model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4e8fccc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 5 student checkpoints:\n",
      "  - f0dinov2-base-local_train(5)Folds_log_fusion-gating_epochs30_bs4_gradacc4_lr3e-05_wd0.05_dr0.2_hr0.5-fold0-epoch=23-val_loss_img=0.0000-val_comp_metric_img=0.7129.ckpt\n",
      "  - f1dinov2-base-local_train(5)Folds_log_fusion-gating_epochs30_bs4_gradacc4_lr3e-05_wd0.05_dr0.2_hr0.5-fold1-epoch=24-val_loss_img=0.0000-val_comp_metric_img=0.726.ckpt\n",
      "  - f2dinov2-base-local_train(5)Folds_log_fusion-gating_epochs30_bs4_gradacc4_lr3e-05_wd0.05_dr0.2_hr0.5-fold2-epoch=28-val_loss_img=0.0000-val_comp_metric_img=0.773.ckpt\n",
      "  - f3dinov2-base-local_train(5)Folds_log_fusion-gating_epochs30_bs4_gradacc4_lr3e-05_wd0.05_dr0.2_hr0.5-fold3-epoch=25-val_loss_img=0.0000-val_comp_metric_img=0.725.ckpt\n",
      "  - f4dinov2-base-local_train(5)Folds_log_fusion-gating_epochs30_bs4_gradacc4_lr3e-05_wd0.05_dr0.2_hr0.5-fold4-epoch=26-val_loss_img=0.0000-val_comp_metric_img=0.7169.ckpt\n"
     ]
    }
   ],
   "source": [
    "# Discover fold checkpoints and best overall\n",
    "ckpt_files = sorted([\n",
    "    f for f in os.listdir(STUDENT_MODELS_DIR) if f.endswith('.ckpt')\n",
    "])\n",
    "print(f\"Found {len(ckpt_files)} student checkpoints:\")\n",
    "for f in ckpt_files:\n",
    "    print(f\"  - {f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "dff816d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select 5 folds: filenames starting with f{fold}\n",
    "fold_ckpts = []\n",
    "for fold_id in range(N_FOLDS):\n",
    "    candidates = [f for f in ckpt_files if f.startswith(f\"f{fold_id}\")]\n",
    "    if not candidates:\n",
    "        continue\n",
    "    # pick best metric per fold\n",
    "    candidates.sort(key=parse_metric_from_filename, reverse=True)\n",
    "    fold_ckpts.append(os.path.join(STUDENT_MODELS_DIR, candidates[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d6e10f33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected fold checkpoints:\n",
      "  f0dinov2-base-local_train(5)Folds_log_fusion-gating_epochs30_bs4_gradacc4_lr3e-05_wd0.05_dr0.2_hr0.5-fold0-epoch=23-val_loss_img=0.0000-val_comp_metric_img=0.7129.ckpt\n",
      "  f1dinov2-base-local_train(5)Folds_log_fusion-gating_epochs30_bs4_gradacc4_lr3e-05_wd0.05_dr0.2_hr0.5-fold1-epoch=24-val_loss_img=0.0000-val_comp_metric_img=0.726.ckpt\n",
      "  f2dinov2-base-local_train(5)Folds_log_fusion-gating_epochs30_bs4_gradacc4_lr3e-05_wd0.05_dr0.2_hr0.5-fold2-epoch=28-val_loss_img=0.0000-val_comp_metric_img=0.773.ckpt\n",
      "  f3dinov2-base-local_train(5)Folds_log_fusion-gating_epochs30_bs4_gradacc4_lr3e-05_wd0.05_dr0.2_hr0.5-fold3-epoch=25-val_loss_img=0.0000-val_comp_metric_img=0.725.ckpt\n",
      "  f4dinov2-base-local_train(5)Folds_log_fusion-gating_epochs30_bs4_gradacc4_lr3e-05_wd0.05_dr0.2_hr0.5-fold4-epoch=26-val_loss_img=0.0000-val_comp_metric_img=0.7169.ckpt\n",
      "Best checkpoint: f2dinov2-base-local_train(5)Folds_log_fusion-gating_epochs30_bs4_gradacc4_lr3e-05_wd0.05_dr0.2_hr0.5-fold2-epoch=28-val_loss_img=0.0000-val_comp_metric_img=0.773.ckpt\n"
     ]
    }
   ],
   "source": [
    "# Best overall by metric\n",
    "best_ckpt = None\n",
    "if ckpt_files:\n",
    "    best_ckpt = os.path.join(\n",
    "        STUDENT_MODELS_DIR,\n",
    "        sorted(ckpt_files, key=parse_metric_from_filename, reverse=True)[0]\n",
    "    )\n",
    "\n",
    "print(\"Selected fold checkpoints:\")\n",
    "for p in fold_ckpts:\n",
    "    print(f\"  {os.path.basename(p)}\")\n",
    "print(\n",
    "    f\"Best checkpoint: {os.path.basename(best_ckpt) if best_ckpt else 'None'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "480e90a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loading fold model: f0dinov2-base-local_train(5)Folds_log_fusion-gating_epochs30_bs4_gradacc4_lr3e-05_wd0.05_dr0.2_hr0.5-fold0-epoch=23-val_loss_img=0.0000-val_comp_metric_img=0.7129.ckpt\n",
      "\n",
      "Loading fold model: f1dinov2-base-local_train(5)Folds_log_fusion-gating_epochs30_bs4_gradacc4_lr3e-05_wd0.05_dr0.2_hr0.5-fold1-epoch=24-val_loss_img=0.0000-val_comp_metric_img=0.726.ckpt\n",
      "\n",
      "Loading fold model: f2dinov2-base-local_train(5)Folds_log_fusion-gating_epochs30_bs4_gradacc4_lr3e-05_wd0.05_dr0.2_hr0.5-fold2-epoch=28-val_loss_img=0.0000-val_comp_metric_img=0.773.ckpt\n",
      "\n",
      "Loading fold model: f3dinov2-base-local_train(5)Folds_log_fusion-gating_epochs30_bs4_gradacc4_lr3e-05_wd0.05_dr0.2_hr0.5-fold3-epoch=25-val_loss_img=0.0000-val_comp_metric_img=0.725.ckpt\n",
      "\n",
      "Loading fold model: f4dinov2-base-local_train(5)Folds_log_fusion-gating_epochs30_bs4_gradacc4_lr3e-05_wd0.05_dr0.2_hr0.5-fold4-epoch=26-val_loss_img=0.0000-val_comp_metric_img=0.7169.ckpt\n",
      "\n",
      "Loading best model: f2dinov2-base-local_train(5)Folds_log_fusion-gating_epochs30_bs4_gradacc4_lr3e-05_wd0.05_dr0.2_hr0.5-fold2-epoch=28-val_loss_img=0.0000-val_comp_metric_img=0.773.ckpt\n",
      "\n",
      "Successfully loaded 5 student models\n",
      "Ready for offline inference on Kaggle!\n"
     ]
    }
   ],
   "source": [
    "# Load models WITHOUT internet (offline inference on Kaggle)\n",
    "student_models = []\n",
    "for ckpt_path in fold_ckpts:\n",
    "    print(f\"\\nLoading fold model: {os.path.basename(ckpt_path)}\")\n",
    "    student_models.append(load_student_model(\n",
    "        ckpt_path, backbone_weights_path=WEIGHTS_PATH))\n",
    "\n",
    "if best_ckpt:\n",
    "    print(f\"\\nLoading best model: {os.path.basename(best_ckpt)}\")\n",
    "    best_model = load_student_model(\n",
    "        best_ckpt, backbone_weights_path=WEIGHTS_PATH)\n",
    "\n",
    "print(f\"\\nSuccessfully loaded {len(student_models)} student models\")\n",
    "print(\"Ready for offline inference on Kaggle!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8fb839a9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-11T19:12:45.749267Z",
     "iopub.status.busy": "2025-12-11T19:12:45.749041Z",
     "iopub.status.idle": "2025-12-11T19:12:45.759525Z",
     "shell.execute_reply": "2025-12-11T19:12:45.758695Z"
    },
    "papermill": {
     "duration": 0.016971,
     "end_time": "2025-12-11T19:12:45.760856",
     "exception": false,
     "start_time": "2025-12-11T19:12:45.743885",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create test dataset\n",
    "class BiomassTestDataset(Dataset):\n",
    "    \"\"\"Test dataset for inference - no targets needed.\"\"\"\n",
    "\n",
    "    def __init__(self, df: pd.DataFrame, img_dir: str, transform=None):\n",
    "        self.df = df.reset_index(drop=True)\n",
    "        self.img_dir = img_dir\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "\n",
    "        # Load image\n",
    "        img_path = os.path.join(\n",
    "            self.img_dir, row['image_path'].replace('test/', ''))\n",
    "        image = cv2.imread(img_path)\n",
    "\n",
    "        if image is None:\n",
    "            raise FileNotFoundError(f\"Cannot load image: {img_path}\")\n",
    "\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        # Split into left and right patches\n",
    "        h, w, c = image.shape\n",
    "        mid_w = w // 2\n",
    "\n",
    "        left_patch = image[:, :mid_w, :]\n",
    "        right_patch = image[:, mid_w:, :]\n",
    "\n",
    "        # Convert to PIL\n",
    "        left_pil = Image.fromarray(left_patch)\n",
    "        right_pil = Image.fromarray(right_patch)\n",
    "\n",
    "        # Apply transforms\n",
    "        if self.transform:\n",
    "            left_tensor = self.transform(left_pil)\n",
    "            right_tensor = self.transform(right_pil)\n",
    "        else:\n",
    "            left_tensor = transforms.ToTensor()(left_pil)\n",
    "            right_tensor = transforms.ToTensor()(right_pil)\n",
    "\n",
    "        return {\n",
    "            'left_image': left_tensor,\n",
    "            'right_image': right_tensor,\n",
    "            'image_id': row['image_path'].split('/')[-1].replace('.jpg', ''),\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a88be98f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loader created: 1 batches\n"
     ]
    }
   ],
   "source": [
    "# Create test dataloader\n",
    "test_dataset = BiomassTestDataset(\n",
    "    df=test_pivot,\n",
    "    img_dir=PATH_TEST_IMG,\n",
    "    transform=student_val_transform\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=BATCH_SIZE * 2,\n",
    "    shuffle=False,\n",
    "    num_workers=min(NUM_WORKERS, 4),\n",
    "    pin_memory=True if torch.cuda.is_available() else False\n",
    ")\n",
    "\n",
    "print(f\"Test loader created: {len(test_loader)} batches\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5f3f8267",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-11T19:12:45.770355Z",
     "iopub.status.busy": "2025-12-11T19:12:45.770150Z",
     "iopub.status.idle": "2025-12-11T19:12:46.742565Z",
     "shell.execute_reply": "2025-12-11T19:12:46.741710Z"
    },
    "papermill": {
     "duration": 0.978602,
     "end_time": "2025-12-11T19:12:46.743771",
     "exception": false,
     "start_time": "2025-12-11T19:12:45.765169",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running inference on test set...\n",
      "Using tabular_dim=21 for inference\n"
     ]
    }
   ],
   "source": [
    "# Run inference on test set with TTA\n",
    "print(\"Running inference on test set...\")\n",
    "\n",
    "all_predictions = []\n",
    "all_image_ids = []\n",
    "\n",
    "if len(student_models) == 0:\n",
    "    raise RuntimeError(\"No student models loaded for inference\")\n",
    "\n",
    "tabular_dim = student_models[0].hparams.tabular_dim\n",
    "\n",
    "print(f\"Using tabular_dim={tabular_dim} for inference\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "900254a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inference: 100%|██████████| 1/1 [00:01<00:00,  1.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions shape: (1, 5)\n",
      "Image IDs count: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "if IS_ENSEMBLE:\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, batch in enumerate(tqdm(test_loader, desc=\"Inference\")):\n",
    "            # Move to device\n",
    "            batch['left_image'] = batch['left_image'].to(DEVICE)\n",
    "            batch['right_image'] = batch['right_image'].to(DEVICE)\n",
    "            batch['tabular'] = torch.zeros(\n",
    "                batch['left_image'].size(0), tabular_dim, device=DEVICE)\n",
    "\n",
    "            # Ensemble predictions from all models with TTA\n",
    "            batch_preds_list = []\n",
    "            for model in student_models:\n",
    "                model_preds = predict_model_batch(\n",
    "                    model, batch, TTA_TYPES)  # [B,5]\n",
    "                batch_preds_list.append(model_preds.cpu())\n",
    "\n",
    "            # Average predictions across models\n",
    "            batch_preds_avg = torch.stack(\n",
    "                batch_preds_list, dim=0).mean(dim=0)  # [B,5]\n",
    "\n",
    "            all_predictions.append(batch_preds_avg.numpy())\n",
    "            all_image_ids.extend(batch['image_id'])\n",
    "\n",
    "    # Concatenate all predictions\n",
    "    all_predictions_array = np.concatenate(all_predictions, axis=0)\n",
    "else:\n",
    "    model = best_model\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, batch in enumerate(tqdm(test_loader, desc=\"Inference\")):\n",
    "            # Move to device\n",
    "            batch['left_image'] = batch['left_image'].to(DEVICE)\n",
    "            batch['right_image'] = batch['right_image'].to(DEVICE)\n",
    "            batch['tabular'] = torch.zeros(\n",
    "                batch['left_image'].size(0), tabular_dim, device=DEVICE)\n",
    "\n",
    "            # Single model predictions with TTA\n",
    "            model_preds = predict_model_batch(model, batch, TTA_TYPES)  # [B,5]\n",
    "\n",
    "            all_predictions.append(model_preds.cpu().numpy())\n",
    "            all_image_ids.extend(batch['image_id'])\n",
    "\n",
    "    # Concatenate all predictions\n",
    "    all_predictions_array = np.concatenate(all_predictions, axis=0)\n",
    "\n",
    "print(f\"Predictions shape: {all_predictions_array.shape}\")\n",
    "print(f\"Image IDs count: {len(all_image_ids)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e73e7ece",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-11T19:12:46.753154Z",
     "iopub.status.busy": "2025-12-11T19:12:46.752693Z",
     "iopub.status.idle": "2025-12-11T19:12:46.760545Z",
     "shell.execute_reply": "2025-12-11T19:12:46.759842Z"
    },
    "papermill": {
     "duration": 0.01355,
     "end_time": "2025-12-11T19:12:46.761589",
     "exception": false,
     "start_time": "2025-12-11T19:12:46.748039",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submission shape: (5, 2)\n",
      "Expected shape: (5, 2)\n",
      "                    sample_id     target\n",
      "0  ID1001187975__Dry_Clover_g   0.147445\n",
      "1    ID1001187975__Dry_Dead_g  25.364407\n",
      "2   ID1001187975__Dry_Green_g  38.670818\n",
      "3   ID1001187975__Dry_Total_g  64.182671\n",
      "4         ID1001187975__GDM_g  38.818264\n"
     ]
    }
   ],
   "source": [
    "# Format submission CSV\n",
    "# Columns order: Dry_Clover_g, Dry_Dead_g, Dry_Green_g, Dry_Total_g, GDM_g\n",
    "target_names = ['Dry_Clover_g', 'Dry_Dead_g',\n",
    "                'Dry_Green_g', 'Dry_Total_g', 'GDM_g']\n",
    "\n",
    "submission_rows = []\n",
    "\n",
    "for img_idx, image_id in enumerate(all_image_ids):\n",
    "    predictions = all_predictions_array[img_idx]  # [5] values for 5 targets\n",
    "\n",
    "    for target_idx, target_name in enumerate(target_names):\n",
    "        sample_id = f\"{image_id}__{target_name}\"\n",
    "        target_value = float(predictions[target_idx])\n",
    "\n",
    "        submission_rows.append({\n",
    "            'sample_id': sample_id,\n",
    "            'target': target_value\n",
    "        })\n",
    "\n",
    "# Create submission dataframe\n",
    "submission_df = pd.DataFrame(submission_rows)\n",
    "\n",
    "print(f\"Submission shape: {submission_df.shape}\")\n",
    "print(f\"Expected shape: ({len(test_pivot) * 5}, 2)\")\n",
    "print(submission_df.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "790b67fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "SUBMISSION_NAME = 'submission.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b4dcbdf7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-11T19:12:46.770420Z",
     "iopub.status.busy": "2025-12-11T19:12:46.770195Z",
     "iopub.status.idle": "2025-12-11T19:12:46.777650Z",
     "shell.execute_reply": "2025-12-11T19:12:46.777008Z"
    },
    "papermill": {
     "duration": 0.01308,
     "end_time": "2025-12-11T19:12:46.778684",
     "exception": false,
     "start_time": "2025-12-11T19:12:46.765604",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submission saved to: submission.csv\n"
     ]
    }
   ],
   "source": [
    "# Save submission\n",
    "submission_df.to_csv(SUBMISSION_NAME, index=False)\n",
    "\n",
    "print(f\"Submission saved to: {SUBMISSION_NAME}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb94176b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "databundleVersionId": 14254895,
     "sourceId": 112509,
     "sourceType": "competition"
    },
    {
     "databundleVersionId": 14898463,
     "datasetId": 8990306,
     "sourceId": 14114255,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31192,
   "isGpuEnabled": true,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "image2biomass",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 32.832499,
   "end_time": "2025-12-11T19:12:49.486768",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-12-11T19:12:16.654269",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
