{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "865c1bde",
   "metadata": {
    "papermill": {
     "duration": 0.004115,
     "end_time": "2025-12-11T19:12:20.041103",
     "exception": false,
     "start_time": "2025-12-11T19:12:20.036988",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Distillation. Student Inference\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c4c0636",
   "metadata": {
    "papermill": {
     "duration": 0.003092,
     "end_time": "2025-12-11T19:12:20.047506",
     "exception": false,
     "start_time": "2025-12-11T19:12:20.044414",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Imports\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "e299f95a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-11T19:12:20.054508Z",
     "iopub.status.busy": "2025-12-11T19:12:20.054261Z",
     "iopub.status.idle": "2025-12-11T19:12:38.383713Z",
     "shell.execute_reply": "2025-12-11T19:12:38.382820Z"
    },
    "papermill": {
     "duration": 18.334462,
     "end_time": "2025-12-11T19:12:38.385055",
     "exception": false,
     "start_time": "2025-12-11T19:12:20.050593",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch: 2.9.1+cu128\n",
      "Device: NVIDIA GeForce RTX 5050 Laptop GPU\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import gc\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "\n",
    "import cv2\n",
    "import timm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import pytorch_lightning as pl\n",
    "from tqdm import tqdm\n",
    "from torch.optim import AdamW\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "from transformers import Dinov2Model, Dinov2Config\n",
    "\n",
    "import warnings\n",
    "\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "print(f\"PyTorch: {torch.__version__}\")\n",
    "print(\n",
    "    f\"Device: {torch.cuda.get_device_name(0) if torch.cuda.is_available() else 'CPU'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "077a1548",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-11T19:12:38.393412Z",
     "iopub.status.busy": "2025-12-11T19:12:38.392973Z",
     "iopub.status.idle": "2025-12-11T19:12:38.652255Z",
     "shell.execute_reply": "2025-12-11T19:12:38.651736Z"
    },
    "papermill": {
     "duration": 0.264771,
     "end_time": "2025-12-11T19:12:38.653430",
     "exception": false,
     "start_time": "2025-12-11T19:12:38.388659",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        # print(os.path.join(dirname, filename))\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "89d6f31a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-11T19:12:38.661069Z",
     "iopub.status.busy": "2025-12-11T19:12:38.660833Z",
     "iopub.status.idle": "2025-12-11T19:12:38.674456Z",
     "shell.execute_reply": "2025-12-11T19:12:38.673819Z"
    },
    "papermill": {
     "duration": 0.018698,
     "end_time": "2025-12-11T19:12:38.675573",
     "exception": false,
     "start_time": "2025-12-11T19:12:38.656875",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 1488\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DESCRIPTION_FULL: facebook/dinov2-base-kaggle_train[5]Folds_log_fusion-gating_epochs25_bs16_gradacc1_lr0.0001_wd0.05_dr0.3_hr0.5\n",
      "Effective batch size: 16\n"
     ]
    }
   ],
   "source": [
    "cpu_count = os.cpu_count()\n",
    "NUM_WORKERS = 0\n",
    "\n",
    "LR = 1e-4\n",
    "EPOCHS = 25\n",
    "N_FOLDS = 5\n",
    "GRAD_ACCUM = 1\n",
    "BATCH_SIZE = 16\n",
    "DROPOUT_RATE = 0.3\n",
    "# Weight for distillation loss\n",
    "# Loss = DISTILL_ALPHA * Distillation_Loss + (1 - DISTILL_ALPHA) * Hard_Loss\n",
    "DISTILL_ALPHA = 0.5\n",
    "WEIGHT_DECAY = 0.05\n",
    "HIDDEN_RATIO = 0.5\n",
    "TRAIN_SPLIT_RATIO = 0.02  # Used if N_FOLDS = 0\n",
    "\n",
    "MODEL = 'facebook/dinov2-base'\n",
    "CHECKPOINTS_DIR = f\"./kaggle/input/2head/\"\n",
    "WEIGHTS_PATH = f\"{CHECKPOINTS_DIR}{MODEL.replace('/', '_')}.pth\"\n",
    "PROJECT_NAME = \"csiro-image2biomass-prediction\"\n",
    "# Whether to use OOF soft targets or 100% ensemble soft targets\n",
    "USE_OOF_SOFT_TARGETS = False\n",
    "\n",
    "# Each patch is 1000x1000, resize to 768x768 for vision transformers\n",
    "SIZE = 768\n",
    "USE_LOG_TARGET = True     # Whether to use log1p transformation on target variable\n",
    "FUSION_METHOD = 'gating'  # ('concat', 'mean', 'max') OR 'gating'\n",
    "\n",
    "DESCRIPTION = \"kaggle\" + \\\n",
    "    (f\"_train{TRAIN_SPLIT_RATIO}\" if N_FOLDS == 0 else f\"_train[{N_FOLDS}]Folds\") + (\n",
    "        f\"_log\" if USE_LOG_TARGET else \"\") + f\"_fusion-{FUSION_METHOD}\"\n",
    "DESCRIPTION_FULL = MODEL + \"-\" + DESCRIPTION + \\\n",
    "    f\"_epochs{EPOCHS}_bs{BATCH_SIZE}_gradacc{GRAD_ACCUM}_lr{LR}_wd{WEIGHT_DECAY}_dr{DROPOUT_RATE}_hr{HIDDEN_RATIO}\"\n",
    "SUBMISSION_NAME = f\"{DESCRIPTION_FULL}_submission.csv\"\n",
    "SUBMISSION_ENSEMBLE_NAME = f\"{DESCRIPTION_FULL}_ensemble_submission.csv\"\n",
    "SUBMISSION_MSG = DESCRIPTION_FULL.replace(\"_\", \" \")\n",
    "\n",
    "SEED = 1488\n",
    "torch.manual_seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "pl.seed_everything(SEED)\n",
    "\n",
    "print(\"DESCRIPTION_FULL:\", DESCRIPTION_FULL)\n",
    "print(f\"Effective batch size: {BATCH_SIZE * GRAD_ACCUM}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "13369e6c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-11T19:12:38.683004Z",
     "iopub.status.busy": "2025-12-11T19:12:38.682783Z",
     "iopub.status.idle": "2025-12-11T19:12:38.924229Z",
     "shell.execute_reply": "2025-12-11T19:12:38.923358Z"
    },
    "papermill": {
     "duration": 0.246526,
     "end_time": "2025-12-11T19:12:38.925434",
     "exception": false,
     "start_time": "2025-12-11T19:12:38.678908",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "NUM_WORKERS: 0\n",
      "\n",
      "NVIDIA GeForce RTX 5050 Laptop GPU\n",
      "Memory Usage:\n",
      "Allocated: 0.0 GB\n",
      "Cached:    0.0 GB\n"
     ]
    }
   ],
   "source": [
    "# setting device on GPU if available, else CPU\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Using device:', DEVICE)\n",
    "print('NUM_WORKERS:', NUM_WORKERS)\n",
    "print()\n",
    "\n",
    "# Additional Info when using cuda\n",
    "if DEVICE.type == 'cuda':\n",
    "    # clean GPU memory\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "\n",
    "    # torch.set_float32_matmul_precision('high')\n",
    "\n",
    "    print(torch.cuda.get_device_name(0))\n",
    "    print('Memory Usage:')\n",
    "    print('Allocated:', round(torch.cuda.memory_allocated(0)/1024**3, 1), 'GB')\n",
    "    print('Cached:   ', round(torch.cuda.memory_reserved(0)/1024**3, 1), 'GB')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee0baaac",
   "metadata": {
    "papermill": {
     "duration": 0.003521,
     "end_time": "2025-12-11T19:12:38.932551",
     "exception": false,
     "start_time": "2025-12-11T19:12:38.929030",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Model Architecture\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "6d676db0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-11T19:12:38.940125Z",
     "iopub.status.busy": "2025-12-11T19:12:38.939895Z",
     "iopub.status.idle": "2025-12-11T19:12:38.945085Z",
     "shell.execute_reply": "2025-12-11T19:12:38.944346Z"
    },
    "papermill": {
     "duration": 0.010274,
     "end_time": "2025-12-11T19:12:38.946120",
     "exception": false,
     "start_time": "2025-12-11T19:12:38.935846",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "labels = [\n",
    "    \"Dry_Clover_g\",\n",
    "    \"Dry_Dead_g\",\n",
    "    \"Dry_Green_g\",\n",
    "    \"Dry_Total_g\",\n",
    "    \"GDM_g\"\n",
    "]\n",
    "\n",
    "weights = {\n",
    "    'Dry_Green_g': 0.1,\n",
    "    'Dry_Dead_g': 0.1,\n",
    "    'Dry_Clover_g': 0.1,\n",
    "    'GDM_g': 0.2,\n",
    "    'Dry_Total_g': 0.5,\n",
    "}\n",
    "\n",
    "\n",
    "def competition_metric(y_true, y_pred) -> float:\n",
    "    \"\"\"Function to calculate the competition's official evaluation metric (weighted R2 score).\"\"\"\n",
    "    weights_array = np.array([weights[l] for l in labels])\n",
    "\n",
    "    # Align with this calculation method\n",
    "    y_weighted_mean = np.average(y_true, weights=weights_array, axis=1).mean()\n",
    "\n",
    "    # For ss_res and ss_tot, also take the weighted average on axis=1, then the mean of the result\n",
    "    ss_res = np.average((y_true - y_pred)**2,\n",
    "                        weights=weights_array, axis=1).mean()\n",
    "    ss_tot = np.average((y_true - y_weighted_mean)**2,\n",
    "                        weights=weights_array, axis=1).mean()\n",
    "\n",
    "    return 1 - ss_res / ss_tot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "da81bbf2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-11T19:12:38.953854Z",
     "iopub.status.busy": "2025-12-11T19:12:38.953631Z",
     "iopub.status.idle": "2025-12-11T19:12:38.961408Z",
     "shell.execute_reply": "2025-12-11T19:12:38.960794Z"
    },
    "papermill": {
     "duration": 0.012976,
     "end_time": "2025-12-11T19:12:38.962524",
     "exception": false,
     "start_time": "2025-12-11T19:12:38.949548",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class BiomassTeacherModelPatches(pl.LightningModule):\n",
    "    \"\"\"\n",
    "    Dual-head teacher model with patch-level predictions using DINOv2.\n",
    "    - Image-only head: for distillation to student\n",
    "    - Privileged head: with tabular features for maximum accuracy\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        backbone_name: str = 'facebook/dinov2-base',\n",
    "        tabular_dim: int = 10,\n",
    "        num_targets: int = 3,\n",
    "        lr: float = 1e-4,\n",
    "        weight_decay: float = 1e-5,\n",
    "        hidden_ratio: float = 0.5,\n",
    "        dropout: float = 0.2,\n",
    "        fusion_method: str = 'gating',\n",
    "        use_log_target: bool = True,\n",
    "        tabular_dropout_prob: float = 0.3,\n",
    "        lambda_cons: float = 0.5,\n",
    "        pretrained_backbone: bool = True,\n",
    "        backbone_weights_path: str | None = None,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            backbone_name: DINOv2 model name\n",
    "            tabular_dim: dimension of tabular features\n",
    "            num_targets: number of regression targets (3)\n",
    "            lr: learning rate\n",
    "            weight_decay: weight decay for optimizer\n",
    "            hidden_ratio: ratio for hidden layer size\n",
    "            dropout: dropout probability\n",
    "            fusion_method: how to use tabular features ('gating' or 'concat')\n",
    "            use_log_target: if True, predict log1p transformed targets\n",
    "            tabular_dropout_prob: probability of zeroing tabular features during training\n",
    "            lambda_cons: weight for consistency loss between image-only and privileged heads\n",
    "            pretrained_backbone: whether to download/use pretrained backbone weights\n",
    "            backbone_weights_path: optional path to local backbone weights (state_dict)\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "\n",
    "        # Load DINOv2 backbone (supports offline)\n",
    "        if pretrained_backbone:\n",
    "            self.backbone = Dinov2Model.from_pretrained(backbone_name)\n",
    "        else:\n",
    "            try:\n",
    "                config = Dinov2Config.from_pretrained(\n",
    "                    backbone_name, local_files_only=True)\n",
    "            except Exception:\n",
    "                config = Dinov2Config()\n",
    "            self.backbone = Dinov2Model(config)\n",
    "            if backbone_weights_path and os.path.exists(backbone_weights_path):\n",
    "                state_dict = torch.load(\n",
    "                    backbone_weights_path, map_location='cpu')\n",
    "                missing, unexpected = self.backbone.load_state_dict(\n",
    "                    state_dict, strict=False)\n",
    "                if missing or unexpected:\n",
    "                    print(\n",
    "                        f\"Backbone load - missing: {missing}, unexpected: {unexpected}\")\n",
    "\n",
    "        self.backbone.train()\n",
    "\n",
    "        self.hidden_dim = self.backbone.config.hidden_size\n",
    "        self.patch_size = self.backbone.config.patch_size\n",
    "\n",
    "        self.lr = lr\n",
    "        self.weight_decay = weight_decay\n",
    "        self.fusion_method = fusion_method\n",
    "        self.use_log_target = use_log_target\n",
    "        self.tabular_dropout_prob = tabular_dropout_prob\n",
    "        self.lambda_cons = lambda_cons\n",
    "\n",
    "        # Mode for prediction: 'img' for image-only, 'priv' for privileged\n",
    "        self.prediction_mode = 'priv'\n",
    "\n",
    "        # Patch-level MLPs (shared across all patches)\n",
    "        hidden_size = max(32, int(self.hidden_dim * hidden_ratio))\n",
    "\n",
    "        def make_patch_head():\n",
    "            \"\"\"MLP for patch-level prediction\"\"\"\n",
    "            return nn.Sequential(\n",
    "                nn.Linear(self.hidden_dim, hidden_size),\n",
    "                nn.LayerNorm(hidden_size),\n",
    "                nn.ReLU(inplace=True),\n",
    "                nn.Dropout(dropout),\n",
    "                nn.Linear(hidden_size, 1)\n",
    "            )\n",
    "\n",
    "        # IMAGE-ONLY heads (for distillation to student)\n",
    "        self.img_head_green = make_patch_head()\n",
    "        self.img_head_clover = make_patch_head()\n",
    "        self.img_head_dead = make_patch_head()\n",
    "\n",
    "        # PRIVILEGED heads (with tabular features for best accuracy)\n",
    "        self.priv_head_green = make_patch_head()\n",
    "        self.priv_head_clover = make_patch_head()\n",
    "        self.priv_head_dead = make_patch_head()\n",
    "\n",
    "        # Tabular features fusion for privileged branch\n",
    "        if self.fusion_method == 'gating':\n",
    "            self.tabular_gate = nn.Sequential(\n",
    "                nn.Linear(tabular_dim, hidden_size),\n",
    "                nn.ReLU(inplace=True),\n",
    "                nn.Linear(hidden_size, 1),\n",
    "                nn.Sigmoid()\n",
    "            )\n",
    "        elif self.fusion_method == 'concat':\n",
    "            self.fusion_layer = nn.Sequential(\n",
    "                nn.Linear(3 + tabular_dim, hidden_size),\n",
    "                nn.ReLU(inplace=True),\n",
    "                nn.Dropout(dropout),\n",
    "                nn.Linear(hidden_size, 3)\n",
    "            )\n",
    "\n",
    "        self.validation_step_outputs = []\n",
    "\n",
    "        msg = (\n",
    "            \"Dual-Head Patch-level Teacher Model initialized:\\n\"\n",
    "            f\"backbone={backbone_name}, hidden_dim={self.hidden_dim}, patch_size={self.patch_size},\\n\"\n",
    "            f\"fusion_method={fusion_method}, use_log_target={use_log_target},\\n\"\n",
    "            f\"tabular_dropout_prob={tabular_dropout_prob}, lambda_cons={lambda_cons}\"\n",
    "        )\n",
    "        print(msg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "bc59dd50",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-11T19:12:38.996769Z",
     "iopub.status.busy": "2025-12-11T19:12:38.996566Z",
     "iopub.status.idle": "2025-12-11T19:12:40.693006Z",
     "shell.execute_reply": "2025-12-11T19:12:40.692116Z"
    },
    "papermill": {
     "duration": 1.701985,
     "end_time": "2025-12-11T19:12:40.694624",
     "exception": false,
     "start_time": "2025-12-11T19:12:38.992639",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "config = Dinov2Config.from_pretrained(MODEL)\n",
    "temp_backbone = Dinov2Model.from_pretrained(MODEL, config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "20603376",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # save model weights to file\n",
    "# torch.save(temp_backbone.state_dict(), WEIGHTS_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "6be3cb04",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs_size = config.image_size\n",
    "mean = [0.485, 0.456, 0.406]\n",
    "std = [0.229, 0.224, 0.225]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "1586a378",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-11T19:12:40.704683Z",
     "iopub.status.busy": "2025-12-11T19:12:40.704457Z",
     "iopub.status.idle": "2025-12-11T19:12:40.978567Z",
     "shell.execute_reply": "2025-12-11T19:12:40.977920Z"
    },
    "papermill": {
     "duration": 0.280794,
     "end_time": "2025-12-11T19:12:40.979953",
     "exception": false,
     "start_time": "2025-12-11T19:12:40.699159",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Backbone expected input size: 518, using SIZE=518\n",
      "Backbone expected mean: [0.485, 0.456, 0.406], std: [0.229, 0.224, 0.225]\n",
      "768\n"
     ]
    }
   ],
   "source": [
    "SIZE = inputs_size\n",
    "print(f\"Backbone expected input size: {inputs_size}, using SIZE={SIZE}\")\n",
    "print(f\"Backbone expected mean: {mean}, std: {std}\")\n",
    "\n",
    "# Get backbone output dimension\n",
    "with torch.no_grad():\n",
    "    dummy = torch.randn(1, 3, SIZE, SIZE)\n",
    "    outputs = temp_backbone(dummy)\n",
    "    feat_dim = outputs.last_hidden_state.sum(\n",
    "        dim=1).shape[1]  # Average pooling\n",
    "    print(feat_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "7ad7904b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-11T19:12:40.988400Z",
     "iopub.status.busy": "2025-12-11T19:12:40.988135Z",
     "iopub.status.idle": "2025-12-11T19:12:40.992065Z",
     "shell.execute_reply": "2025-12-11T19:12:40.991493Z"
    },
    "papermill": {
     "duration": 0.009317,
     "end_time": "2025-12-11T19:12:40.993095",
     "exception": false,
     "start_time": "2025-12-11T19:12:40.983778",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "student_val_transform = transforms.Compose([\n",
    "    transforms.Resize((SIZE, SIZE)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=mean, std=std)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "34f57baa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TTA helpers\n",
    "TTA_TYPES = ['id', 'hflip', 'vflip', 'hvflip']\n",
    "\n",
    "\n",
    "def apply_tta(left: torch.Tensor, right: torch.Tensor, tta: str) -> tuple[torch.Tensor, torch.Tensor]:\n",
    "    \"\"\"Apply simple flip-based TTA to both patches.\"\"\"\n",
    "    if tta == 'hflip':\n",
    "        return torch.flip(left, dims=[2]), torch.flip(right, dims=[2])\n",
    "    if tta == 'vflip':\n",
    "        return torch.flip(left, dims=[1]), torch.flip(right, dims=[1])\n",
    "    if tta == 'hvflip':\n",
    "        return torch.flip(left, dims=[1, 2]), torch.flip(right, dims=[1, 2])\n",
    "    return left, right"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "5a5a5df0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stack_targets_from_preds(preds_3: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"Given [B,3] (clover, dead, green) produce [B,5] ordered targets.\"\"\"\n",
    "    clover = preds_3[:, 0]\n",
    "    dead = preds_3[:, 1]\n",
    "    green = preds_3[:, 2]\n",
    "    total = green + dead + clover\n",
    "    gdm = clover + green\n",
    "    return torch.stack([clover, dead, green, total, gdm], dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "ac2c6e1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_model_batch(model: BiomassTeacherModelPatches, batch: dict, tta_types: list[str]) -> torch.Tensor:\n",
    "    \"\"\"Run model over TTA variants and average. Returns [B,5].\"\"\"\n",
    "    model_preds = []\n",
    "    for tta in tta_types:\n",
    "        left_t, right_t = apply_tta(\n",
    "            batch['left_image'], batch['right_image'], tta)\n",
    "        tta_batch = {\n",
    "            'left_image': left_t,\n",
    "            'right_image': right_t,\n",
    "            'tabular': batch['tabular'],\n",
    "        }\n",
    "        preds_3 = model.predict_step(tta_batch, 0)  # [B,3]\n",
    "        model_preds.append(stack_targets_from_preds(preds_3))\n",
    "    return torch.stack(model_preds, dim=0).mean(dim=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b494fd8",
   "metadata": {
    "papermill": {
     "duration": 0.003342,
     "end_time": "2025-12-11T19:12:40.999896",
     "exception": false,
     "start_time": "2025-12-11T19:12:40.996554",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Inference on Test Set\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "c97fc78c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-11T19:12:41.007903Z",
     "iopub.status.busy": "2025-12-11T19:12:41.007699Z",
     "iopub.status.idle": "2025-12-11T19:12:41.010988Z",
     "shell.execute_reply": "2025-12-11T19:12:41.010419Z"
    },
    "papermill": {
     "duration": 0.008328,
     "end_time": "2025-12-11T19:12:41.012015",
     "exception": false,
     "start_time": "2025-12-11T19:12:41.003687",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "PATH_DATA = './kaggle/input/csiro-biomass'\n",
    "STUDENT_MODELS_DIR = './kaggle/input/2head'\n",
    "PATH_TEST_CSV = os.path.join(PATH_DATA, 'test.csv')\n",
    "PATH_TEST_IMG = os.path.join(PATH_DATA, 'test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "14d08303",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-11T19:12:41.019779Z",
     "iopub.status.busy": "2025-12-11T19:12:41.019584Z",
     "iopub.status.idle": "2025-12-11T19:12:41.056030Z",
     "shell.execute_reply": "2025-12-11T19:12:41.055336Z"
    },
    "papermill": {
     "duration": 0.041627,
     "end_time": "2025-12-11T19:12:41.057105",
     "exception": false,
     "start_time": "2025-12-11T19:12:41.015478",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set size: 1\n",
      "              image_path                   sample_id   target_name\n",
      "0  test/ID1001187975.jpg  ID1001187975__Dry_Clover_g  Dry_Clover_g\n"
     ]
    }
   ],
   "source": [
    "# Load test CSV\n",
    "test_df = pd.read_csv(PATH_TEST_CSV)\n",
    "test_df = test_df[~test_df['target_name'].isin(['Dry_Total_g', 'GDM_g'])]\n",
    "\n",
    "# Pivot to one row per image\n",
    "test_pivot = test_df.pivot_table(\n",
    "    index='image_path',\n",
    "    aggfunc='first'\n",
    ").reset_index()\n",
    "\n",
    "print(f\"Test set size: {len(test_pivot)}\")\n",
    "print(test_pivot.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "a254b7d0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-11T19:12:41.065121Z",
     "iopub.status.busy": "2025-12-11T19:12:41.064924Z",
     "iopub.status.idle": "2025-12-11T19:12:45.737686Z",
     "shell.execute_reply": "2025-12-11T19:12:45.736663Z"
    },
    "papermill": {
     "duration": 4.678651,
     "end_time": "2025-12-11T19:12:45.739256",
     "exception": false,
     "start_time": "2025-12-11T19:12:41.060605",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Checkpoint discovery and loading\n",
    "def parse_metric_from_filename(filename: str) -> float:\n",
    "    \"\"\"Extract val_comp_metric_img from filename like ...val_comp_metric_img=0.7129.ckpt.\"\"\"\n",
    "    try:\n",
    "        metric_part = filename.split('val_comp_metric_img=')[-1]\n",
    "        return float(metric_part.replace('.ckpt', ''))\n",
    "    except Exception:\n",
    "        return -float('inf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "795f1792",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_student_model(ckpt_path: str, backbone_weights_path: str | None = None) -> BiomassTeacherModelPatches:\n",
    "    \"\"\"Load model from checkpoint without internet (uses saved weights).\"\"\"\n",
    "    checkpoint = torch.load(ckpt_path, map_location='cpu')\n",
    "    hparams = checkpoint['hyper_parameters']\n",
    "\n",
    "    model = BiomassTeacherModelPatches(\n",
    "        backbone_name=hparams['backbone_name'],\n",
    "        tabular_dim=hparams.get('tabular_dim', 0),\n",
    "        num_targets=hparams['num_targets'],\n",
    "        lr=hparams['lr'],\n",
    "        weight_decay=hparams['weight_decay'],\n",
    "        hidden_ratio=hparams['hidden_ratio'],\n",
    "        dropout=hparams['dropout'],\n",
    "        fusion_method=hparams['fusion_method'],\n",
    "        use_log_target=hparams['use_log_target'],\n",
    "        tabular_dropout_prob=hparams.get('tabular_dropout_prob', 0.0),\n",
    "        lambda_cons=hparams.get('lambda_cons', 0.0),\n",
    "        pretrained_backbone=False,\n",
    "        backbone_weights_path=backbone_weights_path,\n",
    "    )\n",
    "\n",
    "    model.load_state_dict(checkpoint['state_dict'])\n",
    "    model.prediction_mode = 'img'  # image-only branch for inference\n",
    "    model.eval()\n",
    "    model.to(DEVICE)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "4e8fccc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 5 student checkpoints:\n",
      "  - f0dinov2-base-local_train(5)Folds_log_fusion-gating_epochs30_bs4_gradacc4_lr3e-05_wd0.05_dr0.2_hr0.5-fold0-epoch=23-val_loss_img=0.0000-val_comp_metric_img=0.7129.ckpt\n",
      "  - f1dinov2-base-local_train(5)Folds_log_fusion-gating_epochs30_bs4_gradacc4_lr3e-05_wd0.05_dr0.2_hr0.5-fold1-epoch=24-val_loss_img=0.0000-val_comp_metric_img=0.726.ckpt\n",
      "  - f2dinov2-base-local_train(5)Folds_log_fusion-gating_epochs30_bs4_gradacc4_lr3e-05_wd0.05_dr0.2_hr0.5-fold2-epoch=28-val_loss_img=0.0000-val_comp_metric_img=0.773.ckpt\n",
      "  - f3dinov2-base-local_train(5)Folds_log_fusion-gating_epochs30_bs4_gradacc4_lr3e-05_wd0.05_dr0.2_hr0.5-fold3-epoch=25-val_loss_img=0.0000-val_comp_metric_img=0.725.ckpt\n",
      "  - f4dinov2-base-local_train(5)Folds_log_fusion-gating_epochs30_bs4_gradacc4_lr3e-05_wd0.05_dr0.2_hr0.5-fold4-epoch=26-val_loss_img=0.0000-val_comp_metric_img=0.7169.ckpt\n"
     ]
    }
   ],
   "source": [
    "# Discover fold checkpoints and best overall\n",
    "ckpt_files = sorted([\n",
    "    f for f in os.listdir(STUDENT_MODELS_DIR) if f.endswith('.ckpt')\n",
    "])\n",
    "print(f\"Found {len(ckpt_files)} student checkpoints:\")\n",
    "for f in ckpt_files:\n",
    "    print(f\"  - {f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "dff816d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select 5 folds: filenames starting with f{fold}\n",
    "fold_ckpts = []\n",
    "for fold_id in range(N_FOLDS):\n",
    "    candidates = [f for f in ckpt_files if f.startswith(f\"f{fold_id}\")]\n",
    "    if not candidates:\n",
    "        continue\n",
    "    # pick best metric per fold\n",
    "    candidates.sort(key=parse_metric_from_filename, reverse=True)\n",
    "    fold_ckpts.append(os.path.join(STUDENT_MODELS_DIR, candidates[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "d6e10f33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected fold checkpoints:\n",
      "  f0dinov2-base-local_train(5)Folds_log_fusion-gating_epochs30_bs4_gradacc4_lr3e-05_wd0.05_dr0.2_hr0.5-fold0-epoch=23-val_loss_img=0.0000-val_comp_metric_img=0.7129.ckpt\n",
      "  f1dinov2-base-local_train(5)Folds_log_fusion-gating_epochs30_bs4_gradacc4_lr3e-05_wd0.05_dr0.2_hr0.5-fold1-epoch=24-val_loss_img=0.0000-val_comp_metric_img=0.726.ckpt\n",
      "  f2dinov2-base-local_train(5)Folds_log_fusion-gating_epochs30_bs4_gradacc4_lr3e-05_wd0.05_dr0.2_hr0.5-fold2-epoch=28-val_loss_img=0.0000-val_comp_metric_img=0.773.ckpt\n",
      "  f3dinov2-base-local_train(5)Folds_log_fusion-gating_epochs30_bs4_gradacc4_lr3e-05_wd0.05_dr0.2_hr0.5-fold3-epoch=25-val_loss_img=0.0000-val_comp_metric_img=0.725.ckpt\n",
      "  f4dinov2-base-local_train(5)Folds_log_fusion-gating_epochs30_bs4_gradacc4_lr3e-05_wd0.05_dr0.2_hr0.5-fold4-epoch=26-val_loss_img=0.0000-val_comp_metric_img=0.7169.ckpt\n",
      "Best checkpoint: f2dinov2-base-local_train(5)Folds_log_fusion-gating_epochs30_bs4_gradacc4_lr3e-05_wd0.05_dr0.2_hr0.5-fold2-epoch=28-val_loss_img=0.0000-val_comp_metric_img=0.773.ckpt\n"
     ]
    }
   ],
   "source": [
    "# Best overall by metric\n",
    "best_ckpt = None\n",
    "if ckpt_files:\n",
    "    best_ckpt = os.path.join(\n",
    "        STUDENT_MODELS_DIR,\n",
    "        sorted(ckpt_files, key=parse_metric_from_filename, reverse=True)[0]\n",
    "    )\n",
    "\n",
    "print(\"Selected fold checkpoints:\")\n",
    "for p in fold_ckpts:\n",
    "    print(f\"  {os.path.basename(p)}\")\n",
    "print(\n",
    "    f\"Best checkpoint: {os.path.basename(best_ckpt) if best_ckpt else 'None'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "480e90a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loading fold model: f0dinov2-base-local_train(5)Folds_log_fusion-gating_epochs30_bs4_gradacc4_lr3e-05_wd0.05_dr0.2_hr0.5-fold0-epoch=23-val_loss_img=0.0000-val_comp_metric_img=0.7129.ckpt\n",
      "Dual-Head Patch-level Teacher Model initialized:\n",
      "backbone=facebook/dinov2-base, hidden_dim=768, patch_size=14,\n",
      "fusion_method=gating, use_log_target=True,\n",
      "tabular_dropout_prob=0.3, lambda_cons=0.5\n",
      "\n",
      "Loading fold model: f1dinov2-base-local_train(5)Folds_log_fusion-gating_epochs30_bs4_gradacc4_lr3e-05_wd0.05_dr0.2_hr0.5-fold1-epoch=24-val_loss_img=0.0000-val_comp_metric_img=0.726.ckpt\n",
      "Dual-Head Patch-level Teacher Model initialized:\n",
      "backbone=facebook/dinov2-base, hidden_dim=768, patch_size=14,\n",
      "fusion_method=gating, use_log_target=True,\n",
      "tabular_dropout_prob=0.3, lambda_cons=0.5\n",
      "\n",
      "Loading fold model: f2dinov2-base-local_train(5)Folds_log_fusion-gating_epochs30_bs4_gradacc4_lr3e-05_wd0.05_dr0.2_hr0.5-fold2-epoch=28-val_loss_img=0.0000-val_comp_metric_img=0.773.ckpt\n",
      "Dual-Head Patch-level Teacher Model initialized:\n",
      "backbone=facebook/dinov2-base, hidden_dim=768, patch_size=14,\n",
      "fusion_method=gating, use_log_target=True,\n",
      "tabular_dropout_prob=0.3, lambda_cons=0.5\n",
      "\n",
      "Loading fold model: f3dinov2-base-local_train(5)Folds_log_fusion-gating_epochs30_bs4_gradacc4_lr3e-05_wd0.05_dr0.2_hr0.5-fold3-epoch=25-val_loss_img=0.0000-val_comp_metric_img=0.725.ckpt\n",
      "Dual-Head Patch-level Teacher Model initialized:\n",
      "backbone=facebook/dinov2-base, hidden_dim=768, patch_size=14,\n",
      "fusion_method=gating, use_log_target=True,\n",
      "tabular_dropout_prob=0.3, lambda_cons=0.5\n",
      "\n",
      "Loading fold model: f4dinov2-base-local_train(5)Folds_log_fusion-gating_epochs30_bs4_gradacc4_lr3e-05_wd0.05_dr0.2_hr0.5-fold4-epoch=26-val_loss_img=0.0000-val_comp_metric_img=0.7169.ckpt\n",
      "Dual-Head Patch-level Teacher Model initialized:\n",
      "backbone=facebook/dinov2-base, hidden_dim=768, patch_size=14,\n",
      "fusion_method=gating, use_log_target=True,\n",
      "tabular_dropout_prob=0.3, lambda_cons=0.5\n",
      "\n",
      "Loading best model: f2dinov2-base-local_train(5)Folds_log_fusion-gating_epochs30_bs4_gradacc4_lr3e-05_wd0.05_dr0.2_hr0.5-fold2-epoch=28-val_loss_img=0.0000-val_comp_metric_img=0.773.ckpt\n",
      "Dual-Head Patch-level Teacher Model initialized:\n",
      "backbone=facebook/dinov2-base, hidden_dim=768, patch_size=14,\n",
      "fusion_method=gating, use_log_target=True,\n",
      "tabular_dropout_prob=0.3, lambda_cons=0.5\n",
      "\n",
      "Successfully loaded 6 student models\n",
      "Ready for offline inference on Kaggle!\n"
     ]
    }
   ],
   "source": [
    "# Load models WITHOUT internet (offline inference on Kaggle)\n",
    "student_models = []\n",
    "for ckpt_path in fold_ckpts:\n",
    "    print(f\"\\nLoading fold model: {os.path.basename(ckpt_path)}\")\n",
    "    student_models.append(load_student_model(\n",
    "        ckpt_path, backbone_weights_path=WEIGHTS_PATH))\n",
    "\n",
    "if best_ckpt:\n",
    "    print(f\"\\nLoading best model: {os.path.basename(best_ckpt)}\")\n",
    "    student_models.append(load_student_model(\n",
    "        best_ckpt, backbone_weights_path=WEIGHTS_PATH))\n",
    "\n",
    "print(f\"\\nSuccessfully loaded {len(student_models)} student models\")\n",
    "print(\"Ready for offline inference on Kaggle!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "8fb839a9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-11T19:12:45.749267Z",
     "iopub.status.busy": "2025-12-11T19:12:45.749041Z",
     "iopub.status.idle": "2025-12-11T19:12:45.759525Z",
     "shell.execute_reply": "2025-12-11T19:12:45.758695Z"
    },
    "papermill": {
     "duration": 0.016971,
     "end_time": "2025-12-11T19:12:45.760856",
     "exception": false,
     "start_time": "2025-12-11T19:12:45.743885",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create test dataset\n",
    "class BiomassTestDataset(Dataset):\n",
    "    \"\"\"Test dataset for inference - no targets needed.\"\"\"\n",
    "\n",
    "    def __init__(self, df: pd.DataFrame, img_dir: str, transform=None):\n",
    "        self.df = df.reset_index(drop=True)\n",
    "        self.img_dir = img_dir\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "\n",
    "        # Load image\n",
    "        img_path = os.path.join(\n",
    "            self.img_dir, row['image_path'].replace('test/', ''))\n",
    "        image = cv2.imread(img_path)\n",
    "\n",
    "        if image is None:\n",
    "            raise FileNotFoundError(f\"Cannot load image: {img_path}\")\n",
    "\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        # Split into left and right patches\n",
    "        h, w, c = image.shape\n",
    "        mid_w = w // 2\n",
    "\n",
    "        left_patch = image[:, :mid_w, :]\n",
    "        right_patch = image[:, mid_w:, :]\n",
    "\n",
    "        # Convert to PIL\n",
    "        left_pil = Image.fromarray(left_patch)\n",
    "        right_pil = Image.fromarray(right_patch)\n",
    "\n",
    "        # Apply transforms\n",
    "        if self.transform:\n",
    "            left_tensor = self.transform(left_pil)\n",
    "            right_tensor = self.transform(right_pil)\n",
    "        else:\n",
    "            left_tensor = transforms.ToTensor()(left_pil)\n",
    "            right_tensor = transforms.ToTensor()(right_pil)\n",
    "\n",
    "        return {\n",
    "            'left_image': left_tensor,\n",
    "            'right_image': right_tensor,\n",
    "            'image_id': row['image_path'].split('/')[-1].replace('.jpg', ''),\n",
    "            'state': row.get('State', None)\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "a88be98f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loader created: 1 batches\n"
     ]
    }
   ],
   "source": [
    "# Create test dataloader\n",
    "test_dataset = BiomassTestDataset(\n",
    "    df=test_pivot,\n",
    "    img_dir=PATH_TEST_IMG,\n",
    "    transform=student_val_transform\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=BATCH_SIZE * 2,\n",
    "    shuffle=False,\n",
    "    num_workers=min(NUM_WORKERS, 4),\n",
    "    pin_memory=True if torch.cuda.is_available() else False\n",
    ")\n",
    "\n",
    "print(f\"Test loader created: {len(test_loader)} batches\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "5f3f8267",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-11T19:12:45.770355Z",
     "iopub.status.busy": "2025-12-11T19:12:45.770150Z",
     "iopub.status.idle": "2025-12-11T19:12:46.742565Z",
     "shell.execute_reply": "2025-12-11T19:12:46.741710Z"
    },
    "papermill": {
     "duration": 0.978602,
     "end_time": "2025-12-11T19:12:46.743771",
     "exception": false,
     "start_time": "2025-12-11T19:12:45.765169",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running inference on test set...\n",
      "Using tabular_dim=21 for inference\n"
     ]
    }
   ],
   "source": [
    "# Run inference on test set with TTA and WA rule\n",
    "print(\"Running inference on test set...\")\n",
    "\n",
    "all_predictions = []\n",
    "all_image_ids = []\n",
    "all_states = []\n",
    "\n",
    "if len(student_models) == 0:\n",
    "    raise RuntimeError(\"No student models loaded for inference\")\n",
    "\n",
    "tabular_dim = student_models[0].hparams.tabular_dim\n",
    "\n",
    "print(f\"Using tabular_dim={tabular_dim} for inference\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "900254a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inference:   0%|          | 0/1 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "default_collate: batch must contain tensors, numpy arrays, numbers, dicts or lists; found <class 'NoneType'>",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\_GitHub\\CSIRO-Image2Biomass-Prediction\\.venv\\Lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:172\u001b[39m, in \u001b[36mcollate\u001b[39m\u001b[34m(batch, collate_fn_map)\u001b[39m\n\u001b[32m    169\u001b[39m clone = copy.copy(elem)\n\u001b[32m    170\u001b[39m clone.update(\n\u001b[32m    171\u001b[39m     {\n\u001b[32m--> \u001b[39m\u001b[32m172\u001b[39m         key: \u001b[43mcollate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    173\u001b[39m \u001b[43m            \u001b[49m\u001b[43m[\u001b[49m\u001b[43md\u001b[49m\u001b[43m[\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43md\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcollate_fn_map\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcollate_fn_map\u001b[49m\n\u001b[32m    174\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    175\u001b[39m         \u001b[38;5;28;01mfor\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m elem\n\u001b[32m    176\u001b[39m     }\n\u001b[32m    177\u001b[39m )\n\u001b[32m    178\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m clone\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\_GitHub\\CSIRO-Image2Biomass-Prediction\\.venv\\Lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:240\u001b[39m, in \u001b[36mcollate\u001b[39m\u001b[34m(batch, collate_fn_map)\u001b[39m\n\u001b[32m    235\u001b[39m             \u001b[38;5;28;01mreturn\u001b[39;00m [\n\u001b[32m    236\u001b[39m                 collate(samples, collate_fn_map=collate_fn_map)\n\u001b[32m    237\u001b[39m                 \u001b[38;5;28;01mfor\u001b[39;00m samples \u001b[38;5;129;01min\u001b[39;00m transposed\n\u001b[32m    238\u001b[39m             ]\n\u001b[32m--> \u001b[39m\u001b[32m240\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(default_collate_err_msg_format.format(elem_type))\n",
      "\u001b[31mTypeError\u001b[39m: default_collate: batch must contain tensors, numpy arrays, numbers, dicts or lists; found <class 'NoneType'>",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[72]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m torch.no_grad():\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mbatch_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtqdm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtest_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdesc\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mInference\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m      3\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstates\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpop\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mstate\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m      4\u001b[39m \u001b[43m        \u001b[49m\u001b[43mall_states\u001b[49m\u001b[43m.\u001b[49m\u001b[43mextend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstates\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\_GitHub\\CSIRO-Image2Biomass-Prediction\\.venv\\Lib\\site-packages\\tqdm\\std.py:1181\u001b[39m, in \u001b[36mtqdm.__iter__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1178\u001b[39m time = \u001b[38;5;28mself\u001b[39m._time\n\u001b[32m   1180\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1181\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43miterable\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   1182\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01myield\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\n\u001b[32m   1183\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Update and possibly print the progressbar.\u001b[39;49;00m\n\u001b[32m   1184\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;49;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\_GitHub\\CSIRO-Image2Biomass-Prediction\\.venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:732\u001b[39m, in \u001b[36m_BaseDataLoaderIter.__next__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    729\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    730\u001b[39m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[32m    731\u001b[39m     \u001b[38;5;28mself\u001b[39m._reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m732\u001b[39m data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    733\u001b[39m \u001b[38;5;28mself\u001b[39m._num_yielded += \u001b[32m1\u001b[39m\n\u001b[32m    734\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    735\u001b[39m     \u001b[38;5;28mself\u001b[39m._dataset_kind == _DatasetKind.Iterable\n\u001b[32m    736\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    737\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._num_yielded > \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called\n\u001b[32m    738\u001b[39m ):\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\_GitHub\\CSIRO-Image2Biomass-Prediction\\.venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:788\u001b[39m, in \u001b[36m_SingleProcessDataLoaderIter._next_data\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    786\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    787\u001b[39m     index = \u001b[38;5;28mself\u001b[39m._next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m788\u001b[39m     data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[32m    789\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._pin_memory:\n\u001b[32m    790\u001b[39m         data = _utils.pin_memory.pin_memory(data, \u001b[38;5;28mself\u001b[39m._pin_memory_device)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\_GitHub\\CSIRO-Image2Biomass-Prediction\\.venv\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:55\u001b[39m, in \u001b[36m_MapDatasetFetcher.fetch\u001b[39m\u001b[34m(self, possibly_batched_index)\u001b[39m\n\u001b[32m     53\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     54\u001b[39m     data = \u001b[38;5;28mself\u001b[39m.dataset[possibly_batched_index]\n\u001b[32m---> \u001b[39m\u001b[32m55\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcollate_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\_GitHub\\CSIRO-Image2Biomass-Prediction\\.venv\\Lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:398\u001b[39m, in \u001b[36mdefault_collate\u001b[39m\u001b[34m(batch)\u001b[39m\n\u001b[32m    337\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdefault_collate\u001b[39m(batch):\n\u001b[32m    338\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33mr\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    339\u001b[39m \u001b[33;03m    Take in a batch of data and put the elements within the batch into a tensor with an additional outer dimension - batch size.\u001b[39;00m\n\u001b[32m    340\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m    396\u001b[39m \u001b[33;03m        >>> default_collate(batch)  # Handle `CustomType` automatically\u001b[39;00m\n\u001b[32m    397\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m398\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcollate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcollate_fn_map\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdefault_collate_fn_map\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\_GitHub\\CSIRO-Image2Biomass-Prediction\\.venv\\Lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:192\u001b[39m, in \u001b[36mcollate\u001b[39m\u001b[34m(batch, collate_fn_map)\u001b[39m\n\u001b[32m    180\u001b[39m             \u001b[38;5;28;01mreturn\u001b[39;00m elem_type(\n\u001b[32m    181\u001b[39m                 {\n\u001b[32m    182\u001b[39m                     key: collate(\n\u001b[32m   (...)\u001b[39m\u001b[32m    186\u001b[39m                 }\n\u001b[32m    187\u001b[39m             )\n\u001b[32m    188\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[32m    189\u001b[39m         \u001b[38;5;66;03m# The mapping type may not support `copy()` / `update(mapping)`\u001b[39;00m\n\u001b[32m    190\u001b[39m         \u001b[38;5;66;03m# or `__init__(iterable)`.\u001b[39;00m\n\u001b[32m    191\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m {\n\u001b[32m--> \u001b[39m\u001b[32m192\u001b[39m             key: \u001b[43mcollate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43md\u001b[49m\u001b[43m[\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43md\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcollate_fn_map\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcollate_fn_map\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    193\u001b[39m             \u001b[38;5;28;01mfor\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m elem\n\u001b[32m    194\u001b[39m         }\n\u001b[32m    195\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(elem, \u001b[38;5;28mtuple\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(elem, \u001b[33m\"\u001b[39m\u001b[33m_fields\u001b[39m\u001b[33m\"\u001b[39m):  \u001b[38;5;66;03m# namedtuple\u001b[39;00m\n\u001b[32m    196\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m elem_type(\n\u001b[32m    197\u001b[39m         *(\n\u001b[32m    198\u001b[39m             collate(samples, collate_fn_map=collate_fn_map)\n\u001b[32m    199\u001b[39m             \u001b[38;5;28;01mfor\u001b[39;00m samples \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(*batch)\n\u001b[32m    200\u001b[39m         )\n\u001b[32m    201\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\_GitHub\\CSIRO-Image2Biomass-Prediction\\.venv\\Lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:240\u001b[39m, in \u001b[36mcollate\u001b[39m\u001b[34m(batch, collate_fn_map)\u001b[39m\n\u001b[32m    232\u001b[39m         \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[32m    233\u001b[39m             \u001b[38;5;66;03m# The sequence type may not support `copy()` / `__setitem__(index, item)`\u001b[39;00m\n\u001b[32m    234\u001b[39m             \u001b[38;5;66;03m# or `__init__(iterable)` (e.g., `range`).\u001b[39;00m\n\u001b[32m    235\u001b[39m             \u001b[38;5;28;01mreturn\u001b[39;00m [\n\u001b[32m    236\u001b[39m                 collate(samples, collate_fn_map=collate_fn_map)\n\u001b[32m    237\u001b[39m                 \u001b[38;5;28;01mfor\u001b[39;00m samples \u001b[38;5;129;01min\u001b[39;00m transposed\n\u001b[32m    238\u001b[39m             ]\n\u001b[32m--> \u001b[39m\u001b[32m240\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(default_collate_err_msg_format.format(elem_type))\n",
      "\u001b[31mTypeError\u001b[39m: default_collate: batch must contain tensors, numpy arrays, numbers, dicts or lists; found <class 'NoneType'>"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    for batch_idx, batch in enumerate(tqdm(test_loader, desc=\"Inference\")):\n",
    "        states = batch.pop('state')\n",
    "        all_states.extend(states)\n",
    "\n",
    "        # Move to device\n",
    "        batch['left_image'] = batch['left_image'].to(DEVICE)\n",
    "        batch['right_image'] = batch['right_image'].to(DEVICE)\n",
    "        batch['tabular'] = torch.zeros(\n",
    "            batch['left_image'].size(0), tabular_dim, device=DEVICE)\n",
    "\n",
    "        # Ensemble predictions from all models with TTA\n",
    "        batch_preds_list = []\n",
    "        for model in student_models:\n",
    "            model_preds = predict_model_batch(model, batch, TTA_TYPES)  # [B,5]\n",
    "            batch_preds_list.append(model_preds.cpu())\n",
    "\n",
    "        # Average predictions across models\n",
    "        batch_preds_avg = torch.stack(\n",
    "            batch_preds_list, dim=0).mean(dim=0)  # [B,5]\n",
    "\n",
    "        # WA rule: if state == \"WA\" then dead_g = 0\n",
    "        batch_preds_np = batch_preds_avg.numpy()\n",
    "        for i, st in enumerate(states):\n",
    "            if st == 'WA':\n",
    "                # indices: [clover, dead, green, total, gdm]\n",
    "                batch_preds_np[i, 1] = 0.0\n",
    "                batch_preds_np[i, 3] = batch_preds_np[i, 0] + \\\n",
    "                    batch_preds_np[i, 2]\n",
    "\n",
    "        all_predictions.append(batch_preds_np)\n",
    "        all_image_ids.extend(batch['image_id'])\n",
    "\n",
    "# Concatenate all predictions\n",
    "all_predictions_array = np.concatenate(all_predictions, axis=0)\n",
    "print(f\"Predictions shape: {all_predictions_array.shape}\")\n",
    "print(f\"Image IDs count: {len(all_image_ids)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e73e7ece",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-11T19:12:46.753154Z",
     "iopub.status.busy": "2025-12-11T19:12:46.752693Z",
     "iopub.status.idle": "2025-12-11T19:12:46.760545Z",
     "shell.execute_reply": "2025-12-11T19:12:46.759842Z"
    },
    "papermill": {
     "duration": 0.01355,
     "end_time": "2025-12-11T19:12:46.761589",
     "exception": false,
     "start_time": "2025-12-11T19:12:46.748039",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Format submission CSV\n",
    "# Columns order: Dry_Clover_g, Dry_Dead_g, Dry_Green_g, Dry_Total_g, GDM_g\n",
    "target_names = ['Dry_Clover_g', 'Dry_Dead_g',\n",
    "                'Dry_Green_g', 'Dry_Total_g', 'GDM_g']\n",
    "\n",
    "submission_rows = []\n",
    "\n",
    "for img_idx, image_id in enumerate(all_image_ids):\n",
    "    predictions = all_predictions_array[img_idx]  # [5] values for 5 targets\n",
    "\n",
    "    for target_idx, target_name in enumerate(target_names):\n",
    "        sample_id = f\"{image_id}__{target_name}\"\n",
    "        target_value = float(predictions[target_idx])\n",
    "\n",
    "        submission_rows.append({\n",
    "            'sample_id': sample_id,\n",
    "            'target': target_value\n",
    "        })\n",
    "\n",
    "# Create submission dataframe\n",
    "submission_df = pd.DataFrame(submission_rows)\n",
    "\n",
    "print(f\"Submission shape: {submission_df.shape}\")\n",
    "print(f\"Expected shape: ({len(test_pivot) * 5}, 2)\")\n",
    "print(submission_df.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4dcbdf7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-11T19:12:46.770420Z",
     "iopub.status.busy": "2025-12-11T19:12:46.770195Z",
     "iopub.status.idle": "2025-12-11T19:12:46.777650Z",
     "shell.execute_reply": "2025-12-11T19:12:46.777008Z"
    },
    "papermill": {
     "duration": 0.01308,
     "end_time": "2025-12-11T19:12:46.778684",
     "exception": false,
     "start_time": "2025-12-11T19:12:46.765604",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Save submission\n",
    "submission_df.to_csv(SUBMISSION_NAME, index=False)\n",
    "\n",
    "print(f\"Submission saved to: {SUBMISSION_NAME}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb94176b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "databundleVersionId": 14254895,
     "sourceId": 112509,
     "sourceType": "competition"
    },
    {
     "databundleVersionId": 14898463,
     "datasetId": 8990306,
     "sourceId": 14114255,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31192,
   "isGpuEnabled": true,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "image2biomass",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 32.832499,
   "end_time": "2025-12-11T19:12:49.486768",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-12-11T19:12:16.654269",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
