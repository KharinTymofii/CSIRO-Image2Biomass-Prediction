{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9dafc480",
   "metadata": {},
   "source": [
    "# Distillation. Teacher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bba164c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: Currently logged in as: dmykhailov (dmykhailov-kyiv-school-of-economics) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin\n"
     ]
    }
   ],
   "source": [
    "machine = \"local\"\n",
    "!wandb login"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34bdd7d3",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff175635",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available variables: ['BASE_DIR', 'DATA_DIR', 'Path', 'directory', 'find_project_root', 'project_root', 'sys']\n",
      "PyTorch: 2.9.1+cu128\n",
      "Device: NVIDIA GeForce RTX 5050 Laptop GPU\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import gc\n",
    "import logging\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "from typing import cast\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "\n",
    "import cv2\n",
    "import timm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import pytorch_lightning as pl\n",
    "from tqdm import tqdm\n",
    "from torch.optim import AdamW\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "from transformers import Dinov2Model, Dinov2Config\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint, EarlyStopping, LearningRateMonitor\n",
    "from pytorch_lightning.loggers import WandbLogger\n",
    "\n",
    "import warnings\n",
    "from notebooks_config import setup_logging, CustomLogger\n",
    "\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "print(f\"PyTorch: {torch.__version__}\")\n",
    "print(f\"Device: {torch.cuda.get_device_name(0) if torch.cuda.is_available() else 'CPU'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ba1c03c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;2;105;254;105m\n",
      "[2025-12-13 15:11:33]\n",
      "SUCCESS: Logging configured successfully âœ…\u001b[0m\n",
      "\u001b[38;2;105;254;105m\n",
      "[2025-12-13 15:11:33]\n",
      "SUCCESS: Logging configuration test completed.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "logger = setup_logging(level=logging.DEBUG, full_color=True, include_function=False)\n",
    "logger = cast(CustomLogger, logger)  # Type hinting\n",
    "logger.success(\"Logging configuration test completed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "e452128a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 1488\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DESCRIPTION_FULL: facebook/dinov2-base-local_train[5]Folds_log_fusion-gating_epochs20_bs4_gradacc4_lr3e-05_wd0.05_dr0.2_hr0.5\n",
      "Effective batch size: 16\n"
     ]
    }
   ],
   "source": [
    "cpu_count = os.cpu_count()\n",
    "NUM_WORKERS = 0\n",
    "\n",
    "LR = 3e-5\n",
    "EPOCHS = 20\n",
    "N_FOLDS = 5\n",
    "GRAD_ACCUM = 4\n",
    "BATCH_SIZE = 4\n",
    "DROPOUT_RATE = 0.2\n",
    "WEIGHT_DECAY = 0.05\n",
    "HIDDEN_RATIO = 0.5\n",
    "TRAIN_SPLIT_RATIO = 0.02 # Used if N_FOLDS = 0\n",
    "\n",
    "MODEL = 'facebook/dinov2-base'  # 'swinv2_tiny_window8_256'\n",
    "PROJECT_NAME = \"csiro-image2biomass-prediction\"\n",
    "CHECKPOINTS_DIR = \"./kaggle/checkpoints/teacher/\"\n",
    "\n",
    "# Each patch is 1000x1000, resize to 768x768 for vision transformers\n",
    "SIZE = 768\n",
    "USE_LOG_TARGET = True     # Whether to use log1p transformation on target variable\n",
    "FUSION_METHOD = 'gating'  # ('concat', 'mean', 'max') OR 'gating'\n",
    "\n",
    "DESCRIPTION = machine + \\\n",
    "    (f\"_train{TRAIN_SPLIT_RATIO}\" if N_FOLDS == 0 else f\"_train[{N_FOLDS}]Folds\") + (\n",
    "        f\"_log\" if USE_LOG_TARGET else \"\") + f\"_fusion-{FUSION_METHOD}\"\n",
    "DESCRIPTION_FULL = MODEL + \"-\" + DESCRIPTION + \\\n",
    "    f\"_epochs{EPOCHS}_bs{BATCH_SIZE}_gradacc{GRAD_ACCUM}_lr{LR}_wd{WEIGHT_DECAY}_dr{DROPOUT_RATE}_hr{HIDDEN_RATIO}\"\n",
    "SUBMISSION_NAME = f\"{DESCRIPTION_FULL}_submission.csv\"\n",
    "SUBMISSION_ENSEMBLE_NAME = f\"{DESCRIPTION_FULL}_ensemble_submission.csv\"\n",
    "SUBMISSION_MSG = DESCRIPTION_FULL.replace(\"_\", \" \")\n",
    "\n",
    "SEED = 1488\n",
    "torch.manual_seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "pl.seed_everything(SEED)\n",
    "\n",
    "print(\"DESCRIPTION_FULL:\", DESCRIPTION_FULL)\n",
    "print(f\"Effective batch size: {BATCH_SIZE * GRAD_ACCUM}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "1804fc71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "NUM_WORKERS: 0\n",
      "\n",
      "NVIDIA GeForce RTX 5050 Laptop GPU\n",
      "Memory Usage:\n",
      "Allocated: 0.0 GB\n",
      "Cached:    0.0 GB\n"
     ]
    }
   ],
   "source": [
    "# setting device on GPU if available, else CPU\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Using device:', DEVICE)\n",
    "print('NUM_WORKERS:', NUM_WORKERS)\n",
    "print()\n",
    "\n",
    "# Additional Info when using cuda\n",
    "if DEVICE.type == 'cuda':\n",
    "    # clean GPU memory\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "\n",
    "    torch.set_float32_matmul_precision('high') if machine == \"local\" else None\n",
    "\n",
    "    print(torch.cuda.get_device_name(0))\n",
    "    print('Memory Usage:')\n",
    "    print('Allocated:', round(torch.cuda.memory_allocated(0)/1024**3, 1), 'GB')\n",
    "    print('Cached:   ', round(torch.cuda.memory_reserved(0)/1024**3, 1), 'GB')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b8b9504",
   "metadata": {},
   "source": [
    "## Data Loading and Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "e402a80c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset size: (1071, 9)\n"
     ]
    },
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "sample_id",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "image_path",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "Sampling_Date",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "State",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "Species",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "Pre_GSHH_NDVI",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Height_Ave_cm",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "target_name",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "target",
         "rawType": "float64",
         "type": "float"
        }
       ],
       "ref": "1bcd3a92-d153-4f79-b2e9-e0f6ec41c29d",
       "rows": [
        [
         "0",
         "ID1011485656__Dry_Clover_g",
         "train/ID1011485656.jpg",
         "2015/9/4",
         "Tas",
         "Ryegrass_Clover",
         "0.62",
         "4.6667",
         "Dry_Clover_g",
         "0.0"
        ],
        [
         "1",
         "ID1011485656__Dry_Dead_g",
         "train/ID1011485656.jpg",
         "2015/9/4",
         "Tas",
         "Ryegrass_Clover",
         "0.62",
         "4.6667",
         "Dry_Dead_g",
         "31.9984"
        ],
        [
         "2",
         "ID1011485656__Dry_Green_g",
         "train/ID1011485656.jpg",
         "2015/9/4",
         "Tas",
         "Ryegrass_Clover",
         "0.62",
         "4.6667",
         "Dry_Green_g",
         "16.275"
        ],
        [
         "5",
         "ID1012260530__Dry_Clover_g",
         "train/ID1012260530.jpg",
         "2015/4/1",
         "NSW",
         "Lucerne",
         "0.55",
         "16.0",
         "Dry_Clover_g",
         "0.0"
        ],
        [
         "6",
         "ID1012260530__Dry_Dead_g",
         "train/ID1012260530.jpg",
         "2015/4/1",
         "NSW",
         "Lucerne",
         "0.55",
         "16.0",
         "Dry_Dead_g",
         "0.0"
        ]
       ],
       "shape": {
        "columns": 9,
        "rows": 5
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sample_id</th>\n",
       "      <th>image_path</th>\n",
       "      <th>Sampling_Date</th>\n",
       "      <th>State</th>\n",
       "      <th>Species</th>\n",
       "      <th>Pre_GSHH_NDVI</th>\n",
       "      <th>Height_Ave_cm</th>\n",
       "      <th>target_name</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ID1011485656__Dry_Clover_g</td>\n",
       "      <td>train/ID1011485656.jpg</td>\n",
       "      <td>2015/9/4</td>\n",
       "      <td>Tas</td>\n",
       "      <td>Ryegrass_Clover</td>\n",
       "      <td>0.62</td>\n",
       "      <td>4.6667</td>\n",
       "      <td>Dry_Clover_g</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ID1011485656__Dry_Dead_g</td>\n",
       "      <td>train/ID1011485656.jpg</td>\n",
       "      <td>2015/9/4</td>\n",
       "      <td>Tas</td>\n",
       "      <td>Ryegrass_Clover</td>\n",
       "      <td>0.62</td>\n",
       "      <td>4.6667</td>\n",
       "      <td>Dry_Dead_g</td>\n",
       "      <td>31.9984</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ID1011485656__Dry_Green_g</td>\n",
       "      <td>train/ID1011485656.jpg</td>\n",
       "      <td>2015/9/4</td>\n",
       "      <td>Tas</td>\n",
       "      <td>Ryegrass_Clover</td>\n",
       "      <td>0.62</td>\n",
       "      <td>4.6667</td>\n",
       "      <td>Dry_Green_g</td>\n",
       "      <td>16.2750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>ID1012260530__Dry_Clover_g</td>\n",
       "      <td>train/ID1012260530.jpg</td>\n",
       "      <td>2015/4/1</td>\n",
       "      <td>NSW</td>\n",
       "      <td>Lucerne</td>\n",
       "      <td>0.55</td>\n",
       "      <td>16.0000</td>\n",
       "      <td>Dry_Clover_g</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>ID1012260530__Dry_Dead_g</td>\n",
       "      <td>train/ID1012260530.jpg</td>\n",
       "      <td>2015/4/1</td>\n",
       "      <td>NSW</td>\n",
       "      <td>Lucerne</td>\n",
       "      <td>0.55</td>\n",
       "      <td>16.0000</td>\n",
       "      <td>Dry_Dead_g</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    sample_id              image_path Sampling_Date State  \\\n",
       "0  ID1011485656__Dry_Clover_g  train/ID1011485656.jpg      2015/9/4   Tas   \n",
       "1    ID1011485656__Dry_Dead_g  train/ID1011485656.jpg      2015/9/4   Tas   \n",
       "2   ID1011485656__Dry_Green_g  train/ID1011485656.jpg      2015/9/4   Tas   \n",
       "5  ID1012260530__Dry_Clover_g  train/ID1012260530.jpg      2015/4/1   NSW   \n",
       "6    ID1012260530__Dry_Dead_g  train/ID1012260530.jpg      2015/4/1   NSW   \n",
       "\n",
       "           Species  Pre_GSHH_NDVI  Height_Ave_cm   target_name   target  \n",
       "0  Ryegrass_Clover           0.62         4.6667  Dry_Clover_g   0.0000  \n",
       "1  Ryegrass_Clover           0.62         4.6667    Dry_Dead_g  31.9984  \n",
       "2  Ryegrass_Clover           0.62         4.6667   Dry_Green_g  16.2750  \n",
       "5          Lucerne           0.55        16.0000  Dry_Clover_g   0.0000  \n",
       "6          Lucerne           0.55        16.0000    Dry_Dead_g   0.0000  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "PATH_DATA = './kaggle/input/csiro-biomass'\n",
    "PATH_TRAIN_CSV = os.path.join(PATH_DATA, 'train.csv')\n",
    "PATH_TEST_CSV = os.path.join(PATH_DATA, 'test.csv')\n",
    "PATH_TRAIN_IMG = os.path.join(PATH_DATA, 'train')\n",
    "PATH_TEST_IMG = os.path.join(PATH_DATA, 'test')\n",
    "\n",
    "df = pd.read_csv(PATH_TRAIN_CSV)\n",
    "# Remove unneeded targets\n",
    "df = df[~df['target_name'].isin(['Dry_Total_g', 'GDM_g'])]\n",
    "print(f\"Dataset size: {df.shape}\")\n",
    "display(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "e1285873",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(357, 9)\n",
      "               image_path Sampling_Date State            Species  \\\n",
      "0  train/ID1011485656.jpg      2015/9/4   Tas    Ryegrass_Clover   \n",
      "1  train/ID1012260530.jpg      2015/4/1   NSW            Lucerne   \n",
      "2  train/ID1025234388.jpg      2015/9/1    WA  SubcloverDalkeith   \n",
      "3  train/ID1028611175.jpg     2015/5/18   Tas           Ryegrass   \n",
      "4  train/ID1035947949.jpg     2015/9/11   Tas           Ryegrass   \n",
      "\n",
      "   Height_Ave_cm  Pre_GSHH_NDVI  Dry_Clover_g  Dry_Dead_g  Dry_Green_g  \n",
      "0         4.6667           0.62        0.0000     31.9984      16.2750  \n",
      "1        16.0000           0.55        0.0000      0.0000       7.6000  \n",
      "2         1.0000           0.38        6.0500      0.0000       0.0000  \n",
      "3         5.0000           0.66        0.0000     30.9703      24.2376  \n",
      "4         3.5000           0.54        0.4343     23.2239      10.5261  \n"
     ]
    }
   ],
   "source": [
    "# pivot the dataframe to have one row per image with multiple target columns\n",
    "tabular_df = df.pivot_table(index=['image_path', 'Sampling_Date', 'State', 'Species', 'Height_Ave_cm', 'Pre_GSHH_NDVI'],\n",
    "                            columns='target_name', values='target', aggfunc='first').reset_index()\n",
    "tabular_df.columns.name = None  # remove the aggregation name\n",
    "print(tabular_df.shape)\n",
    "print(tabular_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "0c96a1db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['image_path', 'Sampling_Date', 'State', 'Species', 'Height_Ave_cm', 'Pre_GSHH_NDVI', 'Dry_Clover_g', 'Dry_Dead_g', 'Dry_Green_g']\n"
     ]
    }
   ],
   "source": [
    "print(tabular_df.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "0220a26c",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_cols  = ['Dry_Clover_g', 'Dry_Dead_g', 'Dry_Green_g']\n",
    "num_features = ['Height_Ave_cm', 'Pre_GSHH_NDVI']\n",
    "cat_features = ['Species', 'State']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "b44b2e92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BUG: data leakage, will be fixed later in this code\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', StandardScaler(), num_features), # normalizing numeric features\n",
    "        ('cat', OneHotEncoder(sparse_output=False, handle_unknown='ignore'), cat_features) # OHE for categorical features\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "cdb7c40c",
   "metadata": {},
   "outputs": [],
   "source": [
    "tabular_data = preprocessor.fit_transform(tabular_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "6bd47682",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(357, 21)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[-0.28520388, -0.24631873,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  1.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  1.        ,  0.        ,\n",
       "         0.        ],\n",
       "       [ 0.81823967, -0.70706013,  0.        ,  0.        ,  0.        ,\n",
       "         1.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  1.        ,  0.        ,  0.        ,\n",
       "         0.        ],\n",
       "       [-0.64220462, -1.82600352,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  1.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         1.        ],\n",
       "       [-0.25275281,  0.01696207,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  1.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  1.        ,  0.        ,\n",
       "         0.        ],\n",
       "       [-0.39879723, -0.77288032,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  1.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  1.        ,  0.        ,\n",
       "         0.        ]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(tabular_data.shape)\n",
    "display(tabular_data[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7dbb541",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "413e7e05",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BiomassDataset(Dataset):\n",
    "    \"\"\"Dataset for biomass prediction with image + tabular features\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        df: pd.DataFrame,\n",
    "        tabular_features: np.ndarray,\n",
    "        target_cols: list[str],\n",
    "        img_dir: str,\n",
    "        transform: transforms.Compose | None = None,\n",
    "        is_test: bool = False,\n",
    "        use_log_target: bool = True\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            df: DataFrame with image_id, image_path, and target columns\n",
    "            tabular_features: Preprocessed tabular features (shape: [n_samples, n_features])\n",
    "            target_cols: List of target column names\n",
    "            img_dir: Root directory for images\n",
    "            transform: torchvision transform pipeline\n",
    "            is_test: If True, targets are not expected in df\n",
    "            use_log_target: If True, apply log1p transform to targets\n",
    "        \"\"\"\n",
    "        self.df = df.reset_index(drop=True)\n",
    "        self.tabular_features = tabular_features\n",
    "        self.target_cols = target_cols\n",
    "        self.img_dir = img_dir\n",
    "        self.transform = transform\n",
    "        self.is_test = is_test\n",
    "        self.use_log_target = use_log_target\n",
    "\n",
    "        assert len(self.df) == len(self.tabular_features), \\\n",
    "            f\"DataFrame length {len(self.df)} != tabular features length {len(self.tabular_features)}\"\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx: int) -> dict:\n",
    "        \"\"\"\n",
    "        Returns:\n",
    "            dict with keys:\n",
    "                - 'left_image': tensor [C, H, W]\n",
    "                - 'right_image': tensor [C, H, W]\n",
    "                - 'tabular': tensor [n_features]\n",
    "                - 'targets': tensor [n_targets] (if not test)\n",
    "                - 'image_id': str\n",
    "        \"\"\"\n",
    "        row = self.df.iloc[idx]\n",
    "\n",
    "        # Load image\n",
    "        img_path = os.path.join(self.img_dir, row['image_path'].replace('train/', '').replace('test/', ''))\n",
    "        image = cv2.imread(img_path)\n",
    "\n",
    "        if image is None:\n",
    "            raise FileNotFoundError(f\"Cannot load image: {img_path}\")\n",
    "\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        # Split into left and right patches\n",
    "        # Original image shape: [H, W, C] = [1000, 2000, 3]\n",
    "        h, w, c = image.shape\n",
    "        mid_w = w // 2\n",
    "\n",
    "        left_patch = image[:, :mid_w, :]   # [1000, 1000, 3]\n",
    "        right_patch = image[:, mid_w:, :]  # [1000, 1000, 3]\n",
    "\n",
    "        # Convert to PIL Image for torchvision transforms\n",
    "        left_pil = Image.fromarray(left_patch)\n",
    "        right_pil = Image.fromarray(right_patch)\n",
    "\n",
    "        # Apply transforms\n",
    "        if self.transform:\n",
    "            left_tensor = self.transform(left_pil)\n",
    "            right_tensor = self.transform(right_pil)\n",
    "        else:\n",
    "            left_tensor = transforms.ToTensor()(left_pil)\n",
    "            right_tensor = transforms.ToTensor()(right_pil)\n",
    "\n",
    "        # Get tabular features\n",
    "        tabular = torch.tensor(self.tabular_features[idx], dtype=torch.float32)\n",
    "\n",
    "        # Prepare output\n",
    "        output = {\n",
    "            'left_image': left_tensor,\n",
    "            'right_image': right_tensor,\n",
    "            'tabular': tabular,\n",
    "            'image_id': row['image_path'].split('/')[-1].replace('.jpg', '')\n",
    "        }\n",
    "\n",
    "        # Add targets if not test\n",
    "        if not self.is_test:\n",
    "            targets = row[self.target_cols].values.astype(np.float32)\n",
    "\n",
    "            # Apply log transform if enabled\n",
    "            if self.use_log_target:\n",
    "                # log1p handles zeros: log(1+0) = 0\n",
    "                targets = np.log1p(targets)\n",
    "\n",
    "            output['targets'] = torch.tensor(targets, dtype=torch.float32)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "14e0673c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_img_data_stat(df: pd.DataFrame):\n",
    "    \"\"\"Calculate mean and std of image data for normalization.\"\"\"\n",
    "    means = []\n",
    "    stds = []\n",
    "\n",
    "    loader = tqdm(df['image_path'], desc=\"Calculating image stats\")\n",
    "\n",
    "    for img_path in loader:\n",
    "        full_path = os.path.join(PATH_TRAIN_IMG, img_path.replace('train/', '').replace('test/', ''))\n",
    "        image = cv2.imread(full_path)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        image = image / 255.0  # Normalize to [0, 1]\n",
    "\n",
    "        means.append(np.mean(image, axis=(0, 1)))\n",
    "        stds.append(np.std(image, axis=(0, 1)))\n",
    "\n",
    "    mean = np.mean(means, axis=0)\n",
    "    std = np.mean(stds, axis=0)\n",
    "\n",
    "    return mean, std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "7b4abf49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_mean, train_std = calculate_img_data_stat(tabular_df)\n",
    "# print(f\"Train Image Mean: {train_mean}, Std: {train_std}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eed0d17b",
   "metadata": {},
   "source": [
    "Train Image Mean: [0.44173591 0.50362967 0.30579783]\n",
    "\n",
    "Std: [0.2364247  0.23557117 0.22199257]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "f7f4ee23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error loading model facebook/dinov2-base with timm: Unknown model (dinov2-base)\n",
      "Trying Hugging Face transformers...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Dinov2Config {\n",
       "  \"apply_layernorm\": true,\n",
       "  \"architectures\": [\n",
       "    \"Dinov2Model\"\n",
       "  ],\n",
       "  \"attention_probs_dropout_prob\": 0.0,\n",
       "  \"drop_path_rate\": 0.0,\n",
       "  \"dtype\": \"float32\",\n",
       "  \"hidden_act\": \"gelu\",\n",
       "  \"hidden_dropout_prob\": 0.0,\n",
       "  \"hidden_size\": 768,\n",
       "  \"image_size\": 518,\n",
       "  \"initializer_range\": 0.02,\n",
       "  \"layer_norm_eps\": 1e-06,\n",
       "  \"layerscale_value\": 1.0,\n",
       "  \"mlp_ratio\": 4,\n",
       "  \"model_type\": \"dinov2\",\n",
       "  \"num_attention_heads\": 12,\n",
       "  \"num_channels\": 3,\n",
       "  \"num_hidden_layers\": 12,\n",
       "  \"out_features\": [\n",
       "    \"stage12\"\n",
       "  ],\n",
       "  \"out_indices\": [\n",
       "    12\n",
       "  ],\n",
       "  \"patch_size\": 14,\n",
       "  \"qkv_bias\": true,\n",
       "  \"reshape_hidden_states\": true,\n",
       "  \"stage_names\": [\n",
       "    \"stem\",\n",
       "    \"stage1\",\n",
       "    \"stage2\",\n",
       "    \"stage3\",\n",
       "    \"stage4\",\n",
       "    \"stage5\",\n",
       "    \"stage6\",\n",
       "    \"stage7\",\n",
       "    \"stage8\",\n",
       "    \"stage9\",\n",
       "    \"stage10\",\n",
       "    \"stage11\",\n",
       "    \"stage12\"\n",
       "  ],\n",
       "  \"transformers_version\": \"4.57.3\",\n",
       "  \"use_mask_token\": true,\n",
       "  \"use_swiglu_ffn\": false\n",
       "}"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Image backbone (processes each patch independently)\n",
    "try:\n",
    "    temp_backbone = timm.create_model(\n",
    "        MODEL,\n",
    "        pretrained=True,\n",
    "        num_classes=0,  # remove classification head\n",
    "        global_pool='avg'\n",
    "    )\n",
    "    temp_backbone.to(DEVICE)\n",
    "    temp_backbone.eval()\n",
    "    config = temp_backbone.default_cfg\n",
    "\n",
    "except RuntimeError as e:\n",
    "    print(f\"Error loading model {MODEL} with timm: {e}\")\n",
    "    # Try loading from Hugging Face transformer\n",
    "    print(\"Trying Hugging Face transformers...\")\n",
    "    config = Dinov2Config.from_pretrained(MODEL)\n",
    "    temp_backbone = Dinov2Model.from_pretrained(MODEL, config=config)\n",
    "\n",
    "config\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "793c3419",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error accessing model config: 'Dinov2Config' object is not subscriptable\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    inputs_size = config['input_size']\n",
    "    inputs_size = int(inputs_size[1]) if inputs_size is not None and inputs_size[1] == inputs_size[2] else 256\n",
    "    mean = config['mean']\n",
    "    std = config['std']\n",
    "except TypeError as e:\n",
    "    print(f\"Error accessing model config: {e}\")\n",
    "    inputs_size = config.image_size\n",
    "    mean = [0.485, 0.456, 0.406]\n",
    "    std  = [0.229, 0.224, 0.225]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "aff82f7c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dinov2Model(\n",
       "  (embeddings): Dinov2Embeddings(\n",
       "    (patch_embeddings): Dinov2PatchEmbeddings(\n",
       "      (projection): Conv2d(3, 768, kernel_size=(14, 14), stride=(14, 14))\n",
       "    )\n",
       "    (dropout): Dropout(p=0.0, inplace=False)\n",
       "  )\n",
       "  (encoder): Dinov2Encoder(\n",
       "    (layer): ModuleList(\n",
       "      (0-11): 12 x Dinov2Layer(\n",
       "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (attention): Dinov2Attention(\n",
       "          (attention): Dinov2SelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (output): Dinov2SelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (layer_scale1): Dinov2LayerScale()\n",
       "        (drop_path): Identity()\n",
       "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): Dinov2MLP(\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (activation): GELUActivation()\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "        (layer_scale2): Dinov2LayerScale()\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (layernorm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       ")"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp_backbone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "d60de83f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Backbone expected input size: 518, using SIZE=518\n",
      "Backbone expected mean: [0.485, 0.456, 0.406], std: [0.229, 0.224, 0.225]\n",
      "AttributeError: 'BaseModelOutputWithPooling' object has no attribute 'shape'. Trying Hugging Face model forward pass.\n",
      "768\n"
     ]
    }
   ],
   "source": [
    "SIZE = inputs_size\n",
    "print(f\"Backbone expected input size: {inputs_size}, using SIZE={SIZE}\")\n",
    "print(f\"Backbone expected mean: {mean}, std: {std}\")\n",
    "\n",
    "# Get backbone output dimension\n",
    "with torch.no_grad():\n",
    "    try:\n",
    "        dummy = torch.randn(1, 3, SIZE, SIZE)\n",
    "        feat_dim = temp_backbone(dummy).shape[1]\n",
    "    except AttributeError as ae:\n",
    "        print(f\"AttributeError: {ae}. Trying Hugging Face model forward pass.\")\n",
    "        dummy = torch.randn(1, 3, SIZE, SIZE)\n",
    "        outputs = temp_backbone(dummy)\n",
    "        feat_dim = outputs.last_hidden_state.sum(dim=1).shape[1]  # Average pooling\n",
    "        print(feat_dim)\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error getting backbone feature dimension: {e}\")\n",
    "        raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "a19de392",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1370, 768])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs.last_hidden_state.shape  # [1, 1370, 768] where 1370 is number of patches -> (518 / 14) **2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "06c353b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transform = transforms.Compose([\n",
    "    transforms.Resize((SIZE, SIZE)),\n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "    transforms.RandomVerticalFlip(p=0.5),\n",
    "    transforms.RandomRotation(degrees=90),  # Increase to 90 for top-down view\n",
    "    transforms.RandomApply([\n",
    "        transforms.ColorJitter(brightness=0.3, contrast=0.3, saturation=0.3, hue=0.1)\n",
    "    ], p=0.5),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=mean, std=std),\n",
    "    transforms.RandomErasing(p=0.2, scale=(0.02, 0.1))  # Simulate occlusions\n",
    "])\n",
    "\n",
    "val_transform = transforms.Compose([\n",
    "    transforms.Resize((SIZE, SIZE)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=mean, std=std)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "cc5ef120",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Left image shape: torch.Size([3, 518, 518])\n",
      "Right image shape: torch.Size([3, 518, 518])\n",
      "Tabular shape: torch.Size([21])\n",
      "Targets shape: torch.Size([3])\n",
      "Image ID: ID1011485656\n",
      "Target values: tensor([0.0000, 3.4965, 2.8493])\n",
      "\n",
      "Original targets from df: [np.float64(0.0) np.float64(31.9984) np.float64(16.275)]\n",
      "Log-transformed targets: tensor([0.0000, 3.4965, 2.8493])\n",
      "Should be close to: [0.        3.496459  2.8492603]\n"
     ]
    }
   ],
   "source": [
    "# Create dataset instance\n",
    "train_dataset = BiomassDataset(\n",
    "    df=tabular_df,\n",
    "    tabular_features=tabular_data,\n",
    "    target_cols=target_cols,\n",
    "    img_dir=PATH_TRAIN_IMG,\n",
    "    transform=train_transform,\n",
    "    is_test=False\n",
    ")\n",
    "\n",
    "# Test it\n",
    "sample = train_dataset[0]\n",
    "print(f\"Left image shape: {sample['left_image'].shape}\")\n",
    "print(f\"Right image shape: {sample['right_image'].shape}\")\n",
    "print(f\"Tabular shape: {sample['tabular'].shape}\")\n",
    "print(f\"Targets shape: {sample['targets'].shape}\")\n",
    "print(f\"Image ID: {sample['image_id']}\")\n",
    "print(f\"Target values: {sample['targets']}\")\n",
    "print()\n",
    "\n",
    "# Test dataset with log transform\n",
    "print(f\"Original targets from df: {tabular_df.iloc[0][target_cols].values}\")\n",
    "print(f\"Log-transformed targets: {sample['targets']}\")\n",
    "print(f\"Should be close to: {np.log1p(tabular_df.iloc[0][target_cols].values.astype(np.float32))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42f322e7",
   "metadata": {},
   "source": [
    "## Spliting Data (StratifiedKFold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "29b90a93",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_season(date_str: str) -> str:\n",
    "    \"\"\"\n",
    "    Convert date string to season.\n",
    "\n",
    "    Args:\n",
    "        date_str: Date in format 'YYYY/M/D' or 'YYYY/MM/DD'\n",
    "\n",
    "    Returns:\n",
    "        Season name: 'Summer', 'Autumn', 'Winter', 'Spring'\n",
    "    \"\"\"\n",
    "    # Parse month from date string\n",
    "    month = int(date_str.split('/')[1])\n",
    "\n",
    "    # Australian seasons (Southern Hemisphere)\n",
    "    if month in [12, 1, 2]:\n",
    "        return 'Summer'\n",
    "    elif month in [3, 4, 5]:\n",
    "        return 'Autumn'\n",
    "    elif month in [6, 7, 8]:\n",
    "        return 'Winter'\n",
    "    else:  # 9, 10, 11\n",
    "        return 'Spring'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "d5e023e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique stratification groups:\n",
      "strat_group\n",
      "Spring_Tas_Ryegrass_Clover                                                41\n",
      "Winter_Vic_Phalaris_Clover                                                32\n",
      "Autumn_Tas_Ryegrass                                                       22\n",
      "Winter_Vic_Ryegrass_Clover                                                21\n",
      "Spring_Tas_Clover                                                         21\n",
      "Winter_WA_Clover                                                          20\n",
      "Winter_Tas_Ryegrass_Clover                                                18\n",
      "Autumn_NSW_Lucerne                                                        15\n",
      "Spring_Vic_Ryegrass_Clover                                                11\n",
      "Spring_Vic_Phalaris_BarleyGrass_SilverGrass_SpearGrass_Clover_Capeweed    11\n",
      "Spring_NSW_Fescue                                                         11\n",
      "Summer_NSW_Fescue_CrumbWeed                                               10\n",
      "Spring_Vic_Phalaris_Clover                                                10\n",
      "Winter_Tas_WhiteClover                                                    10\n",
      "Winter_Vic_Ryegrass                                                       10\n",
      "Winter_Tas_Ryegrass                                                       10\n",
      "Spring_Tas_Ryegrass                                                        9\n",
      "Summer_NSW_Fescue                                                          9\n",
      "Summer_NSW_Phalaris                                                        8\n",
      "Autumn_NSW_Fescue                                                          8\n",
      "Winter_Vic_Phalaris_Ryegrass_Clover                                        8\n",
      "Summer_NSW_Lucerne                                                         7\n",
      "Spring_Vic_Phalaris_Clover_Ryegrass_Barleygrass_Bromegrass                 7\n",
      "Autumn_Tas_Ryegrass_Clover                                                 7\n",
      "Summer_NSW_Ryegrass                                                        7\n",
      "Spring_WA_SubcloverLosa                                                    5\n",
      "Spring_WA_Ryegrass                                                         4\n",
      "Spring_WA_SubcloverDalkeith                                                3\n",
      "Winter_Vic_Mixed                                                           2\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Total groups: 29\n"
     ]
    }
   ],
   "source": [
    "# Add season column\n",
    "tabular_df['Season'] = tabular_df['Sampling_Date'].apply(get_season)\n",
    "\n",
    "# Create stratification column combining Season, State, and Species\n",
    "tabular_df['strat_group'] = (\n",
    "    tabular_df['Season'].astype(str) + '_' +\n",
    "    tabular_df['State'].astype(str) + '_' +\n",
    "    tabular_df['Species'].astype(str)\n",
    ")\n",
    "\n",
    "print(\"Unique stratification groups:\")\n",
    "print(tabular_df['strat_group'].value_counts())\n",
    "print(f\"\\nTotal groups: {tabular_df['strat_group'].nunique()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "7d5f0fd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Fold 1:\n",
      "  Train samples: 285\n",
      "  Val samples: 72\n",
      "\n",
      "Fold 2:\n",
      "  Train samples: 285\n",
      "  Val samples: 72\n",
      "\n",
      "Fold 3:\n",
      "  Train samples: 286\n",
      "  Val samples: 71\n",
      "\n",
      "Fold 4:\n",
      "  Train samples: 286\n",
      "  Val samples: 71\n",
      "\n",
      "Fold 5:\n",
      "  Train samples: 286\n",
      "  Val samples: 71\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\_GitHub\\CSIRO-Image2Biomass-Prediction\\.venv\\Lib\\site-packages\\sklearn\\model_selection\\_split.py:813: UserWarning: The least populated class in y has only 2 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Initialize StratifiedKFold\n",
    "n_folds = 5\n",
    "skf = StratifiedKFold(n_splits=n_folds, shuffle=True, random_state=42)\n",
    "\n",
    "# Get stratification labels\n",
    "strat_labels = tabular_df['strat_group'].values\n",
    "\n",
    "# Create fold assignments\n",
    "tabular_df['fold'] = -1\n",
    "\n",
    "for fold_idx, (train_idx, val_idx) in enumerate(skf.split(tabular_df, strat_labels)):\n",
    "    tabular_df.loc[val_idx, 'fold'] = fold_idx\n",
    "\n",
    "    print(f\"\\nFold {fold_idx + 1}:\")\n",
    "    print(f\"  Train samples: {len(train_idx)}\")\n",
    "    print(f\"  Val samples: {len(val_idx)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "20130d85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stratification verification:\n",
      "\n",
      "Fold 1:\n",
      "  Season distribution:\n",
      "Season\n",
      "Winter    0.361\n",
      "Spring    0.347\n",
      "Autumn    0.167\n",
      "Summer    0.125\n",
      "Name: proportion, dtype: float64\n",
      "  State distribution:\n",
      "State\n",
      "Tas    0.389\n",
      "Vic    0.319\n",
      "NSW    0.222\n",
      "WA     0.069\n",
      "Name: proportion, dtype: float64\n",
      "  Species distribution:\n",
      "Species\n",
      "Ryegrass_Clover                                                0.292\n",
      "Ryegrass                                                       0.167\n",
      "Phalaris_Clover                                                0.111\n",
      "Clover                                                         0.111\n",
      "Fescue                                                         0.083\n",
      "Lucerne                                                        0.056\n",
      "Fescue_CrumbWeed                                               0.028\n",
      "Phalaris                                                       0.028\n",
      "WhiteClover                                                    0.028\n",
      "Phalaris_Clover_Ryegrass_Barleygrass_Bromegrass                0.028\n",
      "Phalaris_BarleyGrass_SilverGrass_SpearGrass_Clover_Capeweed    0.028\n",
      "SubcloverLosa                                                  0.014\n",
      "Phalaris_Ryegrass_Clover                                       0.014\n",
      "Mixed                                                          0.014\n",
      "Name: proportion, dtype: float64\n",
      "\n",
      "Fold 2:\n",
      "  Season distribution:\n",
      "Season\n",
      "Spring    0.389\n",
      "Winter    0.375\n",
      "Autumn    0.125\n",
      "Summer    0.111\n",
      "Name: proportion, dtype: float64\n",
      "  State distribution:\n",
      "State\n",
      "Tas    0.375\n",
      "Vic    0.333\n",
      "NSW    0.194\n",
      "WA     0.097\n",
      "Name: proportion, dtype: float64\n",
      "  Species distribution:\n",
      "Species\n",
      "Ryegrass_Clover                                                0.278\n",
      "Ryegrass                                                       0.167\n",
      "Phalaris_Clover                                                0.125\n",
      "Clover                                                         0.111\n",
      "Fescue                                                         0.069\n",
      "Lucerne                                                        0.056\n",
      "Phalaris_BarleyGrass_SilverGrass_SpearGrass_Clover_Capeweed    0.042\n",
      "Fescue_CrumbWeed                                               0.028\n",
      "Phalaris                                                       0.028\n",
      "WhiteClover                                                    0.028\n",
      "SubcloverDalkeith                                              0.014\n",
      "Phalaris_Clover_Ryegrass_Barleygrass_Bromegrass                0.014\n",
      "Phalaris_Ryegrass_Clover                                       0.014\n",
      "Mixed                                                          0.014\n",
      "SubcloverLosa                                                  0.014\n",
      "Name: proportion, dtype: float64\n",
      "\n",
      "Fold 3:\n",
      "  Season distribution:\n",
      "Season\n",
      "Spring    0.380\n",
      "Winter    0.380\n",
      "Autumn    0.127\n",
      "Summer    0.113\n",
      "Name: proportion, dtype: float64\n",
      "  State distribution:\n",
      "State\n",
      "Tas    0.380\n",
      "Vic    0.310\n",
      "NSW    0.211\n",
      "WA     0.099\n",
      "Name: proportion, dtype: float64\n",
      "  Species distribution:\n",
      "Species\n",
      "Ryegrass_Clover                                                0.268\n",
      "Ryegrass                                                       0.169\n",
      "Phalaris_Clover                                                0.127\n",
      "Clover                                                         0.113\n",
      "Fescue                                                         0.085\n",
      "Lucerne                                                        0.056\n",
      "WhiteClover                                                    0.028\n",
      "Phalaris_BarleyGrass_SilverGrass_SpearGrass_Clover_Capeweed    0.028\n",
      "Phalaris                                                       0.028\n",
      "Fescue_CrumbWeed                                               0.028\n",
      "Phalaris_Ryegrass_Clover                                       0.028\n",
      "Phalaris_Clover_Ryegrass_Barleygrass_Bromegrass                0.014\n",
      "SubcloverDalkeith                                              0.014\n",
      "SubcloverLosa                                                  0.014\n",
      "Name: proportion, dtype: float64\n",
      "\n",
      "Fold 4:\n",
      "  Season distribution:\n",
      "Season\n",
      "Spring    0.380\n",
      "Winter    0.366\n",
      "Autumn    0.141\n",
      "Summer    0.113\n",
      "Name: proportion, dtype: float64\n",
      "  State distribution:\n",
      "State\n",
      "Tas    0.394\n",
      "Vic    0.296\n",
      "NSW    0.211\n",
      "WA     0.099\n",
      "Name: proportion, dtype: float64\n",
      "  Species distribution:\n",
      "Species\n",
      "Ryegrass_Clover                                                0.268\n",
      "Ryegrass                                                       0.169\n",
      "Clover                                                         0.127\n",
      "Phalaris_Clover                                                0.113\n",
      "Fescue                                                         0.085\n",
      "Lucerne                                                        0.070\n",
      "Phalaris_Ryegrass_Clover                                       0.028\n",
      "WhiteClover                                                    0.028\n",
      "Fescue_CrumbWeed                                               0.028\n",
      "Phalaris_BarleyGrass_SilverGrass_SpearGrass_Clover_Capeweed    0.028\n",
      "Phalaris                                                       0.014\n",
      "Phalaris_Clover_Ryegrass_Barleygrass_Bromegrass                0.014\n",
      "SubcloverLosa                                                  0.014\n",
      "SubcloverDalkeith                                              0.014\n",
      "Name: proportion, dtype: float64\n",
      "\n",
      "Fold 5:\n",
      "  Season distribution:\n",
      "Season\n",
      "Spring    0.366\n",
      "Winter    0.352\n",
      "Autumn    0.169\n",
      "Summer    0.113\n",
      "Name: proportion, dtype: float64\n",
      "  State distribution:\n",
      "State\n",
      "Tas    0.394\n",
      "Vic    0.310\n",
      "NSW    0.211\n",
      "WA     0.085\n",
      "Name: proportion, dtype: float64\n",
      "  Species distribution:\n",
      "Species\n",
      "Ryegrass_Clover                                                0.268\n",
      "Ryegrass                                                       0.197\n",
      "Clover                                                         0.113\n",
      "Phalaris_Clover                                                0.113\n",
      "Fescue                                                         0.070\n",
      "Lucerne                                                        0.070\n",
      "WhiteClover                                                    0.028\n",
      "Phalaris_Ryegrass_Clover                                       0.028\n",
      "Fescue_CrumbWeed                                               0.028\n",
      "Phalaris_Clover_Ryegrass_Barleygrass_Bromegrass                0.028\n",
      "Phalaris_BarleyGrass_SilverGrass_SpearGrass_Clover_Capeweed    0.028\n",
      "Phalaris                                                       0.014\n",
      "SubcloverLosa                                                  0.014\n",
      "Name: proportion, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Verify stratification worked\n",
    "print(\"Stratification verification:\")\n",
    "\n",
    "for fold in range(n_folds):\n",
    "    fold_df = tabular_df[tabular_df['fold'] == fold]\n",
    "    print(f\"\\nFold {fold + 1}:\")\n",
    "    print(f\"  Season distribution:\")\n",
    "    print(fold_df['Season'].value_counts(normalize=True).round(3))\n",
    "    print(f\"  State distribution:\")\n",
    "    print(fold_df['State'].value_counts(normalize=True).round(3))\n",
    "    print(f\"  Species distribution:\")\n",
    "    print(fold_df['Species'].value_counts(normalize=True).round(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "7cea12fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_fold_data(df: pd.DataFrame, fold: int):\n",
    "    \"\"\"\n",
    "    Get train/val split for specific fold.\n",
    "\n",
    "    Args:\n",
    "        df: DataFrame with 'fold' column\n",
    "        fold: Fold index to use as validation\n",
    "\n",
    "    Returns:\n",
    "        train_df, val_df\n",
    "    \"\"\"\n",
    "    train_df = df[df['fold'] != fold].reset_index(drop=True)\n",
    "    val_df = df[df['fold'] == fold].reset_index(drop=True)\n",
    "\n",
    "    return train_df, val_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "25e99df9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_loaders(fold: int, bs: int) -> tuple[DataLoader, DataLoader]:\n",
    "    \"\"\"Get dataloaders for specific fold with proper preprocessing.\"\"\"\n",
    "    train_df, val_df = get_fold_data(tabular_df, fold)\n",
    "\n",
    "    print(f\"Training fold {fold}:\")\n",
    "    print(f\"  Train size: {len(train_df)}\")\n",
    "    print(f\"  Val size: {len(val_df)}\")\n",
    "\n",
    "    # Create NEW preprocessor for each fold\n",
    "    fold_preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "            ('num', StandardScaler(), num_features),\n",
    "            ('cat', OneHotEncoder(sparse_output=False, handle_unknown='ignore'), cat_features)\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    # FIT only on train, TRANSFORM both\n",
    "    train_tabular = fold_preprocessor.fit_transform(train_df)\n",
    "    val_tabular = fold_preprocessor.transform(val_df)  # Only transform!\n",
    "\n",
    "    # Create datasets\n",
    "    train_dataset = BiomassDataset(\n",
    "        df=train_df,\n",
    "        tabular_features=train_tabular,\n",
    "        target_cols=target_cols,\n",
    "        img_dir=PATH_TRAIN_IMG,\n",
    "        transform=train_transform,\n",
    "        is_test=False,\n",
    "        use_log_target=USE_LOG_TARGET\n",
    "    )\n",
    "\n",
    "    val_dataset = BiomassDataset(\n",
    "        df=val_df,\n",
    "        tabular_features=val_tabular,\n",
    "        target_cols=target_cols,\n",
    "        img_dir=PATH_TRAIN_IMG,\n",
    "        transform=val_transform,\n",
    "        is_test=False,\n",
    "        use_log_target=USE_LOG_TARGET\n",
    "    )\n",
    "\n",
    "    # Create dataloaders with num_workers\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=bs,\n",
    "        shuffle=True,\n",
    "        num_workers=NUM_WORKERS,\n",
    "    )\n",
    "\n",
    "    val_loader = DataLoader(\n",
    "        val_dataset,\n",
    "        batch_size=bs * 2,  # Larger batch for validation\n",
    "        shuffle=False,\n",
    "        num_workers=NUM_WORKERS,\n",
    "    )\n",
    "\n",
    "    print(f\"Train batches: {len(train_loader)}\")\n",
    "    print(f\"Val batches: {len(val_loader)}\")\n",
    "    print(f\"Tabular features dimension: {train_tabular.shape[1]}\")\n",
    "\n",
    "    return train_loader, val_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "7f071c27",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_loaders_patches(fold: int, bs: int) -> tuple[DataLoader, DataLoader, int]:\n",
    "    \"\"\"Get dataloaders for specific fold with proper preprocessing.\"\"\"\n",
    "    train_df, val_df = get_fold_data(tabular_df, fold)\n",
    "\n",
    "    print(f\"Training fold {fold}:\")\n",
    "    print(f\"  Train size: {len(train_df)}\")\n",
    "    print(f\"  Val size: {len(val_df)}\")\n",
    "\n",
    "    # Create NEW preprocessor for each fold\n",
    "    fold_preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "            ('num', StandardScaler(), num_features),\n",
    "            ('cat', OneHotEncoder(sparse_output=False, handle_unknown='ignore'), cat_features)\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    # FIT only on train, TRANSFORM both\n",
    "    train_tabular = fold_preprocessor.fit_transform(train_df)\n",
    "    val_tabular = fold_preprocessor.transform(val_df)  # Only transform!\n",
    "\n",
    "    # Create datasets\n",
    "    train_dataset = BiomassDataset(\n",
    "        df=train_df,\n",
    "        tabular_features=train_tabular,\n",
    "        target_cols=target_cols,\n",
    "        img_dir=PATH_TRAIN_IMG,\n",
    "        transform=train_transform,\n",
    "        is_test=False,\n",
    "        use_log_target=USE_LOG_TARGET\n",
    "    )\n",
    "\n",
    "    val_dataset = BiomassDataset(\n",
    "        df=val_df,\n",
    "        tabular_features=val_tabular,\n",
    "        target_cols=target_cols,\n",
    "        img_dir=PATH_TRAIN_IMG,\n",
    "        transform=val_transform,\n",
    "        is_test=False,\n",
    "        use_log_target=USE_LOG_TARGET\n",
    "    )\n",
    "\n",
    "    # Create dataloaders with num_workers\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=bs,\n",
    "        shuffle=True,\n",
    "        num_workers=min(NUM_WORKERS, 8),  # Limit to avoid overhead\n",
    "        pin_memory=True if torch.cuda.is_available() else False,\n",
    "        persistent_workers=True if NUM_WORKERS > 0 else False\n",
    "    )\n",
    "\n",
    "    val_loader = DataLoader(\n",
    "        val_dataset,\n",
    "        batch_size=bs * 2,  # Larger batch for validation\n",
    "        shuffle=False,\n",
    "        num_workers=min(NUM_WORKERS, 8),\n",
    "        pin_memory=True if torch.cuda.is_available() else False,\n",
    "        persistent_workers=True if NUM_WORKERS > 0 else False\n",
    "    )\n",
    "\n",
    "    tabular_dim = train_tabular.shape[1]\n",
    "\n",
    "    print(f\"Train batches: {len(train_loader)}\")\n",
    "    print(f\"Val batches: {len(val_loader)}\")\n",
    "    print(f\"Tabular features dimension: {tabular_dim}\")\n",
    "\n",
    "    return train_loader, val_loader, tabular_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "6d896893",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training fold 0:\n",
      "  Train size: 285\n",
      "  Val size: 72\n",
      "Train batches: 72\n",
      "Val batches: 9\n",
      "Tabular features dimension: 21\n",
      "21\n",
      "Training fold 0:\n",
      "  Train size: 285\n",
      "  Val size: 72\n",
      "Train batches: 72\n",
      "Val batches: 9\n",
      "Tabular features dimension: 21\n",
      "21\n"
     ]
    }
   ],
   "source": [
    "train_loader, val_loader = get_loaders(fold=0, bs=BATCH_SIZE)\n",
    "print(train_loader.dataset.tabular_features.shape[1])\n",
    "train_loader, val_loader, tabular_dim = get_loaders_patches(fold=0, bs=BATCH_SIZE)\n",
    "print(tabular_dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53fd54d9",
   "metadata": {},
   "source": [
    "## Ligtning Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "0f7a48a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = [\n",
    "    \"Dry_Clover_g\",\n",
    "    \"Dry_Dead_g\",\n",
    "    \"Dry_Green_g\",\n",
    "    \"Dry_Total_g\",\n",
    "    \"GDM_g\"\n",
    "]\n",
    "\n",
    "weights = {\n",
    "    'Dry_Green_g': 0.1,\n",
    "    'Dry_Dead_g': 0.1,\n",
    "    'Dry_Clover_g': 0.1,\n",
    "    'GDM_g': 0.2,\n",
    "    'Dry_Total_g': 0.5,\n",
    "}\n",
    "\n",
    "\n",
    "def competition_metric(y_true, y_pred) -> float:\n",
    "    \"\"\"Function to calculate the competition's official evaluation metric (weighted R2 score).\"\"\"\n",
    "    weights_array = np.array([weights[l] for l in labels])\n",
    "\n",
    "    # Align with this calculation method\n",
    "    y_weighted_mean = np.average(y_true, weights=weights_array, axis=1).mean()\n",
    "\n",
    "    # For ss_res and ss_tot, also take the weighted average on axis=1, then the mean of the result\n",
    "    ss_res = np.average((y_true - y_pred)**2,\n",
    "                        weights=weights_array, axis=1).mean()\n",
    "    ss_tot = np.average((y_true - y_weighted_mean)**2,\n",
    "                        weights=weights_array, axis=1).mean()\n",
    "\n",
    "    return 1 - ss_res / ss_tot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "9e8b2e58",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BiomassTeacherModel(pl.LightningModule):\n",
    "    \"\"\"Teacher model for biomass prediction with dual-patch image + tabular features\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        backbone_name: str = 'facebook/dinov2-base',\n",
    "        tabular_dim: int = 10,\n",
    "        num_targets: int = 3,\n",
    "        lr: float = 1e-4,\n",
    "        weight_decay: float = 1e-5,\n",
    "        hidden_ratio: float = 0.5,\n",
    "        dropout: float = 0.2,\n",
    "        fusion_method: str = 'gating',\n",
    "        use_log_target: bool = True\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            backbone_name: timm model name\n",
    "            tabular_dim: dimension of tabular features\n",
    "            num_targets: number of regression targets\n",
    "            lr: learning rate\n",
    "            weight_decay: optimizer weight decay\n",
    "            hidden_ratio: ratio of hidden layer size relative to feature dim\n",
    "            dropout: dropout probability\n",
    "            fusion_method: how to combine left/right features ('concat', 'mean', 'max')\n",
    "            use_log_target: if True, model predicts log-transformed targets\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "\n",
    "        # Image backbone (processes each patch independently)\n",
    "        self.backbone = timm.create_model(\n",
    "            backbone_name,\n",
    "            pretrained=True,\n",
    "            num_classes=0,\n",
    "            global_pool='avg'\n",
    "        )\n",
    "\n",
    "        self.lr = lr\n",
    "        self.weight_decay = weight_decay\n",
    "\n",
    "        # Get backbone output dimension\n",
    "        with torch.no_grad():\n",
    "            dummy = torch.randn(1, 3, SIZE, SIZE)\n",
    "            feat_dim = self.backbone(dummy).shape[1]\n",
    "\n",
    "        self.feat_dim = feat_dim\n",
    "        self.fusion_method = fusion_method\n",
    "        self.use_log_target = use_log_target\n",
    "\n",
    "        # Combined dimension depends on fusion method\n",
    "        if self.fusion_method == 'concat':\n",
    "            self.combined_dim = feat_dim * 2 + tabular_dim\n",
    "        else:  # mean or max\n",
    "            self.combined_dim = feat_dim + tabular_dim\n",
    "\n",
    "        # Regression heads (NO activation at the end)\n",
    "        hidden_size = max(32, int(self.combined_dim * hidden_ratio))\n",
    "\n",
    "        def make_head():\n",
    "            return nn.Sequential(\n",
    "                nn.Linear(self.combined_dim, hidden_size),\n",
    "                nn.ReLU(inplace=True),\n",
    "                nn.Dropout(dropout),\n",
    "                nn.Linear(hidden_size, 1)\n",
    "                # NO Softplus here!\n",
    "            )\n",
    "\n",
    "        self.head_green = make_head()\n",
    "        self.head_clover = make_head()\n",
    "        self.head_dead = make_head()\n",
    "\n",
    "        # Storage for validation predictions and targets\n",
    "        self.validation_step_outputs = []\n",
    "\n",
    "        logger.info(f\"Model initialized: backbone={backbone_name}, feat_dim={feat_dim}, \"\n",
    "                    f\"combined_dim={self.combined_dim}, fusion={fusion_method}, \"\n",
    "                    f\"use_log_target={use_log_target}\")\n",
    "\n",
    "    def forward(self, batch: dict) -> tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            batch: dict with 'left_image', 'right_image', 'tabular'\n",
    "\n",
    "        Returns:\n",
    "            (green, clover, dead) predictions as separate tensors\n",
    "            Note: predictions are in log-space if use_log_target=True\n",
    "        \"\"\"\n",
    "        # Extract features from each patch\n",
    "        left_feat = self.backbone(batch['left_image'])   # [B, D]\n",
    "        right_feat = self.backbone(batch['right_image'])  # [B, D]\n",
    "\n",
    "        # Fuse image features based on method\n",
    "        if self.fusion_method == 'concat':\n",
    "            img_feat = torch.cat([left_feat, right_feat], dim=1)  # [B, 2*D]\n",
    "        elif self.fusion_method == 'mean':\n",
    "            img_feat = (left_feat + right_feat) / 2  # [B, D]\n",
    "        elif self.fusion_method == 'max':\n",
    "            img_feat = torch.maximum(left_feat, right_feat)  # [B, D]\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown fusion method: {self.fusion_method}\")\n",
    "\n",
    "        # Combine with tabular features\n",
    "        combined = torch.cat([img_feat, batch['tabular']], dim=1)  # [B, combined_dim]\n",
    "\n",
    "        # Predict each target (raw predictions, no activation)\n",
    "        green = self.head_green(combined).squeeze(1)\n",
    "        clover = self.head_clover(combined).squeeze(1)\n",
    "        dead = self.head_dead(combined).squeeze(1)\n",
    "\n",
    "        return green, clover, dead\n",
    "\n",
    "    def compute_all_targets(self, green: torch.Tensor, clover: torch.Tensor, dead: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Compute all 5 targets from 3 predicted ones using linear dependencies.\n",
    "\n",
    "        Args:\n",
    "            green: Dry_Green_g predictions [B]\n",
    "            clover: Dry_Clover_g predictions [B]\n",
    "            dead: Dry_Dead_g predictions [B]\n",
    "\n",
    "        Returns:\n",
    "            All 5 targets [B, 5] in order: [Clover, Dead, Green, Total, GDM]\n",
    "        \"\"\"\n",
    "        # Clamp to ensure non-negative after conversion from log space\n",
    "        green = torch.clamp(green, min=0.0)\n",
    "        clover = torch.clamp(clover, min=0.0)\n",
    "        dead = torch.clamp(dead, min=0.0)\n",
    "\n",
    "        # Calculate derived targets using linear dependencies\n",
    "        # Dry_Total_g = Dry_Green_g + Dry_Dead_g + Dry_Clover_g\n",
    "        total = green + dead + clover\n",
    "\n",
    "        # GDM_g = Dry_Clover_g + Dry_Green_g\n",
    "        gdm = clover + green\n",
    "\n",
    "        # Stack in the order expected by competition_metric:\n",
    "        # [\"Dry_Clover_g\", \"Dry_Dead_g\", \"Dry_Green_g\", \"Dry_Total_g\", \"GDM_g\"]\n",
    "        all_targets = torch.stack(\n",
    "            [clover, dead, green, total, gdm], dim=1)  # [B, 5]\n",
    "\n",
    "        return all_targets\n",
    "\n",
    "    def compute_loss(self, pred: tuple, target: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            pred: (green, clover, dead) predictions\n",
    "            target: [B, 3] ground truth targets [clover, dead, green]\n",
    "\n",
    "        Returns:\n",
    "            MSE loss\n",
    "        \"\"\"\n",
    "        green_pred, clover_pred, dead_pred = pred\n",
    "        clover_true = target[:, 0]  # Dry_Clover_g\n",
    "        dead_true = target[:, 1]    # Dry_Dead_g\n",
    "        green_true = target[:, 2]   # Dry_Green_g\n",
    "\n",
    "        loss_green = F.mse_loss(green_pred, green_true)\n",
    "        loss_clover = F.mse_loss(clover_pred, clover_true)\n",
    "        loss_dead = F.mse_loss(dead_pred, dead_true)\n",
    "\n",
    "        total_loss = loss_green + loss_clover + loss_dead\n",
    "\n",
    "        return total_loss\n",
    "\n",
    "    def training_step(self, batch: dict, batch_idx: int) -> torch.Tensor:\n",
    "        pred = self(batch)\n",
    "        loss = self.compute_loss(pred, batch['targets'])\n",
    "\n",
    "        self.log('train_loss', loss, on_step=True, on_epoch=True, prog_bar=True,\n",
    "                 batch_size=batch['targets'].size(0))\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch: dict, batch_idx: int) -> torch.Tensor:\n",
    "        green_pred, clover_pred, dead_pred = self(batch)\n",
    "\n",
    "        # Compute loss in log-space (if using log targets)\n",
    "        loss = self.compute_loss(\n",
    "            (green_pred, clover_pred, dead_pred), batch['targets'])\n",
    "\n",
    "        self.log('val_loss', loss, on_step=False, on_epoch=True, prog_bar=True,\n",
    "                 batch_size=batch['targets'].size(0))\n",
    "\n",
    "        # Convert predictions from log-space to original scale for metric calculation\n",
    "        if self.use_log_target:\n",
    "            green_pred = torch.expm1(green_pred)\n",
    "            clover_pred = torch.expm1(clover_pred)\n",
    "            dead_pred = torch.expm1(dead_pred)\n",
    "\n",
    "            # Convert targets from log-space to original scale\n",
    "            targets_original = torch.expm1(batch['targets'])\n",
    "        else:\n",
    "            targets_original = batch['targets']\n",
    "\n",
    "        # Compute all 5 targets from 3 predictions\n",
    "        preds_all = self.compute_all_targets(\n",
    "            green_pred, clover_pred, dead_pred)  # [B, 5]\n",
    "\n",
    "        # Compute all 5 targets from 3 ground truth values\n",
    "        clover_true = targets_original[:, 0]\n",
    "        dead_true = targets_original[:, 1]\n",
    "        green_true = targets_original[:, 2]\n",
    "        targets_all = self.compute_all_targets(\n",
    "            green_true, clover_true, dead_true)  # [B, 5]\n",
    "\n",
    "        # Store for epoch-end metric calculation\n",
    "        self.validation_step_outputs.append({\n",
    "            'preds': preds_all.detach().cpu(),\n",
    "            'targets': targets_all.detach().cpu()\n",
    "        })\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def on_validation_epoch_end(self):\n",
    "        \"\"\"Calculate competition metric at the end of validation epoch\"\"\"\n",
    "        if len(self.validation_step_outputs) == 0:\n",
    "            return\n",
    "\n",
    "        # Concatenate all predictions and targets\n",
    "        all_preds = torch.cat(\n",
    "            [x['preds'] for x in self.validation_step_outputs], dim=0).numpy()\n",
    "        all_targets = torch.cat(\n",
    "            [x['targets'] for x in self.validation_step_outputs], dim=0).numpy()\n",
    "\n",
    "        # Calculate competition metric\n",
    "        comp_metric = competition_metric(all_targets, all_preds)\n",
    "\n",
    "        # Log the metric\n",
    "        self.log('val_comp_metric', comp_metric, on_epoch=True, prog_bar=True)\n",
    "\n",
    "        # Clear storage\n",
    "        self.validation_step_outputs.clear()\n",
    "\n",
    "    def predict_step(self, batch: dict, batch_idx: int) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Prediction with proper post-processing\n",
    "\n",
    "        Returns:\n",
    "            Predictions in original scale (not log-transformed), clamped to [0, inf)\n",
    "        \"\"\"\n",
    "        green, clover, dead = self(batch)\n",
    "\n",
    "        # Stack predictions\n",
    "        preds = torch.stack([clover, dead, green], dim=1)  # [B, 3]\n",
    "\n",
    "        # If using log targets, convert back to original scale\n",
    "        if self.use_log_target:\n",
    "            # expm1(x) = exp(x) - 1, inverse of log1p\n",
    "            preds = torch.expm1(preds)\n",
    "\n",
    "        # Clamp to ensure non-negative values\n",
    "        preds = torch.clamp(preds, min=0.0)\n",
    "\n",
    "        return preds\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = AdamW(\n",
    "            self.parameters(),\n",
    "            lr=self.lr,\n",
    "            weight_decay=self.weight_decay\n",
    "        )\n",
    "\n",
    "        scheduler = CosineAnnealingLR(\n",
    "            optimizer,\n",
    "            T_max=self.trainer.max_epochs or 20,\n",
    "            eta_min=self.lr * 0.01\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            'optimizer': optimizer,\n",
    "            'lr_scheduler': {\n",
    "                'scheduler': scheduler,\n",
    "                'interval': 'epoch'  # FIXME: 'step' if needed\n",
    "            }\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "7c63a105",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BiomassTeacherModelPatches(pl.LightningModule):\n",
    "    \"\"\"\n",
    "    Teacher model with patch-level predictions using DINOv2.\n",
    "    - Extracts dense features from DINOv2 for each patch\n",
    "    - Applies shared MLP to each patch independently\n",
    "    - Averages predictions across all patches\n",
    "    - Also incorporates tabular features via gating mechanism\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        backbone_name: str = 'facebook/dinov2-base',\n",
    "        tabular_dim: int = 10,\n",
    "        num_targets: int = 3,\n",
    "        lr: float = 1e-4,\n",
    "        weight_decay: float = 1e-5,\n",
    "        hidden_ratio: float = 0.5,\n",
    "        dropout: float = 0.2,\n",
    "        fusion_method: str = 'gating',  # 'gating' or 'concat'\n",
    "        use_log_target: bool = True\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            backbone_name: DINOv2 model name\n",
    "            tabular_dim: dimension of tabular features\n",
    "            num_targets: number of regression targets (3)\n",
    "            lr: learning rate\n",
    "            weight_decay: weight decay for optimizer\n",
    "            hidden_ratio: ratio for hidden layer size\n",
    "            dropout: dropout probability\n",
    "            fusion_method: how to use tabular features ('gating' or 'concat')\n",
    "            use_log_target: if True, predict log1p transformed targets\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "\n",
    "        # Load DINOv2 backbone\n",
    "        self.backbone = Dinov2Model.from_pretrained(backbone_name)\n",
    "        self.backbone.train()\n",
    "\n",
    "        self.hidden_dim = self.backbone.config.hidden_size\n",
    "        self.patch_size = self.backbone.config.patch_size\n",
    "\n",
    "        self.lr = lr\n",
    "        self.weight_decay = weight_decay\n",
    "        self.fusion_method = fusion_method\n",
    "        self.use_log_target = use_log_target\n",
    "\n",
    "        # Patch-level MLPs (shared across all patches)\n",
    "        hidden_size = max(32, int(self.hidden_dim * hidden_ratio))\n",
    "\n",
    "        def make_patch_head():\n",
    "            \"\"\"MLP for patch-level prediction\"\"\"\n",
    "            return nn.Sequential(\n",
    "                nn.Linear(self.hidden_dim, hidden_size),\n",
    "                nn.LayerNorm(hidden_size),\n",
    "                nn.ReLU(inplace=True),\n",
    "                nn.Dropout(dropout),\n",
    "                nn.Linear(hidden_size, 1)\n",
    "            )\n",
    "\n",
    "        # Separate heads for each target\n",
    "        self.patch_head_green = make_patch_head()\n",
    "        self.patch_head_clover = make_patch_head()\n",
    "        self.patch_head_dead = make_patch_head()\n",
    "\n",
    "        # Tabular features used for modulation\n",
    "        if self.fusion_method == 'gating':\n",
    "            # Gating network - features control the strength of predictions\n",
    "            self.tabular_gate = nn.Sequential(\n",
    "                nn.Linear(tabular_dim, hidden_size),\n",
    "                nn.ReLU(inplace=True),\n",
    "                nn.Linear(hidden_size, 1),\n",
    "                nn.Sigmoid()  # Output [0, 1]\n",
    "            )\n",
    "        elif self.fusion_method == 'concat':\n",
    "            # Concatenate after patch-level averaging\n",
    "            self.fusion_layer = nn.Sequential(\n",
    "                nn.Linear(3 + tabular_dim, hidden_size),\n",
    "                nn.ReLU(inplace=True),\n",
    "                nn.Dropout(dropout),\n",
    "                nn.Linear(hidden_size, 3)\n",
    "            )\n",
    "\n",
    "        self.validation_step_outputs = []\n",
    "\n",
    "        msg = (\n",
    "            \"Patch-level Teacher Model initialized:\\n\"\n",
    "            f\"backbone={backbone_name}, hidden_dim={self.hidden_dim}, patch_size={self.patch_size},\\n\"\n",
    "            f\"fusion_method={fusion_method}, use_log_target={use_log_target}\"\n",
    "        )\n",
    "        logger.info(msg)\n",
    "\n",
    "    def forward(self, batch: dict) -> tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Forward pass with patch-level processing.\n",
    "\n",
    "        Args:\n",
    "            batch: dict with keys:\n",
    "                - 'left_image': [B, 3, H, W]\n",
    "                - 'right_image': [B, 3, H, W]\n",
    "                - 'tabular': [B, tabular_dim]\n",
    "\n",
    "        Returns:\n",
    "            (green, clover, dead) predictions [B]\n",
    "        \"\"\"\n",
    "        # Forward through DINOv2 for left patch\n",
    "        left_outputs = self.backbone(batch['left_image'])\n",
    "        left_features = left_outputs.last_hidden_state  # [B, num_patches+1, hidden_dim]\n",
    "\n",
    "        # Remove CLS token, keep only patch tokens\n",
    "        left_patches = left_features[:, 1:, :]  # [B, num_patches, hidden_dim]\n",
    "\n",
    "        # Forward through DINOv2 for right patch\n",
    "        right_outputs = self.backbone(batch['right_image'])\n",
    "        right_features = right_outputs.last_hidden_state\n",
    "        right_patches = right_features[:, 1:, :]\n",
    "\n",
    "        # Concatenate patches from both images\n",
    "        all_patches = torch.cat([left_patches, right_patches], dim=1)  # [B, 2*num_patches, hidden_dim]\n",
    "\n",
    "        # Apply MLP to each patch\n",
    "        patch_preds_green = self.patch_head_green(all_patches)  # [B, 2*num_patches, 1]\n",
    "        patch_preds_clover = self.patch_head_clover(all_patches)\n",
    "        patch_preds_dead = self.patch_head_dead(all_patches)\n",
    "\n",
    "        # Average predictions across patches\n",
    "        green = patch_preds_green.mean(dim=1).squeeze(1)  # [B]\n",
    "        clover = patch_preds_clover.mean(dim=1).squeeze(1)\n",
    "        dead = patch_preds_dead.mean(dim=1).squeeze(1)\n",
    "\n",
    "        # Use tabular features for modulation\n",
    "        if self.fusion_method == 'gating':\n",
    "            # Gating: modulate final prediction\n",
    "            gate = self.tabular_gate(batch['tabular'])  # [B, 1]\n",
    "            green = green * gate.squeeze(1)\n",
    "            clover = clover * gate.squeeze(1)\n",
    "            dead = dead * gate.squeeze(1)\n",
    "\n",
    "        elif self.fusion_method == 'concat':\n",
    "            # Concatenate: final layer to combine\n",
    "            combined = torch.cat([\n",
    "                green.unsqueeze(1),\n",
    "                clover.unsqueeze(1),\n",
    "                dead.unsqueeze(1),\n",
    "                batch['tabular']\n",
    "            ], dim=1)\n",
    "            output = self.fusion_layer(combined)  # [B, 3]\n",
    "            green = output[:, 0]\n",
    "            clover = output[:, 1]\n",
    "            dead = output[:, 2]\n",
    "\n",
    "        return green, clover, dead\n",
    "\n",
    "    def compute_all_targets(self, green: torch.Tensor, clover: torch.Tensor, dead: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Compute all 5 targets from 3 predicted ones using linear dependencies.\n",
    "\n",
    "        Args:\n",
    "            green: Dry_Green_g predictions [B]\n",
    "            clover: Dry_Clover_g predictions [B]\n",
    "            dead: Dry_Dead_g predictions [B]\n",
    "\n",
    "        Returns:\n",
    "            All 5 targets [B, 5] in order: [Clover, Dead, Green, Total, GDM]\n",
    "        \"\"\"\n",
    "        # Clamp to ensure non-negative after conversion from log space\n",
    "        green = torch.clamp(green, min=0.0)\n",
    "        clover = torch.clamp(clover, min=0.0)\n",
    "        dead = torch.clamp(dead, min=0.0)\n",
    "\n",
    "        # Calculate derived targets using linear dependencies\n",
    "        # Dry_Total_g = Dry_Green_g + Dry_Dead_g + Dry_Clover_g\n",
    "        total = green + dead + clover\n",
    "\n",
    "        # GDM_g = Dry_Clover_g + Dry_Green_g\n",
    "        gdm = clover + green\n",
    "\n",
    "        # Stack in the order expected by competition_metric:\n",
    "        # [\"Dry_Clover_g\", \"Dry_Dead_g\", \"Dry_Green_g\", \"Dry_Total_g\", \"GDM_g\"]\n",
    "        all_targets = torch.stack([clover, dead, green, total, gdm], dim=1)\n",
    "        \n",
    "        return all_targets\n",
    "\n",
    "    def compute_loss(self, pred: tuple, target: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            pred: (green, clover, dead) predictions\n",
    "            target: [B, 3] ground truth targets [clover, dead, green]\n",
    "\n",
    "        Returns:\n",
    "            MSE loss\n",
    "        \"\"\"\n",
    "        green_pred, clover_pred, dead_pred = pred\n",
    "        clover_true = target[:, 0]  # Dry_Clover_g\n",
    "        dead_true = target[:, 1]    # Dry_Dead_g\n",
    "        green_true = target[:, 2]   # Dry_Green_g\n",
    "\n",
    "        loss_green = F.mse_loss(green_pred, green_true)\n",
    "        loss_clover = F.mse_loss(clover_pred, clover_true)\n",
    "        loss_dead = F.mse_loss(dead_pred, dead_true)\n",
    "\n",
    "        total_loss = loss_green + loss_clover + loss_dead\n",
    "\n",
    "        return total_loss\n",
    "\n",
    "    def training_step(self, batch: dict, batch_idx: int) -> torch.Tensor:\n",
    "        pred = self(batch)\n",
    "        loss = self.compute_loss(pred, batch['targets'])\n",
    "        self.log('train_loss', loss, on_step=True, on_epoch=True, prog_bar=True,\n",
    "                batch_size=batch['targets'].size(0))\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch: dict, batch_idx: int) -> torch.Tensor:\n",
    "        green_pred, clover_pred, dead_pred = self(batch)\n",
    "        loss = self.compute_loss((green_pred, clover_pred, dead_pred), batch['targets'])\n",
    "\n",
    "        self.log('val_loss', loss, on_step=False, on_epoch=True, prog_bar=True,\n",
    "                 batch_size=batch['targets'].size(0))\n",
    "\n",
    "        if self.use_log_target:\n",
    "            green_pred = torch.expm1(green_pred)\n",
    "            clover_pred = torch.expm1(clover_pred)\n",
    "            dead_pred = torch.expm1(dead_pred)\n",
    "            targets_original = torch.expm1(batch['targets'])\n",
    "        else:\n",
    "            targets_original = batch['targets']\n",
    "\n",
    "        preds_all = self.compute_all_targets(green_pred, clover_pred, dead_pred)\n",
    "\n",
    "        clover_true = targets_original[:, 0]\n",
    "        dead_true = targets_original[:, 1]\n",
    "        green_true = targets_original[:, 2]\n",
    "        targets_all = self.compute_all_targets(green_true, clover_true, dead_true)\n",
    "\n",
    "        self.validation_step_outputs.append({\n",
    "            'preds': preds_all.detach().cpu(),\n",
    "            'targets': targets_all.detach().cpu()\n",
    "        })\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def on_validation_epoch_end(self):\n",
    "        if len(self.validation_step_outputs) == 0:\n",
    "            return\n",
    "\n",
    "        all_preds = torch.cat(\n",
    "            [x['preds'] for x in self.validation_step_outputs], dim=0).numpy()\n",
    "        all_targets = torch.cat(\n",
    "            [x['targets'] for x in self.validation_step_outputs], dim=0).numpy()\n",
    "\n",
    "        comp_metric = competition_metric(all_targets, all_preds)\n",
    "        self.log('val_comp_metric', comp_metric, on_epoch=True, prog_bar=True)\n",
    "        self.validation_step_outputs.clear()\n",
    "\n",
    "    def predict_step(self, batch: dict, batch_idx: int) -> torch.Tensor:\n",
    "        green, clover, dead = self(batch)\n",
    "        preds = torch.stack([clover, dead, green], dim=1)\n",
    "\n",
    "        if self.use_log_target:\n",
    "            preds = torch.expm1(preds)\n",
    "\n",
    "        preds = torch.clamp(preds, min=0.0)\n",
    "        return preds\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = AdamW(\n",
    "            self.parameters(),\n",
    "            lr=self.lr,\n",
    "            weight_decay=self.weight_decay\n",
    "        )\n",
    "        \n",
    "        scheduler = CosineAnnealingLR(\n",
    "            optimizer,\n",
    "            T_max=self.trainer.max_epochs or 20,\n",
    "            eta_min=self.lr * 0.01\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            'optimizer': optimizer,\n",
    "            'lr_scheduler': {\n",
    "                'scheduler': scheduler,\n",
    "                'interval': 'epoch'  # FIXME: 'step' if needed\n",
    "            }\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b85bdfb",
   "metadata": {},
   "source": [
    "## Folds Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "fd59f4b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training fold 0:\n",
      "  Train size: 285\n",
      "  Val size: 72\n",
      "Train batches: 72\n",
      "Val batches: 9\n",
      "Tabular features dimension: 21\n",
      "\u001b[38;2;161;247;255m\n",
      "[2025-12-13 15:29:12]\n",
      "INFO: Patch-level Teacher Model initialized:\n",
      "backbone=facebook/dinov2-base, hidden_dim=768, patch_size=14,\n",
      "fusion_method=gating, use_log_target=True\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using 16bit Automatic Mixed Precision (AMP)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.23.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>wandb\\run-20251213_152913-mp9b3ykb</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/dmykhailov-kyiv-school-of-economics/csiro-image2biomass-prediction/runs/mp9b3ykb' target=\"_blank\">facebook/dinov2-base-local_train[5]Folds_log_fusion-gating_epochs20_bs4_gradacc4_lr3e-05_wd0.05_dr0.2_hr0.5-fold0</a></strong> to <a href='https://wandb.ai/dmykhailov-kyiv-school-of-economics/csiro-image2biomass-prediction' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/dmykhailov-kyiv-school-of-economics/csiro-image2biomass-prediction' target=\"_blank\">https://wandb.ai/dmykhailov-kyiv-school-of-economics/csiro-image2biomass-prediction</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/dmykhailov-kyiv-school-of-economics/csiro-image2biomass-prediction/runs/mp9b3ykb' target=\"_blank\">https://wandb.ai/dmykhailov-kyiv-school-of-economics/csiro-image2biomass-prediction/runs/mp9b3ykb</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\_GitHub\\CSIRO-Image2Biomass-Prediction\\.venv\\Lib\\site-packages\\pytorch_lightning\\callbacks\\model_checkpoint.py:881: Checkpoint directory C:\\_GitHub\\CSIRO-Image2Biomass-Prediction\\notebooks\\kaggle\\checkpoints\\teacher\\fold0 exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "c:\\_GitHub\\CSIRO-Image2Biomass-Prediction\\.venv\\Lib\\site-packages\\pytorch_lightning\\utilities\\model_summary\\model_summary.py:242: Precision 16-mixed is not supported by the model summary.  Estimated model size in MB will not be accurate. Using 32 bits instead.\n",
      "\n",
      "  | Name              | Type        | Params | Mode  | FLOPs\n",
      "------------------------------------------------------------------\n",
      "0 | backbone          | Dinov2Model | 86.6 M | train | 0    \n",
      "1 | patch_head_green  | Sequential  | 296 K  | train | 0    \n",
      "2 | patch_head_clover | Sequential  | 296 K  | train | 0    \n",
      "3 | patch_head_dead   | Sequential  | 296 K  | train | 0    \n",
      "4 | tabular_gate      | Sequential  | 8.8 K  | train | 0    \n",
      "------------------------------------------------------------------\n",
      "87.5 M    Trainable params\n",
      "0         Non-trainable params\n",
      "87.5 M    Total params\n",
      "349.915   Total estimated model params size (MB)\n",
      "247       Modules in train mode\n",
      "0         Modules in eval mode\n",
      "0         Total Flops\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f2e3a798920d4ff698afdafe8d720318",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\_GitHub\\CSIRO-Image2Biomass-Prediction\\.venv\\Lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:434: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=19` in the `DataLoader` to improve performance.\n",
      "c:\\_GitHub\\CSIRO-Image2Biomass-Prediction\\.venv\\Lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:434: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=19` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eacf784e11db4cf28e41c1566db2020e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fbb6202c5c7746ee8b40db3dc710422d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_comp_metric improved. New best score: -0.089\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "efd83d8de25c4581a096be9722f8c58b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_comp_metric improved by 0.003 >= min_delta = 0.001. New best score: -0.086\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "264d791acac24722b89ce3d89c5d3aba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_comp_metric improved by 0.153 >= min_delta = 0.001. New best score: 0.067\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4bceda2534e9488c8f54e9838eeab08a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_comp_metric improved by 0.119 >= min_delta = 0.001. New best score: 0.185\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "56776894c1524ac3a2e0bf49b2bc341b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_comp_metric improved by 0.080 >= min_delta = 0.001. New best score: 0.265\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0f91b9fcdd97438485158dbe3fca56c5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "34442b195d2c4a909afad1bf8beb63c3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_comp_metric improved by 0.084 >= min_delta = 0.001. New best score: 0.349\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e4f9d8cd2e934b9e8b40450826dc3c71",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_comp_metric improved by 0.033 >= min_delta = 0.001. New best score: 0.382\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "06d2c0d3c01d418e904fe8b4934c06fd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_comp_metric improved by 0.016 >= min_delta = 0.001. New best score: 0.398\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4b995dcb1720449a9fbd4264e2cb8d4b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_comp_metric improved by 0.024 >= min_delta = 0.001. New best score: 0.421\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1590cee9e4a74858bca18db4eedab953",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_comp_metric improved by 0.064 >= min_delta = 0.001. New best score: 0.485\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c1a45dff9cce42aca45643b67772c239",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_comp_metric improved by 0.032 >= min_delta = 0.001. New best score: 0.517\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "79224e545fce41fb860f07283f300f8c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_comp_metric improved by 0.036 >= min_delta = 0.001. New best score: 0.553\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "35d776b92d334717b5e9a01f6912d7c2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "af1ef4c548e14b019b68f2e9ff770e20",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ce1f3cbecc4e4acdb4f2c21e56d11edf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f9e4b9e30963412a8d6fb7a3195c9407",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dc4cf57c629140d78425967aff7ddc54",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_comp_metric improved by 0.014 >= min_delta = 0.001. New best score: 0.567\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "38c80c9d045a40aa9fd2e42bd0f5adc1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "119bd5448ede4c679e75cd50fc631775",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=20` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;2;161;247;255m\n",
      "[2025-12-13 16:05:27]\n",
      "INFO: Loading best model from: C:\\_GitHub\\CSIRO-Image2Biomass-Prediction\\notebooks\\kaggle\\checkpoints\\teacher\\fold0\\facebook/dinov2-base-local_train[5]Folds_log_fusion-gating_epochs20_bs4_gradacc4_lr3e-05_wd0.05_dr0.2_hr0.5-fold0-epoch=17-val_loss=0.8943-val_comp_metric=0.5668.ckpt\u001b[0m\n",
      "\u001b[38;2;161;247;255m\n",
      "[2025-12-13 16:05:29]\n",
      "INFO: Patch-level Teacher Model initialized:\n",
      "backbone=facebook/dinov2-base, hidden_dim=768, patch_size=14,\n",
      "fusion_method=gating, use_log_target=True\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "c:\\_GitHub\\CSIRO-Image2Biomass-Prediction\\.venv\\Lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:434: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=19` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "788d22117e724e78919cecf3a63a7acf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>â–â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–ˆ</td></tr><tr><td>lr-AdamW</td><td>â–ˆâ–ˆâ–ˆâ–ˆâ–‡â–‡â–‡â–†â–†â–…â–„â–„â–ƒâ–ƒâ–‚â–‚â–‚â–â–â–</td></tr><tr><td>train_loss_epoch</td><td>â–ˆâ–…â–„â–ƒâ–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–</td></tr><tr><td>train_loss_step</td><td>â–„â–‚â–ˆâ–ƒâ–†â–‚â–„â–ƒâ–ƒâ–‚â–â–‚â–ƒâ–‚â–‚â–â–‚â–‚â–ƒâ–‚â–‚â–‚â–‚â–‚â–â–â–‚â–â–‚â–â–â–‚â–‚â–â–â–â–â–â–‚â–</td></tr><tr><td>trainer/global_step</td><td>â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–ˆ</td></tr><tr><td>val_comp_metric</td><td>â–â–â–ƒâ–„â–…â–„â–†â–†â–†â–†â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ</td></tr><tr><td>val_loss</td><td>â–ˆâ–…â–„â–„â–ƒâ–ƒâ–‚â–ƒâ–‚â–‚â–‚â–‚â–‚â–â–‚â–â–â–â–â–â–</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>20</td></tr><tr><td>lr-AdamW</td><td>0.0</td></tr><tr><td>train_loss_epoch</td><td>0.43166</td></tr><tr><td>train_loss_step</td><td>0.40986</td></tr><tr><td>trainer/global_step</td><td>360</td></tr><tr><td>val_comp_metric</td><td>0.56682</td></tr><tr><td>val_loss</td><td>0.8943</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">facebook/dinov2-base-local_train[5]Folds_log_fusion-gating_epochs20_bs4_gradacc4_lr3e-05_wd0.05_dr0.2_hr0.5-fold0</strong> at: <a href='https://wandb.ai/dmykhailov-kyiv-school-of-economics/csiro-image2biomass-prediction/runs/mp9b3ykb' target=\"_blank\">https://wandb.ai/dmykhailov-kyiv-school-of-economics/csiro-image2biomass-prediction/runs/mp9b3ykb</a><br> View project at: <a href='https://wandb.ai/dmykhailov-kyiv-school-of-economics/csiro-image2biomass-prediction' target=\"_blank\">https://wandb.ai/dmykhailov-kyiv-school-of-economics/csiro-image2biomass-prediction</a><br>Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>wandb\\run-20251213_152913-mp9b3ykb\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training fold 1:\n",
      "  Train size: 285\n",
      "  Val size: 72\n",
      "Train batches: 72\n",
      "Val batches: 9\n",
      "Tabular features dimension: 21\n",
      "\u001b[38;2;161;247;255m\n",
      "[2025-12-13 16:05:42]\n",
      "INFO: Patch-level Teacher Model initialized:\n",
      "backbone=facebook/dinov2-base, hidden_dim=768, patch_size=14,\n",
      "fusion_method=gating, use_log_target=True\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using 16bit Automatic Mixed Precision (AMP)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.23.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>wandb\\run-20251213_160543-6o7m593d</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/dmykhailov-kyiv-school-of-economics/csiro-image2biomass-prediction/runs/6o7m593d' target=\"_blank\">facebook/dinov2-base-local_train[5]Folds_log_fusion-gating_epochs20_bs4_gradacc4_lr3e-05_wd0.05_dr0.2_hr0.5-fold1</a></strong> to <a href='https://wandb.ai/dmykhailov-kyiv-school-of-economics/csiro-image2biomass-prediction' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/dmykhailov-kyiv-school-of-economics/csiro-image2biomass-prediction' target=\"_blank\">https://wandb.ai/dmykhailov-kyiv-school-of-economics/csiro-image2biomass-prediction</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/dmykhailov-kyiv-school-of-economics/csiro-image2biomass-prediction/runs/6o7m593d' target=\"_blank\">https://wandb.ai/dmykhailov-kyiv-school-of-economics/csiro-image2biomass-prediction/runs/6o7m593d</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\_GitHub\\CSIRO-Image2Biomass-Prediction\\.venv\\Lib\\site-packages\\pytorch_lightning\\callbacks\\model_checkpoint.py:881: Checkpoint directory C:\\_GitHub\\CSIRO-Image2Biomass-Prediction\\notebooks\\kaggle\\checkpoints\\teacher\\fold1 exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "c:\\_GitHub\\CSIRO-Image2Biomass-Prediction\\.venv\\Lib\\site-packages\\pytorch_lightning\\utilities\\model_summary\\model_summary.py:242: Precision 16-mixed is not supported by the model summary.  Estimated model size in MB will not be accurate. Using 32 bits instead.\n",
      "\n",
      "  | Name              | Type        | Params | Mode  | FLOPs\n",
      "------------------------------------------------------------------\n",
      "0 | backbone          | Dinov2Model | 86.6 M | train | 0    \n",
      "1 | patch_head_green  | Sequential  | 296 K  | train | 0    \n",
      "2 | patch_head_clover | Sequential  | 296 K  | train | 0    \n",
      "3 | patch_head_dead   | Sequential  | 296 K  | train | 0    \n",
      "4 | tabular_gate      | Sequential  | 8.8 K  | train | 0    \n",
      "------------------------------------------------------------------\n",
      "87.5 M    Trainable params\n",
      "0         Non-trainable params\n",
      "87.5 M    Total params\n",
      "349.915   Total estimated model params size (MB)\n",
      "247       Modules in train mode\n",
      "0         Modules in eval mode\n",
      "0         Total Flops\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "33b68b9f38f24551b76ec8020237ad05",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\_GitHub\\CSIRO-Image2Biomass-Prediction\\.venv\\Lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:434: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=19` in the `DataLoader` to improve performance.\n",
      "c:\\_GitHub\\CSIRO-Image2Biomass-Prediction\\.venv\\Lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:434: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=19` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "00f361b7067444b8a41bd88b67ebc7d7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bf21706cb2324be18e61a0392e81b4bc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_comp_metric improved. New best score: 0.018\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f48235785c2a47d1821841b5928b4d2d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_comp_metric improved by 0.055 >= min_delta = 0.001. New best score: 0.073\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b47f6e222b544cc9a2bb7e987f8d6d70",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_comp_metric improved by 0.007 >= min_delta = 0.001. New best score: 0.080\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "80f445bcd80841ac89648dbc70667e50",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_comp_metric improved by 0.112 >= min_delta = 0.001. New best score: 0.192\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ae2639159538431ca85ebd2015aedaaa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_comp_metric improved by 0.136 >= min_delta = 0.001. New best score: 0.328\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "530b5b0cfc00460781035ed1ec3c3f0a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "61344e257e944bd7b840cea04ec4c474",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_comp_metric improved by 0.056 >= min_delta = 0.001. New best score: 0.384\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b11b706a49884d648d868b34909b52bb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_comp_metric improved by 0.011 >= min_delta = 0.001. New best score: 0.395\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "324929d1ec21456e8843ab5afcc99531",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_comp_metric improved by 0.044 >= min_delta = 0.001. New best score: 0.438\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "928390e769e842e7bd37299f9f975b75",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_comp_metric improved by 0.024 >= min_delta = 0.001. New best score: 0.463\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4d2b2d9b77604029af43730c37aa727c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_comp_metric improved by 0.024 >= min_delta = 0.001. New best score: 0.487\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b6ad1c77e23e4812a5039131151554b7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_comp_metric improved by 0.031 >= min_delta = 0.001. New best score: 0.518\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fed76cefef7a4e0f9a1d940560c91c40",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_comp_metric improved by 0.038 >= min_delta = 0.001. New best score: 0.555\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "735cee02cebf48c3a0667cb040bc09b2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d2d2a7f4728f4786b34b178286db2978",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_comp_metric improved by 0.008 >= min_delta = 0.001. New best score: 0.563\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9addb60aade04911bb5d54b0028120d0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f3b2135f9c16460aa7371b50ca7882da",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_comp_metric improved by 0.015 >= min_delta = 0.001. New best score: 0.578\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cea369d6afb2443ca90e4e20ac347e1a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f2e2cf61fb7e4284a381e8bba1072407",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b9394eed565b4b7282145618b8f2fe89",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=20` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;2;161;247;255m\n",
      "[2025-12-13 16:42:15]\n",
      "INFO: Loading best model from: C:\\_GitHub\\CSIRO-Image2Biomass-Prediction\\notebooks\\kaggle\\checkpoints\\teacher\\fold1\\facebook/dinov2-base-local_train[5]Folds_log_fusion-gating_epochs20_bs4_gradacc4_lr3e-05_wd0.05_dr0.2_hr0.5-fold1-epoch=16-val_loss=0.8204-val_comp_metric=0.5781.ckpt\u001b[0m\n",
      "\u001b[38;2;161;247;255m\n",
      "[2025-12-13 16:42:19]\n",
      "INFO: Patch-level Teacher Model initialized:\n",
      "backbone=facebook/dinov2-base, hidden_dim=768, patch_size=14,\n",
      "fusion_method=gating, use_log_target=True\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "c:\\_GitHub\\CSIRO-Image2Biomass-Prediction\\.venv\\Lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:434: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=19` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "96e57824d3be4ad494686de3e0687353",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>â–â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–†â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ</td></tr><tr><td>lr-AdamW</td><td>â–ˆâ–ˆâ–ˆâ–ˆâ–‡â–‡â–‡â–†â–†â–…â–„â–„â–ƒâ–ƒâ–‚â–‚â–‚â–â–â–</td></tr><tr><td>train_loss_epoch</td><td>â–ˆâ–…â–„â–ƒâ–ƒâ–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–</td></tr><tr><td>train_loss_step</td><td>â–‚â–ˆâ–…â–ƒâ–ˆâ–ƒâ–ƒâ–ƒâ–ƒâ–‚â–†â–„â–‚â–ƒâ–â–‚â–‚â–â–â–‚â–‚â–â–â–â–‚â–‚â–â–â–‚â–â–â–â–‚â–â–â–â–â–â–â–</td></tr><tr><td>trainer/global_step</td><td>â–â–â–â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆ</td></tr><tr><td>val_comp_metric</td><td>â–â–‚â–‚â–ƒâ–…â–„â–†â–†â–†â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ</td></tr><tr><td>val_loss</td><td>â–ˆâ–†â–†â–„â–„â–ƒâ–ƒâ–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>20</td></tr><tr><td>lr-AdamW</td><td>0.0</td></tr><tr><td>train_loss_epoch</td><td>0.57852</td></tr><tr><td>train_loss_step</td><td>0.77574</td></tr><tr><td>trainer/global_step</td><td>360</td></tr><tr><td>val_comp_metric</td><td>0.57814</td></tr><tr><td>val_loss</td><td>0.82035</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">facebook/dinov2-base-local_train[5]Folds_log_fusion-gating_epochs20_bs4_gradacc4_lr3e-05_wd0.05_dr0.2_hr0.5-fold1</strong> at: <a href='https://wandb.ai/dmykhailov-kyiv-school-of-economics/csiro-image2biomass-prediction/runs/6o7m593d' target=\"_blank\">https://wandb.ai/dmykhailov-kyiv-school-of-economics/csiro-image2biomass-prediction/runs/6o7m593d</a><br> View project at: <a href='https://wandb.ai/dmykhailov-kyiv-school-of-economics/csiro-image2biomass-prediction' target=\"_blank\">https://wandb.ai/dmykhailov-kyiv-school-of-economics/csiro-image2biomass-prediction</a><br>Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>wandb\\run-20251213_160543-6o7m593d\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training fold 2:\n",
      "  Train size: 286\n",
      "  Val size: 71\n",
      "Train batches: 72\n",
      "Val batches: 9\n",
      "Tabular features dimension: 21\n",
      "\u001b[38;2;161;247;255m\n",
      "[2025-12-13 16:42:35]\n",
      "INFO: Patch-level Teacher Model initialized:\n",
      "backbone=facebook/dinov2-base, hidden_dim=768, patch_size=14,\n",
      "fusion_method=gating, use_log_target=True\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using 16bit Automatic Mixed Precision (AMP)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.23.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>wandb\\run-20251213_164236-ax60yeqe</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/dmykhailov-kyiv-school-of-economics/csiro-image2biomass-prediction/runs/ax60yeqe' target=\"_blank\">facebook/dinov2-base-local_train[5]Folds_log_fusion-gating_epochs20_bs4_gradacc4_lr3e-05_wd0.05_dr0.2_hr0.5-fold2</a></strong> to <a href='https://wandb.ai/dmykhailov-kyiv-school-of-economics/csiro-image2biomass-prediction' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/dmykhailov-kyiv-school-of-economics/csiro-image2biomass-prediction' target=\"_blank\">https://wandb.ai/dmykhailov-kyiv-school-of-economics/csiro-image2biomass-prediction</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/dmykhailov-kyiv-school-of-economics/csiro-image2biomass-prediction/runs/ax60yeqe' target=\"_blank\">https://wandb.ai/dmykhailov-kyiv-school-of-economics/csiro-image2biomass-prediction/runs/ax60yeqe</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\_GitHub\\CSIRO-Image2Biomass-Prediction\\.venv\\Lib\\site-packages\\pytorch_lightning\\callbacks\\model_checkpoint.py:881: Checkpoint directory C:\\_GitHub\\CSIRO-Image2Biomass-Prediction\\notebooks\\kaggle\\checkpoints\\teacher\\fold2 exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "c:\\_GitHub\\CSIRO-Image2Biomass-Prediction\\.venv\\Lib\\site-packages\\pytorch_lightning\\utilities\\model_summary\\model_summary.py:242: Precision 16-mixed is not supported by the model summary.  Estimated model size in MB will not be accurate. Using 32 bits instead.\n",
      "\n",
      "  | Name              | Type        | Params | Mode  | FLOPs\n",
      "------------------------------------------------------------------\n",
      "0 | backbone          | Dinov2Model | 86.6 M | train | 0    \n",
      "1 | patch_head_green  | Sequential  | 296 K  | train | 0    \n",
      "2 | patch_head_clover | Sequential  | 296 K  | train | 0    \n",
      "3 | patch_head_dead   | Sequential  | 296 K  | train | 0    \n",
      "4 | tabular_gate      | Sequential  | 8.8 K  | train | 0    \n",
      "------------------------------------------------------------------\n",
      "87.5 M    Trainable params\n",
      "0         Non-trainable params\n",
      "87.5 M    Total params\n",
      "349.915   Total estimated model params size (MB)\n",
      "247       Modules in train mode\n",
      "0         Modules in eval mode\n",
      "0         Total Flops\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3a0ca59f56554b17a659b7bb848813d5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\_GitHub\\CSIRO-Image2Biomass-Prediction\\.venv\\Lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:434: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=19` in the `DataLoader` to improve performance.\n",
      "c:\\_GitHub\\CSIRO-Image2Biomass-Prediction\\.venv\\Lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:434: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=19` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "93897a92a70b455eb81110dc7c64a205",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "461a540c4a6c410eb28ef7d324718ce1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_comp_metric improved. New best score: -0.112\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3b0654e90bb64a88839e396f11e8f2d3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_comp_metric improved by 0.133 >= min_delta = 0.001. New best score: 0.021\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dcbf23bf015b47f8a8a3800599596dda",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_comp_metric improved by 0.057 >= min_delta = 0.001. New best score: 0.078\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ea051d7f85b043cfafb2f7dfb55340f4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_comp_metric improved by 0.134 >= min_delta = 0.001. New best score: 0.212\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7d2c2edd988c4af9b1e45c5a7430d2ef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "71ef23a19f954f16af54fece98ceae69",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f33e3593efaf449abc23b4aa88b8aadc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_comp_metric improved by 0.132 >= min_delta = 0.001. New best score: 0.344\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "16a757732ebe42b79d60700e9a4d881c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5ee9212de60e483c82502decaa0b797f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_comp_metric improved by 0.024 >= min_delta = 0.001. New best score: 0.369\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "de82fd4413c14de0980c50f365f2d109",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_comp_metric improved by 0.053 >= min_delta = 0.001. New best score: 0.422\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c6ed08f7b8b241a1af79e3e350c853b3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_comp_metric improved by 0.017 >= min_delta = 0.001. New best score: 0.439\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "09a68d17272d49d59f40b195a1f18a7c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_comp_metric improved by 0.067 >= min_delta = 0.001. New best score: 0.506\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "469e5aaef8354c3aaef6c804a0c0540e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_comp_metric improved by 0.046 >= min_delta = 0.001. New best score: 0.552\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bda0b521f38a4727adf2812c2d9f8f82",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "394140da9fd64fa898146ba0b3537470",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d6e6607fd9e64d37997834455f3f3045",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c17767384d9b4b92b8ad723c8ecf363e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_comp_metric improved by 0.018 >= min_delta = 0.001. New best score: 0.570\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "95338c8016494ac1b04ec154d7c2af53",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e7475ffcf2bb4f7288c8e227069c20b2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_comp_metric improved by 0.003 >= min_delta = 0.001. New best score: 0.573\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6458ed1302b44c98948cbf7eefbb9e5e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_comp_metric improved by 0.002 >= min_delta = 0.001. New best score: 0.575\n",
      "`Trainer.fit` stopped: `max_epochs=20` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;2;161;247;255m\n",
      "[2025-12-13 17:22:04]\n",
      "INFO: Loading best model from: C:\\_GitHub\\CSIRO-Image2Biomass-Prediction\\notebooks\\kaggle\\checkpoints\\teacher\\fold2\\facebook/dinov2-base-local_train[5]Folds_log_fusion-gating_epochs20_bs4_gradacc4_lr3e-05_wd0.05_dr0.2_hr0.5-fold2-epoch=19-val_loss=0.8927-val_comp_metric=0.5751.ckpt\u001b[0m\n",
      "\u001b[38;2;161;247;255m\n",
      "[2025-12-13 17:22:08]\n",
      "INFO: Patch-level Teacher Model initialized:\n",
      "backbone=facebook/dinov2-base, hidden_dim=768, patch_size=14,\n",
      "fusion_method=gating, use_log_target=True\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "c:\\_GitHub\\CSIRO-Image2Biomass-Prediction\\.venv\\Lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:434: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=19` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ee795c78f9a344019710c7b2c5bb6abd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>â–â–â–â–â–â–â–â–â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ</td></tr><tr><td>lr-AdamW</td><td>â–ˆâ–ˆâ–ˆâ–ˆâ–‡â–‡â–‡â–†â–†â–…â–„â–„â–ƒâ–ƒâ–‚â–‚â–‚â–â–â–</td></tr><tr><td>train_loss_epoch</td><td>â–ˆâ–„â–„â–ƒâ–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–</td></tr><tr><td>train_loss_step</td><td>â–ˆâ–ˆâ–ƒâ–ƒâ–‚â–ƒâ–…â–ƒâ–ƒâ–‚â–ƒâ–ƒâ–ƒâ–‚â–‚â–ƒâ–‚â–â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–‚â–â–â–â–â–â–â–</td></tr><tr><td>trainer/global_step</td><td>â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ</td></tr><tr><td>val_comp_metric</td><td>â–â–‚â–ƒâ–„â–„â–„â–†â–…â–†â–†â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ</td></tr><tr><td>val_loss</td><td>â–ˆâ–…â–…â–„â–ƒâ–„â–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>20</td></tr><tr><td>lr-AdamW</td><td>0.0</td></tr><tr><td>train_loss_epoch</td><td>0.5011</td></tr><tr><td>train_loss_step</td><td>1.83838</td></tr><tr><td>trainer/global_step</td><td>360</td></tr><tr><td>val_comp_metric</td><td>0.57506</td></tr><tr><td>val_loss</td><td>0.89273</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">facebook/dinov2-base-local_train[5]Folds_log_fusion-gating_epochs20_bs4_gradacc4_lr3e-05_wd0.05_dr0.2_hr0.5-fold2</strong> at: <a href='https://wandb.ai/dmykhailov-kyiv-school-of-economics/csiro-image2biomass-prediction/runs/ax60yeqe' target=\"_blank\">https://wandb.ai/dmykhailov-kyiv-school-of-economics/csiro-image2biomass-prediction/runs/ax60yeqe</a><br> View project at: <a href='https://wandb.ai/dmykhailov-kyiv-school-of-economics/csiro-image2biomass-prediction' target=\"_blank\">https://wandb.ai/dmykhailov-kyiv-school-of-economics/csiro-image2biomass-prediction</a><br>Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>wandb\\run-20251213_164236-ax60yeqe\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training fold 3:\n",
      "  Train size: 286\n",
      "  Val size: 71\n",
      "Train batches: 72\n",
      "Val batches: 9\n",
      "Tabular features dimension: 21\n",
      "\u001b[38;2;161;247;255m\n",
      "[2025-12-13 17:22:25]\n",
      "INFO: Patch-level Teacher Model initialized:\n",
      "backbone=facebook/dinov2-base, hidden_dim=768, patch_size=14,\n",
      "fusion_method=gating, use_log_target=True\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using 16bit Automatic Mixed Precision (AMP)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.23.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>wandb\\run-20251213_172225-3lpmbxva</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/dmykhailov-kyiv-school-of-economics/csiro-image2biomass-prediction/runs/3lpmbxva' target=\"_blank\">facebook/dinov2-base-local_train[5]Folds_log_fusion-gating_epochs20_bs4_gradacc4_lr3e-05_wd0.05_dr0.2_hr0.5-fold3</a></strong> to <a href='https://wandb.ai/dmykhailov-kyiv-school-of-economics/csiro-image2biomass-prediction' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/dmykhailov-kyiv-school-of-economics/csiro-image2biomass-prediction' target=\"_blank\">https://wandb.ai/dmykhailov-kyiv-school-of-economics/csiro-image2biomass-prediction</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/dmykhailov-kyiv-school-of-economics/csiro-image2biomass-prediction/runs/3lpmbxva' target=\"_blank\">https://wandb.ai/dmykhailov-kyiv-school-of-economics/csiro-image2biomass-prediction/runs/3lpmbxva</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\_GitHub\\CSIRO-Image2Biomass-Prediction\\.venv\\Lib\\site-packages\\pytorch_lightning\\callbacks\\model_checkpoint.py:881: Checkpoint directory C:\\_GitHub\\CSIRO-Image2Biomass-Prediction\\notebooks\\kaggle\\checkpoints\\teacher\\fold3 exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "c:\\_GitHub\\CSIRO-Image2Biomass-Prediction\\.venv\\Lib\\site-packages\\pytorch_lightning\\utilities\\model_summary\\model_summary.py:242: Precision 16-mixed is not supported by the model summary.  Estimated model size in MB will not be accurate. Using 32 bits instead.\n",
      "\n",
      "  | Name              | Type        | Params | Mode  | FLOPs\n",
      "------------------------------------------------------------------\n",
      "0 | backbone          | Dinov2Model | 86.6 M | train | 0    \n",
      "1 | patch_head_green  | Sequential  | 296 K  | train | 0    \n",
      "2 | patch_head_clover | Sequential  | 296 K  | train | 0    \n",
      "3 | patch_head_dead   | Sequential  | 296 K  | train | 0    \n",
      "4 | tabular_gate      | Sequential  | 8.8 K  | train | 0    \n",
      "------------------------------------------------------------------\n",
      "87.5 M    Trainable params\n",
      "0         Non-trainable params\n",
      "87.5 M    Total params\n",
      "349.915   Total estimated model params size (MB)\n",
      "247       Modules in train mode\n",
      "0         Modules in eval mode\n",
      "0         Total Flops\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c1ab696d95704a9a9ceb42a7e18a82b1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\_GitHub\\CSIRO-Image2Biomass-Prediction\\.venv\\Lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:434: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=19` in the `DataLoader` to improve performance.\n",
      "c:\\_GitHub\\CSIRO-Image2Biomass-Prediction\\.venv\\Lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:434: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=19` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1e79396ef16744b894560cec3dc0dcb5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1e8e7582247b4826967fe16c24f5f085",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_comp_metric improved. New best score: -0.118\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2570f765ffd9463893d320e2afa99b58",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_comp_metric improved by 0.077 >= min_delta = 0.001. New best score: -0.040\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6bfb61dc56974792acf0ce9e0adf7de2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_comp_metric improved by 0.034 >= min_delta = 0.001. New best score: -0.006\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "be2cd2479d01409f91d6472e38a688f7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_comp_metric improved by 0.054 >= min_delta = 0.001. New best score: 0.048\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "88b9faba937e418f8972bbfa6c5dd9a9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_comp_metric improved by 0.168 >= min_delta = 0.001. New best score: 0.216\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1bbb4405c66b49eda29da0b5ff819323",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8d35b8da1a354d149edbe9237cc83ea8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_comp_metric improved by 0.026 >= min_delta = 0.001. New best score: 0.243\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7d01d0289d274812a432c7ac14982f50",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_comp_metric improved by 0.056 >= min_delta = 0.001. New best score: 0.299\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0b1dfac7a60d4c8fbcf7c7ec6d48bf13",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_comp_metric improved by 0.057 >= min_delta = 0.001. New best score: 0.356\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "da84150ea1c24a139d3c0f24056243ff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_comp_metric improved by 0.046 >= min_delta = 0.001. New best score: 0.402\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5022f04a5ebb4208ad06b2c36de4cb19",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_comp_metric improved by 0.003 >= min_delta = 0.001. New best score: 0.405\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "75e0d0fe44d44c73a2140fd9c891801a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_comp_metric improved by 0.085 >= min_delta = 0.001. New best score: 0.490\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9b44f0d79fca412e8c44bab4792e2146",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_comp_metric improved by 0.004 >= min_delta = 0.001. New best score: 0.493\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b10928b98fab4cf9882ef19699cf83c6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_comp_metric improved by 0.030 >= min_delta = 0.001. New best score: 0.523\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7f2d2c0f73cd401580f6ee1b9247063d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_comp_metric improved by 0.002 >= min_delta = 0.001. New best score: 0.525\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "20576982542247c895bef5522ded1f8b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a4fe5beb739d46838b4139fe0445d0d2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_comp_metric improved by 0.003 >= min_delta = 0.001. New best score: 0.528\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f5f2dd66954b4bf6a4b8074b3710be29",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_comp_metric improved by 0.015 >= min_delta = 0.001. New best score: 0.543\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d22857da1d4043609c587bf8f4e4077f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c74b3eea314648d181a189d347553479",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=20` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;2;161;247;255m\n",
      "[2025-12-13 18:02:00]\n",
      "INFO: Loading best model from: C:\\_GitHub\\CSIRO-Image2Biomass-Prediction\\notebooks\\kaggle\\checkpoints\\teacher\\fold3\\facebook/dinov2-base-local_train[5]Folds_log_fusion-gating_epochs20_bs4_gradacc4_lr3e-05_wd0.05_dr0.2_hr0.5-fold3-epoch=17-val_loss=1.3658-val_comp_metric=0.5430.ckpt\u001b[0m\n",
      "\u001b[38;2;161;247;255m\n",
      "[2025-12-13 18:02:03]\n",
      "INFO: Patch-level Teacher Model initialized:\n",
      "backbone=facebook/dinov2-base, hidden_dim=768, patch_size=14,\n",
      "fusion_method=gating, use_log_target=True\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "c:\\_GitHub\\CSIRO-Image2Biomass-Prediction\\.venv\\Lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:434: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=19` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "128bf45af25b432184d33b7c212dd854",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>â–â–â–â–â–â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–…â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ</td></tr><tr><td>lr-AdamW</td><td>â–ˆâ–ˆâ–ˆâ–ˆâ–‡â–‡â–‡â–†â–†â–…â–„â–„â–ƒâ–ƒâ–‚â–‚â–‚â–â–â–</td></tr><tr><td>train_loss_epoch</td><td>â–ˆâ–…â–ƒâ–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–</td></tr><tr><td>train_loss_step</td><td>â–ˆâ–…â–ƒâ–ƒâ–…â–‚â–ƒâ–‚â–‚â–‚â–„â–‚â–‚â–‚â–‚â–‚â–â–‚â–ƒâ–‚â–‚â–â–â–â–â–â–‚â–â–â–â–â–‚â–â–â–â–â–â–â–â–</td></tr><tr><td>trainer/global_step</td><td>â–â–â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–„â–…â–…â–…â–…â–†â–†â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ</td></tr><tr><td>val_comp_metric</td><td>â–â–‚â–‚â–ƒâ–…â–„â–…â–…â–†â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ</td></tr><tr><td>val_loss</td><td>â–ˆâ–†â–ˆâ–„â–ƒâ–ƒâ–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>20</td></tr><tr><td>lr-AdamW</td><td>0.0</td></tr><tr><td>train_loss_epoch</td><td>0.43195</td></tr><tr><td>train_loss_step</td><td>0.75877</td></tr><tr><td>trainer/global_step</td><td>360</td></tr><tr><td>val_comp_metric</td><td>0.543</td></tr><tr><td>val_loss</td><td>1.36578</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">facebook/dinov2-base-local_train[5]Folds_log_fusion-gating_epochs20_bs4_gradacc4_lr3e-05_wd0.05_dr0.2_hr0.5-fold3</strong> at: <a href='https://wandb.ai/dmykhailov-kyiv-school-of-economics/csiro-image2biomass-prediction/runs/3lpmbxva' target=\"_blank\">https://wandb.ai/dmykhailov-kyiv-school-of-economics/csiro-image2biomass-prediction/runs/3lpmbxva</a><br> View project at: <a href='https://wandb.ai/dmykhailov-kyiv-school-of-economics/csiro-image2biomass-prediction' target=\"_blank\">https://wandb.ai/dmykhailov-kyiv-school-of-economics/csiro-image2biomass-prediction</a><br>Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>wandb\\run-20251213_172225-3lpmbxva\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training fold 4:\n",
      "  Train size: 286\n",
      "  Val size: 71\n",
      "Train batches: 72\n",
      "Val batches: 9\n",
      "Tabular features dimension: 21\n",
      "\u001b[38;2;161;247;255m\n",
      "[2025-12-13 18:02:20]\n",
      "INFO: Patch-level Teacher Model initialized:\n",
      "backbone=facebook/dinov2-base, hidden_dim=768, patch_size=14,\n",
      "fusion_method=gating, use_log_target=True\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using 16bit Automatic Mixed Precision (AMP)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.23.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>wandb\\run-20251213_180220-ytym35z5</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/dmykhailov-kyiv-school-of-economics/csiro-image2biomass-prediction/runs/ytym35z5' target=\"_blank\">facebook/dinov2-base-local_train[5]Folds_log_fusion-gating_epochs20_bs4_gradacc4_lr3e-05_wd0.05_dr0.2_hr0.5-fold4</a></strong> to <a href='https://wandb.ai/dmykhailov-kyiv-school-of-economics/csiro-image2biomass-prediction' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/dmykhailov-kyiv-school-of-economics/csiro-image2biomass-prediction' target=\"_blank\">https://wandb.ai/dmykhailov-kyiv-school-of-economics/csiro-image2biomass-prediction</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/dmykhailov-kyiv-school-of-economics/csiro-image2biomass-prediction/runs/ytym35z5' target=\"_blank\">https://wandb.ai/dmykhailov-kyiv-school-of-economics/csiro-image2biomass-prediction/runs/ytym35z5</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\_GitHub\\CSIRO-Image2Biomass-Prediction\\.venv\\Lib\\site-packages\\pytorch_lightning\\callbacks\\model_checkpoint.py:881: Checkpoint directory C:\\_GitHub\\CSIRO-Image2Biomass-Prediction\\notebooks\\kaggle\\checkpoints\\teacher\\fold4 exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "c:\\_GitHub\\CSIRO-Image2Biomass-Prediction\\.venv\\Lib\\site-packages\\pytorch_lightning\\utilities\\model_summary\\model_summary.py:242: Precision 16-mixed is not supported by the model summary.  Estimated model size in MB will not be accurate. Using 32 bits instead.\n",
      "\n",
      "  | Name              | Type        | Params | Mode  | FLOPs\n",
      "------------------------------------------------------------------\n",
      "0 | backbone          | Dinov2Model | 86.6 M | train | 0    \n",
      "1 | patch_head_green  | Sequential  | 296 K  | train | 0    \n",
      "2 | patch_head_clover | Sequential  | 296 K  | train | 0    \n",
      "3 | patch_head_dead   | Sequential  | 296 K  | train | 0    \n",
      "4 | tabular_gate      | Sequential  | 8.8 K  | train | 0    \n",
      "------------------------------------------------------------------\n",
      "87.5 M    Trainable params\n",
      "0         Non-trainable params\n",
      "87.5 M    Total params\n",
      "349.915   Total estimated model params size (MB)\n",
      "247       Modules in train mode\n",
      "0         Modules in eval mode\n",
      "0         Total Flops\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d510f10aac5c487d8382cdcd2ca30a67",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\_GitHub\\CSIRO-Image2Biomass-Prediction\\.venv\\Lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:434: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=19` in the `DataLoader` to improve performance.\n",
      "c:\\_GitHub\\CSIRO-Image2Biomass-Prediction\\.venv\\Lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:434: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=19` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "423173a707054420be58c684d3b6b946",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d16969f98bae4cdfaa8d64753a8df4c5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_comp_metric improved. New best score: -0.333\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9a881cab52154f9f967486b4f9b2984e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_comp_metric improved by 0.267 >= min_delta = 0.001. New best score: -0.066\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4797a5adcb2e4cfba2a02ccdca47a693",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "15fb9f2fb48a4889a2fcd2569658033a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_comp_metric improved by 0.142 >= min_delta = 0.001. New best score: 0.076\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d9e2e37c734048668cf84c6d77a71b57",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0fc263b6c1e349ebb2ae683a4011d957",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_comp_metric improved by 0.075 >= min_delta = 0.001. New best score: 0.151\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5179b917936f4bec85f05c30495120a9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_comp_metric improved by 0.027 >= min_delta = 0.001. New best score: 0.178\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "00142d0f2ee34e098f5cc0ef9e74cf13",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_comp_metric improved by 0.054 >= min_delta = 0.001. New best score: 0.232\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5df9e14793a24cb49d9a70930ef5ee68",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ba1f77289c414eabbacd9232464870eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_comp_metric improved by 0.025 >= min_delta = 0.001. New best score: 0.257\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fab383cb73014b8696cec441141c8e53",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_comp_metric improved by 0.038 >= min_delta = 0.001. New best score: 0.294\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a1820702368d48f792f74bc18836b65f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "025528a3dd05453d93a7b1a3d34dc776",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_comp_metric improved by 0.065 >= min_delta = 0.001. New best score: 0.359\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "99aef0e7f7b8473aadd004236c33d310",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_comp_metric improved by 0.064 >= min_delta = 0.001. New best score: 0.423\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c6a56366a2124c4da33bd1b9a630029a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "168fcfc48b4c47cc93baf66c4da99ed6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_comp_metric improved by 0.022 >= min_delta = 0.001. New best score: 0.445\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e9712625c6fe4fab981b08dc0f1a8497",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d42fbfecebd24e0b9b8fa1688857f70c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7a41e64cd36b4c29889eb0ae6abb48f0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_comp_metric improved by 0.012 >= min_delta = 0.001. New best score: 0.457\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "928f284119af42499817c8a3a65b9e6e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=20` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;2;161;247;255m\n",
      "[2025-12-13 18:41:35]\n",
      "INFO: Loading best model from: C:\\_GitHub\\CSIRO-Image2Biomass-Prediction\\notebooks\\kaggle\\checkpoints\\teacher\\fold4\\facebook/dinov2-base-local_train[5]Folds_log_fusion-gating_epochs20_bs4_gradacc4_lr3e-05_wd0.05_dr0.2_hr0.5-fold4-epoch=18-val_loss=1.0962-val_comp_metric=0.4569.ckpt\u001b[0m\n",
      "\u001b[38;2;161;247;255m\n",
      "[2025-12-13 18:41:37]\n",
      "INFO: Patch-level Teacher Model initialized:\n",
      "backbone=facebook/dinov2-base, hidden_dim=768, patch_size=14,\n",
      "fusion_method=gating, use_log_target=True\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "c:\\_GitHub\\CSIRO-Image2Biomass-Prediction\\.venv\\Lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:434: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=19` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "06603840b0564a56a055f4481bcb9f14",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>â–â–â–â–â–â–â–â–â–â–‚â–‚â–ƒâ–ƒâ–„â–„â–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–ˆ</td></tr><tr><td>lr-AdamW</td><td>â–ˆâ–ˆâ–ˆâ–ˆâ–‡â–‡â–‡â–†â–†â–…â–„â–„â–ƒâ–ƒâ–‚â–‚â–‚â–â–â–</td></tr><tr><td>train_loss_epoch</td><td>â–ˆâ–…â–„â–„â–ƒâ–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–</td></tr><tr><td>train_loss_step</td><td>â–ˆâ–„â–†â–„â–„â–„â–ƒâ–‚â–†â–‚â–„â–‚â–‚â–‚â–…â–‚â–‚â–ƒâ–‚â–‚â–‚â–‚â–â–‚â–‚â–â–â–â–‚â–â–‚â–â–â–â–‚â–‚â–â–â–â–‚</td></tr><tr><td>trainer/global_step</td><td>â–â–â–â–â–â–â–â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ</td></tr><tr><td>val_comp_metric</td><td>â–â–ƒâ–ƒâ–…â–…â–…â–†â–†â–†â–†â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ</td></tr><tr><td>val_loss</td><td>â–ˆâ–†â–…â–†â–ƒâ–ƒâ–ƒâ–ƒâ–‚â–ƒâ–‚â–‚â–‚â–â–â–â–â–â–â–â–</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>20</td></tr><tr><td>lr-AdamW</td><td>0.0</td></tr><tr><td>train_loss_epoch</td><td>0.50348</td></tr><tr><td>train_loss_step</td><td>1.02589</td></tr><tr><td>trainer/global_step</td><td>360</td></tr><tr><td>val_comp_metric</td><td>0.45692</td></tr><tr><td>val_loss</td><td>1.09622</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">facebook/dinov2-base-local_train[5]Folds_log_fusion-gating_epochs20_bs4_gradacc4_lr3e-05_wd0.05_dr0.2_hr0.5-fold4</strong> at: <a href='https://wandb.ai/dmykhailov-kyiv-school-of-economics/csiro-image2biomass-prediction/runs/ytym35z5' target=\"_blank\">https://wandb.ai/dmykhailov-kyiv-school-of-economics/csiro-image2biomass-prediction/runs/ytym35z5</a><br> View project at: <a href='https://wandb.ai/dmykhailov-kyiv-school-of-economics/csiro-image2biomass-prediction' target=\"_blank\">https://wandb.ai/dmykhailov-kyiv-school-of-economics/csiro-image2biomass-prediction</a><br>Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>wandb\\run-20251213_180220-ytym35z5\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Train on all folds\n",
    "fold_results = []\n",
    "\n",
    "for fold_id in range(N_FOLDS):\n",
    "    train_loader, val_loader = get_loaders(fold=fold_id, bs=BATCH_SIZE)\n",
    "\n",
    "    try:\n",
    "        model = BiomassTeacherModel(\n",
    "            backbone_name=MODEL,\n",
    "            tabular_dim=train_loader.dataset.tabular_features.shape[1],\n",
    "            num_targets=len(target_cols),\n",
    "            lr=LR,\n",
    "            weight_decay=WEIGHT_DECAY,\n",
    "            hidden_ratio=HIDDEN_RATIO,\n",
    "            dropout=DROPOUT_RATE,\n",
    "            fusion_method=FUSION_METHOD,\n",
    "            use_log_target=USE_LOG_TARGET\n",
    "        )\n",
    "    except Exception as e:\n",
    "        model = BiomassTeacherModelPatches(\n",
    "            backbone_name=MODEL,\n",
    "            tabular_dim=train_loader.dataset.tabular_features.shape[1],\n",
    "            num_targets=len(target_cols),\n",
    "            lr=LR,\n",
    "            weight_decay=WEIGHT_DECAY,\n",
    "            hidden_ratio=HIDDEN_RATIO,\n",
    "            dropout=DROPOUT_RATE,\n",
    "            fusion_method=FUSION_METHOD,\n",
    "            use_log_target=USE_LOG_TARGET\n",
    "        )\n",
    "\n",
    "\n",
    "    # Callbacks\n",
    "    checkpoint_callback = ModelCheckpoint(\n",
    "        monitor='val_comp_metric',  # 'val_loss'\n",
    "        dirpath=os.path.join(CHECKPOINTS_DIR, f'fold{fold_id}'),\n",
    "        filename=f'{DESCRIPTION_FULL}-fold{fold_id}' +\n",
    "        '-{epoch:02d}-{val_loss:.4f}-{val_comp_metric:.4f}',\n",
    "        save_top_k=3,  # Save top 3 instead of 1\n",
    "        mode='max'  # or 'min' for val_loss\n",
    "    )\n",
    "\n",
    "    early_stopping_callback = EarlyStopping(\n",
    "        monitor='val_comp_metric',  # 'val_loss'\n",
    "        patience=7,\n",
    "        mode='max',\n",
    "        verbose=True,\n",
    "        min_delta=1e-3\n",
    "    )\n",
    "\n",
    "    lr_monitor = LearningRateMonitor(logging_interval='epoch')\n",
    "\n",
    "    # Logger\n",
    "    wandb_logger = WandbLogger(\n",
    "        project=PROJECT_NAME,\n",
    "        name=f'{DESCRIPTION_FULL}-fold{fold_id}',\n",
    "        log_model='all',\n",
    "        tags=['patch-level', f'fold{fold_id}', FUSION_METHOD]\n",
    "    )\n",
    "\n",
    "    # Trainer\n",
    "    trainer = pl.Trainer(\n",
    "        max_epochs=EPOCHS,\n",
    "        accelerator=DEVICE.type,\n",
    "        precision='16-mixed' if torch.cuda.is_available() else 32,\n",
    "        accumulate_grad_batches=GRAD_ACCUM,\n",
    "        callbacks=[checkpoint_callback, early_stopping_callback, lr_monitor],\n",
    "        logger=wandb_logger,\n",
    "        log_every_n_steps=1,\n",
    "        gradient_clip_val=1.0,\n",
    "        enable_progress_bar=True,\n",
    "        deterministic=True\n",
    "    )\n",
    "\n",
    "    try:\n",
    "        # Train\n",
    "        trainer.fit(model, train_loader, val_loader)\n",
    "\n",
    "        # Load best checkpoint\n",
    "        best_model_path = checkpoint_callback.best_model_path\n",
    "        logger.info(f\"Loading best model from: {best_model_path}\")\n",
    "        try:\n",
    "            best_model = BiomassTeacherModel.load_from_checkpoint(best_model_path)\n",
    "        except Exception as e:\n",
    "            best_model = BiomassTeacherModelPatches.load_from_checkpoint(best_model_path)\n",
    "\n",
    "        # Evaluate on validation set\n",
    "        val_result = trainer.validate(best_model, val_loader, verbose=False)\n",
    "        fold_results.append({\n",
    "            'fold': fold_id,\n",
    "            'val_loss': val_result[0]['val_loss'],\n",
    "            'val_comp_metric': val_result[0]['val_comp_metric']\n",
    "        })\n",
    "\n",
    "    except SystemExit:\n",
    "        logger.warning(\n",
    "            f\"Training interrupted during fold {fold_id}. Exiting gracefully.\")\n",
    "        wandb_logger.experiment.finish()\n",
    "\n",
    "        # Clean up memory after each fold\n",
    "        del model, trainer, wandb_logger\n",
    "        del train_loader, val_loader\n",
    "        \n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "        torch.cuda.synchronize()\n",
    "\n",
    "        break\n",
    "\n",
    "    finally:\n",
    "        wandb_logger.experiment.finish()\n",
    "\n",
    "        # Clean up memory after each fold\n",
    "        del model, trainer, wandb_logger\n",
    "        del train_loader, val_loader\n",
    "        \n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "        torch.cuda.synchronize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "1f4c9c8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Summary\n",
      "\n",
      "   fold  val_loss  val_comp_metric\n",
      "0     0  0.894299         0.566824\n",
      "1     1  0.820352         0.578140\n",
      "2     2  0.892732         0.575059\n",
      "3     3  1.365784         0.543001\n",
      "4     4  1.096217         0.456924\n",
      "Mean Val Loss: 1.0139 Â± 0.2219\n"
     ]
    }
   ],
   "source": [
    "print(\"Training Summary\")\n",
    "print()\n",
    "results_df = pd.DataFrame(fold_results)\n",
    "print(results_df)\n",
    "print(f\"Mean Val Loss: {results_df['val_loss'].mean():.4f} Â± {results_df['val_loss'].std():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc5d4c96",
   "metadata": {},
   "source": [
    "## Prepare Data for Student"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "bb0de811",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_teacher_models(checkpoints_dir: str, n_folds: int) -> list:\n",
    "    \"\"\"\n",
    "    Load best checkpoints from all folds.\n",
    "    \n",
    "    Args:\n",
    "        checkpoints_dir: Directory with fold checkpoints\n",
    "        n_folds: Number of folds\n",
    "    \n",
    "    Returns:\n",
    "        List of loaded models\n",
    "    \"\"\"\n",
    "    models = []\n",
    "    \n",
    "    for fold_id in range(n_folds):\n",
    "        fold_dir = os.path.join(checkpoints_dir, f'fold{fold_id}')\n",
    "        \n",
    "        # Find best checkpoint file\n",
    "        ckpt_files = [f for f in os.listdir(fold_dir) if f.endswith('.ckpt')]\n",
    "        if not ckpt_files:\n",
    "            logger.warning(f\"No checkpoint found in {fold_dir}\")\n",
    "            continue\n",
    "\n",
    "        # sort by val_loss in filename (val_loss=xxx.ckpt)\n",
    "        ckpt_files.sort(key=lambda x: float(x.split('=')[-1].replace('.ckpt', '')))\n",
    "        \n",
    "        # Assuming the first one is the best\n",
    "        ckpt_path = os.path.join(fold_dir, ckpt_files[0])\n",
    "        \n",
    "        model = BiomassTeacherModel.load_from_checkpoint(ckpt_path)\n",
    "        logger.info(f\"Loading checkpoint: {ckpt_path}\")\n",
    "        model.eval()\n",
    "        model = model.to(DEVICE)\n",
    "        models.append(model)\n",
    "    \n",
    "    logger.success(f\"Loaded {len(models)} teacher models\")\n",
    "    return models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "d73cb71f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_teacher_models_facebook(checkpoints_dir: str, n_folds: int) -> list:\n",
    "    \"\"\"\n",
    "    Load best checkpoints from all folds.\n",
    "    \n",
    "    Args:\n",
    "        checkpoints_dir: Directory with fold checkpoints\n",
    "        n_folds: Number of folds\n",
    "    \n",
    "    Returns:\n",
    "        List of loaded models\n",
    "    \"\"\"\n",
    "    models = []\n",
    "    \n",
    "    for fold_id in range(n_folds):\n",
    "        fold_dir = os.path.join(checkpoints_dir, f'fold{fold_id}/facebook')\n",
    "        \n",
    "        # Find best checkpoint file\n",
    "        ckpt_files = [f for f in os.listdir(fold_dir) if f.endswith('.ckpt')]\n",
    "        if not ckpt_files:\n",
    "            logger.warning(f\"No checkpoint found in {fold_dir}\")\n",
    "            continue\n",
    "\n",
    "        # sort by comp_val_metric in filename (comp_val_metric=xxx.ckpt)\n",
    "        ckpt_files.sort(key=lambda x: float(x.split('=')[-1].replace('.ckpt', '')))\n",
    "        \n",
    "        # Assuming the last one is the best\n",
    "        ckpt_path = os.path.join(fold_dir, ckpt_files[-1])\n",
    "        \n",
    "        model = BiomassTeacherModelPatches.load_from_checkpoint(ckpt_path)\n",
    "        logger.info(f\"Loading checkpoint: {ckpt_path}\")\n",
    "        model.eval()\n",
    "        model = model.to(DEVICE)\n",
    "        models.append(model)\n",
    "    \n",
    "    logger.success(f\"Loaded {len(models)} teacher models\")\n",
    "    return models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "da56e402",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;2;161;247;255m\n",
      "[2025-12-13 18:41:53]\n",
      "INFO: Patch-level Teacher Model initialized:\n",
      "backbone=facebook/dinov2-base, hidden_dim=768, patch_size=14,\n",
      "fusion_method=gating, use_log_target=True\u001b[0m\n",
      "\u001b[38;2;161;247;255m\n",
      "[2025-12-13 18:41:54]\n",
      "INFO: Loading checkpoint: ./kaggle/checkpoints/teacher/fold0/facebook\\dinov2-base-local_train[5]Folds_log_fusion-gating_epochs20_bs4_gradacc4_lr3e-05_wd0.05_dr0.2_hr0.5-fold0-epoch=17-val_loss=0.8943-val_comp_metric=0.5668.ckpt\u001b[0m\n",
      "\u001b[38;2;161;247;255m\n",
      "[2025-12-13 18:41:55]\n",
      "INFO: Patch-level Teacher Model initialized:\n",
      "backbone=facebook/dinov2-base, hidden_dim=768, patch_size=14,\n",
      "fusion_method=gating, use_log_target=True\u001b[0m\n",
      "\u001b[38;2;161;247;255m\n",
      "[2025-12-13 18:41:56]\n",
      "INFO: Loading checkpoint: ./kaggle/checkpoints/teacher/fold1/facebook\\dinov2-base-local_train[5]Folds_log_fusion-gating_epochs20_bs4_gradacc4_lr3e-05_wd0.05_dr0.2_hr0.5-fold1-epoch=16-val_loss=0.8204-val_comp_metric=0.5781.ckpt\u001b[0m\n",
      "\u001b[38;2;161;247;255m\n",
      "[2025-12-13 18:41:57]\n",
      "INFO: Patch-level Teacher Model initialized:\n",
      "backbone=facebook/dinov2-base, hidden_dim=768, patch_size=14,\n",
      "fusion_method=gating, use_log_target=True\u001b[0m\n",
      "\u001b[38;2;161;247;255m\n",
      "[2025-12-13 18:41:58]\n",
      "INFO: Loading checkpoint: ./kaggle/checkpoints/teacher/fold2/facebook\\dinov2-base-local_train[5]Folds_log_fusion-gating_epochs20_bs4_gradacc4_lr3e-05_wd0.05_dr0.2_hr0.5-fold2-epoch=19-val_loss=0.8927-val_comp_metric=0.5751.ckpt\u001b[0m\n",
      "\u001b[38;2;161;247;255m\n",
      "[2025-12-13 18:41:59]\n",
      "INFO: Patch-level Teacher Model initialized:\n",
      "backbone=facebook/dinov2-base, hidden_dim=768, patch_size=14,\n",
      "fusion_method=gating, use_log_target=True\u001b[0m\n",
      "\u001b[38;2;161;247;255m\n",
      "[2025-12-13 18:41:59]\n",
      "INFO: Loading checkpoint: ./kaggle/checkpoints/teacher/fold3/facebook\\dinov2-base-local_train[5]Folds_log_fusion-gating_epochs20_bs4_gradacc4_lr3e-05_wd0.05_dr0.2_hr0.5-fold3-epoch=17-val_loss=1.3658-val_comp_metric=0.5430.ckpt\u001b[0m\n",
      "\u001b[38;2;161;247;255m\n",
      "[2025-12-13 18:42:01]\n",
      "INFO: Patch-level Teacher Model initialized:\n",
      "backbone=facebook/dinov2-base, hidden_dim=768, patch_size=14,\n",
      "fusion_method=gating, use_log_target=False\u001b[0m\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for BiomassTeacherModelPatches:\n\tMissing key(s) in state_dict: \"patch_head_green.1.weight\", \"patch_head_green.1.bias\", \"patch_head_green.4.weight\", \"patch_head_green.4.bias\", \"patch_head_clover.1.weight\", \"patch_head_clover.1.bias\", \"patch_head_clover.4.weight\", \"patch_head_clover.4.bias\", \"patch_head_dead.1.weight\", \"patch_head_dead.1.bias\", \"patch_head_dead.4.weight\", \"patch_head_dead.4.bias\". \n\tUnexpected key(s) in state_dict: \"patch_head_green.3.weight\", \"patch_head_green.3.bias\", \"patch_head_clover.3.weight\", \"patch_head_clover.3.bias\", \"patch_head_dead.3.weight\", \"patch_head_dead.3.bias\". ",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAssertionError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[71]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m     models = \u001b[43mload_teacher_models\u001b[49m\u001b[43m(\u001b[49m\u001b[43mCHECKPOINTS_DIR\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mN_FOLDS\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[69]\u001b[39m\u001b[32m, line 29\u001b[39m, in \u001b[36mload_teacher_models\u001b[39m\u001b[34m(checkpoints_dir, n_folds)\u001b[39m\n\u001b[32m     27\u001b[39m ckpt_path = os.path.join(fold_dir, ckpt_files[\u001b[32m0\u001b[39m])\n\u001b[32m---> \u001b[39m\u001b[32m29\u001b[39m model = \u001b[43mBiomassTeacherModel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mload_from_checkpoint\u001b[49m\u001b[43m(\u001b[49m\u001b[43mckpt_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     30\u001b[39m logger.info(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mLoading checkpoint: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mckpt_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\_GitHub\\CSIRO-Image2Biomass-Prediction\\.venv\\Lib\\site-packages\\pytorch_lightning\\utilities\\model_helpers.py:130\u001b[39m, in \u001b[36m_restricted_classmethod_impl.__get__.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    126\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[32m    127\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mThe classmethod `\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcls_type.\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.method.\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m` cannot be called on an instance.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    128\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m Please call it on the class type and make sure the return value is used.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    129\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m130\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcls_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\_GitHub\\CSIRO-Image2Biomass-Prediction\\.venv\\Lib\\site-packages\\pytorch_lightning\\core\\module.py:1781\u001b[39m, in \u001b[36mLightningModule.load_from_checkpoint\u001b[39m\u001b[34m(cls, checkpoint_path, map_location, hparams_file, strict, weights_only, **kwargs)\u001b[39m\n\u001b[32m   1696\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33mr\u001b[39m\u001b[33;03m\"\"\"Primary way of loading a model from a checkpoint. When Lightning saves a checkpoint it stores the arguments\u001b[39;00m\n\u001b[32m   1697\u001b[39m \u001b[33;03mpassed to ``__init__``  in the checkpoint under ``\"hyper_parameters\"``.\u001b[39;00m\n\u001b[32m   1698\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m   1779\u001b[39m \n\u001b[32m   1780\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1781\u001b[39m loaded = \u001b[43m_load_from_checkpoint\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1782\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   1783\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcheckpoint_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1784\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmap_location\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1785\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhparams_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1786\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstrict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1787\u001b[39m \u001b[43m    \u001b[49m\u001b[43mweights_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1788\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1789\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1790\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m cast(Self, loaded)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\_GitHub\\CSIRO-Image2Biomass-Prediction\\.venv\\Lib\\site-packages\\pytorch_lightning\\core\\saving.py:93\u001b[39m, in \u001b[36m_load_from_checkpoint\u001b[39m\u001b[34m(cls, checkpoint_path, map_location, hparams_file, strict, weights_only, **kwargs)\u001b[39m\n\u001b[32m     92\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28missubclass\u001b[39m(\u001b[38;5;28mcls\u001b[39m, pl.LightningModule):\n\u001b[32m---> \u001b[39m\u001b[32m93\u001b[39m     model = \u001b[43m_load_state\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcheckpoint\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstrict\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstrict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     94\u001b[39m     state_dict = checkpoint[\u001b[33m\"\u001b[39m\u001b[33mstate_dict\u001b[39m\u001b[33m\"\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\_GitHub\\CSIRO-Image2Biomass-Prediction\\.venv\\Lib\\site-packages\\pytorch_lightning\\core\\saving.py:167\u001b[39m, in \u001b[36m_load_state\u001b[39m\u001b[34m(cls, checkpoint, strict, **cls_kwargs_new)\u001b[39m\n\u001b[32m    165\u001b[39m     _cls_kwargs = {k: v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m _cls_kwargs.items() \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m cls_init_args_name}\n\u001b[32m--> \u001b[39m\u001b[32m167\u001b[39m obj = instantiator(\u001b[38;5;28mcls\u001b[39m, _cls_kwargs) \u001b[38;5;28;01mif\u001b[39;00m instantiator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43m_cls_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    169\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(obj, pl.LightningDataModule):\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[65]\u001b[39m\u001b[32m, line 45\u001b[39m, in \u001b[36mBiomassTeacherModel.__init__\u001b[39m\u001b[34m(self, backbone_name, tabular_dim, num_targets, lr, weight_decay, hidden_ratio, dropout, fusion_method, use_log_target)\u001b[39m\n\u001b[32m     44\u001b[39m     dummy = torch.randn(\u001b[32m1\u001b[39m, \u001b[32m3\u001b[39m, SIZE, SIZE)\n\u001b[32m---> \u001b[39m\u001b[32m45\u001b[39m     feat_dim = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbackbone\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdummy\u001b[49m\u001b[43m)\u001b[49m.shape[\u001b[32m1\u001b[39m]\n\u001b[32m     47\u001b[39m \u001b[38;5;28mself\u001b[39m.feat_dim = feat_dim\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\_GitHub\\CSIRO-Image2Biomass-Prediction\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\_GitHub\\CSIRO-Image2Biomass-Prediction\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\_GitHub\\CSIRO-Image2Biomass-Prediction\\.venv\\Lib\\site-packages\\timm\\models\\swin_transformer_v2.py:1007\u001b[39m, in \u001b[36mSwinTransformerV2.forward\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m    999\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Forward pass through the model.\u001b[39;00m\n\u001b[32m   1000\u001b[39m \n\u001b[32m   1001\u001b[39m \u001b[33;03mArgs:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   1005\u001b[39m \u001b[33;03m    Logits tensor of shape (B, num_classes).\u001b[39;00m\n\u001b[32m   1006\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1007\u001b[39m x = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mforward_features\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1008\u001b[39m x = \u001b[38;5;28mself\u001b[39m.forward_head(x)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\_GitHub\\CSIRO-Image2Biomass-Prediction\\.venv\\Lib\\site-packages\\timm\\models\\swin_transformer_v2.py:981\u001b[39m, in \u001b[36mSwinTransformerV2.forward_features\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m    973\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Forward pass through feature extraction layers.\u001b[39;00m\n\u001b[32m    974\u001b[39m \n\u001b[32m    975\u001b[39m \u001b[33;03mArgs:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    979\u001b[39m \u001b[33;03m    Feature tensor of shape (B, H', W', C).\u001b[39;00m\n\u001b[32m    980\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m981\u001b[39m x = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mpatch_embed\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    982\u001b[39m x = \u001b[38;5;28mself\u001b[39m.layers(x)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\_GitHub\\CSIRO-Image2Biomass-Prediction\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\_GitHub\\CSIRO-Image2Biomass-Prediction\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\_GitHub\\CSIRO-Image2Biomass-Prediction\\.venv\\Lib\\site-packages\\timm\\layers\\patch_embed.py:121\u001b[39m, in \u001b[36mPatchEmbed.forward\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m    120\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.strict_img_size:\n\u001b[32m--> \u001b[39m\u001b[32m121\u001b[39m     \u001b[43m_assert\u001b[49m\u001b[43m(\u001b[49m\u001b[43mH\u001b[49m\u001b[43m \u001b[49m\u001b[43m==\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mimg_size\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43mf\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mInput height (\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mH\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43m) doesn\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mt match model (\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mimg_size\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43m).\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    122\u001b[39m     _assert(W == \u001b[38;5;28mself\u001b[39m.img_size[\u001b[32m1\u001b[39m], \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mInput width (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mW\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m) doesn\u001b[39m\u001b[33m'\u001b[39m\u001b[33mt match model (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.img_size[\u001b[32m1\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m).\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\_GitHub\\CSIRO-Image2Biomass-Prediction\\.venv\\Lib\\site-packages\\torch\\__init__.py:2185\u001b[39m, in \u001b[36m_assert\u001b[39m\u001b[34m(condition, message)\u001b[39m\n\u001b[32m   2182\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m overrides.handle_torch_function(\n\u001b[32m   2183\u001b[39m         _assert, (condition,), condition, message\n\u001b[32m   2184\u001b[39m     )\n\u001b[32m-> \u001b[39m\u001b[32m2185\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m condition, message\n",
      "\u001b[31mAssertionError\u001b[39m: Input height (518) doesn't match model (256).",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[71]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      2\u001b[39m     models = load_teacher_models(CHECKPOINTS_DIR, N_FOLDS)\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m     models = \u001b[43mload_teacher_models_facebook\u001b[49m\u001b[43m(\u001b[49m\u001b[43mCHECKPOINTS_DIR\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mN_FOLDS\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[70]\u001b[39m\u001b[32m, line 29\u001b[39m, in \u001b[36mload_teacher_models_facebook\u001b[39m\u001b[34m(checkpoints_dir, n_folds)\u001b[39m\n\u001b[32m     26\u001b[39m \u001b[38;5;66;03m# Assuming the last one is the best\u001b[39;00m\n\u001b[32m     27\u001b[39m ckpt_path = os.path.join(fold_dir, ckpt_files[-\u001b[32m1\u001b[39m])\n\u001b[32m---> \u001b[39m\u001b[32m29\u001b[39m model = \u001b[43mBiomassTeacherModelPatches\u001b[49m\u001b[43m.\u001b[49m\u001b[43mload_from_checkpoint\u001b[49m\u001b[43m(\u001b[49m\u001b[43mckpt_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     30\u001b[39m logger.info(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mLoading checkpoint: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mckpt_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     31\u001b[39m model.eval()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\_GitHub\\CSIRO-Image2Biomass-Prediction\\.venv\\Lib\\site-packages\\pytorch_lightning\\utilities\\model_helpers.py:130\u001b[39m, in \u001b[36m_restricted_classmethod_impl.__get__.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    125\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m instance \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_scripting:\n\u001b[32m    126\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[32m    127\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mThe classmethod `\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcls_type.\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.method.\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m` cannot be called on an instance.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    128\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m Please call it on the class type and make sure the return value is used.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    129\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m130\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcls_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\_GitHub\\CSIRO-Image2Biomass-Prediction\\.venv\\Lib\\site-packages\\pytorch_lightning\\core\\module.py:1781\u001b[39m, in \u001b[36mLightningModule.load_from_checkpoint\u001b[39m\u001b[34m(cls, checkpoint_path, map_location, hparams_file, strict, weights_only, **kwargs)\u001b[39m\n\u001b[32m   1686\u001b[39m \u001b[38;5;129m@_restricted_classmethod\u001b[39m\n\u001b[32m   1687\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mload_from_checkpoint\u001b[39m(\n\u001b[32m   1688\u001b[39m     \u001b[38;5;28mcls\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1694\u001b[39m     **kwargs: Any,\n\u001b[32m   1695\u001b[39m ) -> Self:\n\u001b[32m   1696\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33mr\u001b[39m\u001b[33;03m\"\"\"Primary way of loading a model from a checkpoint. When Lightning saves a checkpoint it stores the arguments\u001b[39;00m\n\u001b[32m   1697\u001b[39m \u001b[33;03m    passed to ``__init__``  in the checkpoint under ``\"hyper_parameters\"``.\u001b[39;00m\n\u001b[32m   1698\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m   1779\u001b[39m \n\u001b[32m   1780\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1781\u001b[39m     loaded = \u001b[43m_load_from_checkpoint\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1782\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   1783\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcheckpoint_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1784\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmap_location\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1785\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhparams_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1786\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstrict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1787\u001b[39m \u001b[43m        \u001b[49m\u001b[43mweights_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1788\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1789\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1790\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(Self, loaded)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\_GitHub\\CSIRO-Image2Biomass-Prediction\\.venv\\Lib\\site-packages\\pytorch_lightning\\core\\saving.py:93\u001b[39m, in \u001b[36m_load_from_checkpoint\u001b[39m\u001b[34m(cls, checkpoint_path, map_location, hparams_file, strict, weights_only, **kwargs)\u001b[39m\n\u001b[32m     91\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m _load_state(\u001b[38;5;28mcls\u001b[39m, checkpoint, **kwargs)\n\u001b[32m     92\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28missubclass\u001b[39m(\u001b[38;5;28mcls\u001b[39m, pl.LightningModule):\n\u001b[32m---> \u001b[39m\u001b[32m93\u001b[39m     model = \u001b[43m_load_state\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcheckpoint\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstrict\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstrict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     94\u001b[39m     state_dict = checkpoint[\u001b[33m\"\u001b[39m\u001b[33mstate_dict\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m     95\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m state_dict:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\_GitHub\\CSIRO-Image2Biomass-Prediction\\.venv\\Lib\\site-packages\\pytorch_lightning\\core\\saving.py:189\u001b[39m, in \u001b[36m_load_state\u001b[39m\u001b[34m(cls, checkpoint, strict, **cls_kwargs_new)\u001b[39m\n\u001b[32m    186\u001b[39m     obj.on_load_checkpoint(checkpoint)\n\u001b[32m    188\u001b[39m \u001b[38;5;66;03m# load the state_dict on the model automatically\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m189\u001b[39m keys = \u001b[43mobj\u001b[49m\u001b[43m.\u001b[49m\u001b[43mload_state_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcheckpoint\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstate_dict\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstrict\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstrict\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[32m    191\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m strict:\n\u001b[32m    192\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m keys.missing_keys:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\_GitHub\\CSIRO-Image2Biomass-Prediction\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2629\u001b[39m, in \u001b[36mModule.load_state_dict\u001b[39m\u001b[34m(self, state_dict, strict, assign)\u001b[39m\n\u001b[32m   2621\u001b[39m         error_msgs.insert(\n\u001b[32m   2622\u001b[39m             \u001b[32m0\u001b[39m,\n\u001b[32m   2623\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mMissing key(s) in state_dict: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[33m. \u001b[39m\u001b[33m\"\u001b[39m.format(\n\u001b[32m   2624\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33m, \u001b[39m\u001b[33m\"\u001b[39m.join(\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mk\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\u001b[33m'\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m missing_keys)\n\u001b[32m   2625\u001b[39m             ),\n\u001b[32m   2626\u001b[39m         )\n\u001b[32m   2628\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(error_msgs) > \u001b[32m0\u001b[39m:\n\u001b[32m-> \u001b[39m\u001b[32m2629\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m   2630\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mError(s) in loading state_dict for \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[33m:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[33m\"\u001b[39m.format(\n\u001b[32m   2631\u001b[39m             \u001b[38;5;28mself\u001b[39m.\u001b[34m__class__\u001b[39m.\u001b[34m__name__\u001b[39m, \u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[33m\"\u001b[39m.join(error_msgs)\n\u001b[32m   2632\u001b[39m         )\n\u001b[32m   2633\u001b[39m     )\n\u001b[32m   2634\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m _IncompatibleKeys(missing_keys, unexpected_keys)\n",
      "\u001b[31mRuntimeError\u001b[39m: Error(s) in loading state_dict for BiomassTeacherModelPatches:\n\tMissing key(s) in state_dict: \"patch_head_green.1.weight\", \"patch_head_green.1.bias\", \"patch_head_green.4.weight\", \"patch_head_green.4.bias\", \"patch_head_clover.1.weight\", \"patch_head_clover.1.bias\", \"patch_head_clover.4.weight\", \"patch_head_clover.4.bias\", \"patch_head_dead.1.weight\", \"patch_head_dead.1.bias\", \"patch_head_dead.4.weight\", \"patch_head_dead.4.bias\". \n\tUnexpected key(s) in state_dict: \"patch_head_green.3.weight\", \"patch_head_green.3.bias\", \"patch_head_clover.3.weight\", \"patch_head_clover.3.bias\", \"patch_head_dead.3.weight\", \"patch_head_dead.3.bias\". "
     ]
    }
   ],
   "source": [
    "try:\n",
    "    models = load_teacher_models(CHECKPOINTS_DIR, N_FOLDS)\n",
    "except Exception as e:\n",
    "    models = load_teacher_models_facebook(CHECKPOINTS_DIR, N_FOLDS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfbd7b4b",
   "metadata": {},
   "source": [
    "### Direct Ensemble Soft Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "510df361",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_soft_targets_dataset():\n",
    "    \"\"\"\n",
    "    Create dataset with soft targets from teacher ensemble.\n",
    "    Predicts on ALL training data (100%).\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame with soft targets appended\n",
    "    \"\"\"\n",
    "    \n",
    "    # Load all teacher models\n",
    "    try:\n",
    "        teacher_models = load_teacher_models(CHECKPOINTS_DIR, N_FOLDS)\n",
    "    except Exception as e:\n",
    "        teacher_models = load_teacher_models_facebook(CHECKPOINTS_DIR, N_FOLDS)\n",
    "\n",
    "    if not teacher_models:\n",
    "        logger.error(\"No teacher models loaded!\")\n",
    "        return None\n",
    "    \n",
    "    logger.info(f\"Creating soft targets using {len(teacher_models)} teacher models...\")\n",
    "    \n",
    "    # Prepare full dataset with NO augmentation\n",
    "    full_dataset = BiomassDataset(\n",
    "        df=tabular_df,\n",
    "        tabular_features=tabular_data,\n",
    "        target_cols=target_cols,\n",
    "        img_dir=PATH_TRAIN_IMG,\n",
    "        transform=val_transform,  # Use validation transform (no augmentation)\n",
    "        is_test=False,\n",
    "        use_log_target=USE_LOG_TARGET\n",
    "    )\n",
    "    \n",
    "    # Create dataloader for inference\n",
    "    full_loader = DataLoader(\n",
    "        full_dataset,\n",
    "        batch_size=BATCH_SIZE,  # Larger batch for inference\n",
    "        shuffle=False,\n",
    "    )\n",
    "    \n",
    "    logger.info(f\"Total samples for soft targets: {len(full_dataset)}\")\n",
    "    logger.info(f\"Inference batches: {len(full_loader)}\")\n",
    "    \n",
    "    # Store predictions from all models\n",
    "    all_predictions = []\n",
    "    \n",
    "    # For each teacher model\n",
    "    for model_idx, teacher_model in enumerate(teacher_models):\n",
    "        logger.info(f\"Processing teacher model {model_idx + 1}/{len(teacher_models)}...\")\n",
    "        \n",
    "        model_predictions = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch_idx, batch in enumerate(full_loader):\n",
    "                # Move batch to device\n",
    "                batch = {k: v.to(DEVICE) if isinstance(v, torch.Tensor) else v \n",
    "                        for k, v in batch.items()}\n",
    "                \n",
    "                # Get predictions\n",
    "                preds = teacher_model.predict_step(batch, 0)  # [B, 3]\n",
    "                model_predictions.append(preds.cpu().numpy())\n",
    "                \n",
    "                if (batch_idx + 1) % 10 == 0:\n",
    "                    logger.debug(f\"  Batch {batch_idx + 1}/{len(full_loader)}\")\n",
    "        \n",
    "        # Concatenate all batches\n",
    "        model_preds_array = np.concatenate(model_predictions, axis=0)  # [N, 3]\n",
    "        all_predictions.append(model_preds_array)\n",
    "        \n",
    "        logger.success(f\"Model {model_idx + 1} predictions shape: {model_preds_array.shape}\")\n",
    "    \n",
    "    # Average predictions across all models\n",
    "    ensemble_predictions = np.mean(all_predictions, axis=0)  # [N, 3]\n",
    "    \n",
    "    logger.success(f\"Ensemble predictions shape: {ensemble_predictions.shape}\")\n",
    "    logger.info(f\"Ensemble predictions range: min={ensemble_predictions.min():.4f}, max={ensemble_predictions.max():.4f}\")\n",
    "    \n",
    "    # Create DataFrame with soft targets\n",
    "    soft_targets_df = tabular_df.copy()\n",
    "    \n",
    "    # Add soft target columns\n",
    "    soft_targets_df['Dry_Clover_g_soft'] = ensemble_predictions[:, 0]\n",
    "    soft_targets_df['Dry_Dead_g_soft'] = ensemble_predictions[:, 1]\n",
    "    soft_targets_df['Dry_Green_g_soft'] = ensemble_predictions[:, 2]\n",
    "    \n",
    "    # Keep original targets for reference\n",
    "    # (optional: remove them later for student training)\n",
    "    \n",
    "    logger.info(\"\\nSoft targets statistics:\")\n",
    "    logger.info(f\"Dry_Clover_g: mean={soft_targets_df['Dry_Clover_g_soft'].mean():.4f}, \"\n",
    "               f\"std={soft_targets_df['Dry_Clover_g_soft'].std():.4f}\")\n",
    "    logger.info(f\"Dry_Dead_g: mean={soft_targets_df['Dry_Dead_g_soft'].mean():.4f}, \"\n",
    "               f\"std={soft_targets_df['Dry_Dead_g_soft'].std():.4f}\")\n",
    "    logger.info(f\"Dry_Green_g: mean={soft_targets_df['Dry_Green_g_soft'].mean():.4f}, \"\n",
    "               f\"std={soft_targets_df['Dry_Green_g_soft'].std():.4f}\")\n",
    "    \n",
    "    return soft_targets_df, ensemble_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b67d9c59",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_soft_targets(soft_targets_df: pd.DataFrame, output_path: str = './kaggle/input/'):\n",
    "    \"\"\"\n",
    "    Save soft targets to CSV.\n",
    "    \n",
    "    Args:\n",
    "        soft_targets_df: DataFrame with soft targets\n",
    "        output_path: Path to save CSV\n",
    "    \"\"\"\n",
    "    os.makedirs(output_path, exist_ok=True)\n",
    "    \n",
    "    output_file = os.path.join(output_path, 'train_with_soft_targets.csv')\n",
    "    soft_targets_df.to_csv(output_file, index=False)\n",
    "    \n",
    "    logger.success(f\"Saved soft targets to: {output_file}\")\n",
    "    logger.info(f\"Shape: {soft_targets_df.shape}\")\n",
    "    logger.info(f\"Columns: {soft_targets_df.columns.tolist()}\")\n",
    "    \n",
    "    return output_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc1ce7ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_soft_targets(soft_targets_df: pd.DataFrame):\n",
    "    \"\"\"\n",
    "    Validate soft targets quality.\n",
    "    \n",
    "    Args:\n",
    "        soft_targets_df: DataFrame with soft targets\n",
    "    \"\"\"\n",
    "    logger.info(\"\\n=== Soft Targets Validation ===\")\n",
    "    \n",
    "    for col in ['Dry_Clover_g_soft', 'Dry_Dead_g_soft', 'Dry_Green_g_soft']:\n",
    "        original_col = col.replace('_soft', '')\n",
    "        \n",
    "        # Check for NaN\n",
    "        nan_count = soft_targets_df[col].isna().sum()\n",
    "        if nan_count > 0:\n",
    "            logger.warning(f\"{col}: {nan_count} NaN values\")\n",
    "        \n",
    "        # Compare soft vs hard targets\n",
    "        corr = soft_targets_df[original_col].corr(soft_targets_df[col])\n",
    "        mse = np.mean((soft_targets_df[original_col].values - soft_targets_df[col].values)**2)\n",
    "\n",
    "        # logger.info(f\"{col}:\")\n",
    "        # logger.info(f\"  Correlation with hard target: {corr:.4f}\")\n",
    "        # logger.info(f\"  MSE vs hard target: {mse:.4f}\")\n",
    "        # logger.info(f\"  Range: [{soft_targets_df[col].min():.4f}, {soft_targets_df[col].max():.4f}]\")\n",
    "        msg = [f\"{col}:\", f\"Correlation with hard target: {corr:.4f}\", f\"MSE vs hard target: {mse:.4f}\", f\"Range: [{soft_targets_df[col].min():.4f}, {soft_targets_df[col].max():.4f}]\"]\n",
    "        msg = \"\\n\".join(msg)\n",
    "        logger.info(msg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bfb0a17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;2;161;247;255m\n",
      "[2025-12-13 13:46:13]\n",
      "INFO: Patch-level Teacher Model initialized:\n",
      "backbone=facebook/dinov2-base, hidden_dim=768, patch_size=14,\n",
      "fusion_method=gating, use_log_target=False\u001b[0m\n",
      "\u001b[38;2;161;247;255m\n",
      "[2025-12-13 13:46:13]\n",
      "INFO: Loading checkpoint: ./kaggle/checkpoints/teacher/fold0/facebook\\dinov2-base-local_train[5]Folds_fusion-gating_epochs20_bs4_gradacc4_lr0.0001_wd0.05_dr0.2_hr0.5-fold0-epoch=13-val_loss=607.6509-val_comp_metric=0.5166.ckpt\u001b[0m\n",
      "\u001b[38;2;161;247;255m\n",
      "[2025-12-13 13:46:16]\n",
      "INFO: Patch-level Teacher Model initialized:\n",
      "backbone=facebook/dinov2-base, hidden_dim=768, patch_size=14,\n",
      "fusion_method=gating, use_log_target=False\u001b[0m\n",
      "\u001b[38;2;161;247;255m\n",
      "[2025-12-13 13:46:16]\n",
      "INFO: Loading checkpoint: ./kaggle/checkpoints/teacher/fold1/facebook\\dinov2-base-local_train[5]Folds_fusion-gating_epochs20_bs4_gradacc4_lr0.0001_wd0.05_dr0.2_hr0.5-fold1-epoch=19-val_loss=721.7338-val_comp_metric=0.5238.ckpt\u001b[0m\n",
      "\u001b[38;2;161;247;255m\n",
      "[2025-12-13 13:46:18]\n",
      "INFO: Patch-level Teacher Model initialized:\n",
      "backbone=facebook/dinov2-base, hidden_dim=768, patch_size=14,\n",
      "fusion_method=gating, use_log_target=False\u001b[0m\n",
      "\u001b[38;2;161;247;255m\n",
      "[2025-12-13 13:46:18]\n",
      "INFO: Loading checkpoint: ./kaggle/checkpoints/teacher/fold2/facebook\\dinov2-base-local_train[5]Folds_fusion-gating_epochs20_bs4_gradacc4_lr0.0001_wd0.05_dr0.2_hr0.5-fold2-epoch=14-val_loss=677.2773-val_comp_metric=0.4942.ckpt\u001b[0m\n",
      "\u001b[38;2;161;247;255m\n",
      "[2025-12-13 13:46:20]\n",
      "INFO: Patch-level Teacher Model initialized:\n",
      "backbone=facebook/dinov2-base, hidden_dim=768, patch_size=14,\n",
      "fusion_method=gating, use_log_target=False\u001b[0m\n",
      "\u001b[38;2;161;247;255m\n",
      "[2025-12-13 13:46:21]\n",
      "INFO: Loading checkpoint: ./kaggle/checkpoints/teacher/fold3/facebook\\dinov2-base-local_train[5]Folds_fusion-gating_epochs20_bs4_gradacc4_lr0.0001_wd0.05_dr0.2_hr0.5-fold3-epoch=17-val_loss=733.0281-val_comp_metric=0.4977.ckpt\u001b[0m\n",
      "\u001b[38;2;161;247;255m\n",
      "[2025-12-13 13:46:23]\n",
      "INFO: Patch-level Teacher Model initialized:\n",
      "backbone=facebook/dinov2-base, hidden_dim=768, patch_size=14,\n",
      "fusion_method=gating, use_log_target=False\u001b[0m\n",
      "\u001b[38;2;161;247;255m\n",
      "[2025-12-13 13:46:24]\n",
      "INFO: Loading checkpoint: ./kaggle/checkpoints/teacher/fold4/facebook\\dinov2-base-local_train[5]Folds_fusion-gating_epochs20_bs4_gradacc4_lr0.0001_wd0.05_dr0.2_hr0.5-fold4-epoch=17-val_loss=744.9141-val_comp_metric=0.5037.ckpt\u001b[0m\n",
      "\u001b[38;2;105;254;105m\n",
      "[2025-12-13 13:46:24]\n",
      "SUCCESS: Loaded 5 teacher models\u001b[0m\n",
      "\u001b[38;2;161;247;255m\n",
      "[2025-12-13 13:46:24]\n",
      "INFO: Creating soft targets using 5 teacher models...\u001b[0m\n",
      "\u001b[38;2;161;247;255m\n",
      "[2025-12-13 13:46:24]\n",
      "INFO: Total samples for soft targets: 357\u001b[0m\n",
      "\u001b[38;2;161;247;255m\n",
      "[2025-12-13 13:46:24]\n",
      "INFO: Inference batches: 45\u001b[0m\n",
      "\u001b[38;2;161;247;255m\n",
      "[2025-12-13 13:46:24]\n",
      "INFO: Processing teacher model 1/5...\u001b[0m\n",
      "\u001b[38;2;58;206;255m\n",
      "[2025-12-13 13:55:40]\n",
      "DEBUG:   Batch 10/45\u001b[0m\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[56]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Create soft targets\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m soft_targets_df, ensemble_preds = \u001b[43mcreate_soft_targets_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[53]\u001b[39m\u001b[32m, line 62\u001b[39m, in \u001b[36mcreate_soft_targets_dataset\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m     60\u001b[39m \u001b[38;5;66;03m# Get predictions\u001b[39;00m\n\u001b[32m     61\u001b[39m preds = teacher_model.predict_step(batch, \u001b[32m0\u001b[39m)  \u001b[38;5;66;03m# [B, 3]\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m62\u001b[39m model_predictions.append(\u001b[43mpreds\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcpu\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m.numpy())\n\u001b[32m     64\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (batch_idx + \u001b[32m1\u001b[39m) % \u001b[32m10\u001b[39m == \u001b[32m0\u001b[39m:\n\u001b[32m     65\u001b[39m     logger.debug(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m  Batch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbatch_idx\u001b[38;5;250m \u001b[39m+\u001b[38;5;250m \u001b[39m\u001b[32m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(full_loader)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# Create soft targets\n",
    "soft_targets_df, ensemble_preds = create_soft_targets_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52bac444",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;2;161;247;255m\n",
      "[2025-12-11 14:33:55]\n",
      "INFO: \n",
      "=== Soft Targets Validation ===\u001b[0m\n",
      "\u001b[38;2;161;247;255m\n",
      "[2025-12-11 14:33:55]\n",
      "INFO: Dry_Clover_g_soft:\n",
      "Correlation with hard target: 0.9426\n",
      "MSE vs hard target: 26.7841\n",
      "Range: [0.0000, 97.1455]\u001b[0m\n",
      "\u001b[38;2;161;247;255m\n",
      "[2025-12-11 14:33:55]\n",
      "INFO: Dry_Dead_g_soft:\n",
      "Correlation with hard target: 0.8593\n",
      "MSE vs hard target: 40.1800\n",
      "Range: [0.0000, 56.2889]\u001b[0m\n",
      "\u001b[38;2;161;247;255m\n",
      "[2025-12-11 14:33:55]\n",
      "INFO: Dry_Green_g_soft:\n",
      "Correlation with hard target: 0.9462\n",
      "MSE vs hard target: 69.3651\n",
      "Range: [0.0000, 142.3275]\u001b[0m\n",
      "\u001b[38;2;105;254;105m\n",
      "[2025-12-11 14:33:55]\n",
      "SUCCESS: Saved soft targets to: ./kaggle/input/train_with_soft_targets.csv\u001b[0m\n",
      "\u001b[38;2;161;247;255m\n",
      "[2025-12-11 14:33:55]\n",
      "INFO: Shape: (357, 15)\u001b[0m\n",
      "\u001b[38;2;161;247;255m\n",
      "[2025-12-11 14:33:55]\n",
      "INFO: Columns: ['image_path', 'Sampling_Date', 'State', 'Species', 'Height_Ave_cm', 'Pre_GSHH_NDVI', 'Dry_Clover_g', 'Dry_Dead_g', 'Dry_Green_g', 'Season', 'strat_group', 'fold', 'Dry_Clover_g_soft', 'Dry_Dead_g_soft', 'Dry_Green_g_soft']\u001b[0m\n",
      "File: ./kaggle/input/train_with_soft_targets.csv\n",
      "Samples: 357\n",
      "Soft target columns: ['Dry_Clover_g_soft', 'Dry_Dead_g_soft', 'Dry_Green_g_soft']\n"
     ]
    }
   ],
   "source": [
    "# Validate soft targets\n",
    "validate_soft_targets(soft_targets_df)\n",
    "\n",
    "# Save to CSV\n",
    "output_file = save_soft_targets(soft_targets_df)\n",
    "\n",
    "print(f\"File: {output_file}\")\n",
    "print(f\"Samples: {len(soft_targets_df)}\")\n",
    "print(f\"Soft target columns: {[c for c in soft_targets_df.columns if '_soft' in c]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2fd7f00",
   "metadata": {},
   "source": [
    "### Out of Fold Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "468d9e3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_oof_soft_targets():\n",
    "    \"\"\"\n",
    "    Create Out-Of-Fold soft targets from teacher models.\n",
    "    Each model predicts ONLY on its validation fold (unseen data).\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame with OOF soft targets\n",
    "    \"\"\"\n",
    "    logger.info(\"CREATING OUT-OF-FOLD (OOF) SOFT TARGETS\")\n",
    "    \n",
    "    # Initialize array to store OOF predictions\n",
    "    # Shape: [num_samples, 3] for 3 targets\n",
    "    oof_predictions = np.zeros((len(tabular_df), 3))\n",
    "    oof_indices = np.zeros(len(tabular_df), dtype=bool)  # Track which samples got predictions\n",
    "    \n",
    "    # For each fold\n",
    "    for fold_id in range(N_FOLDS):\n",
    "        logger.info(f\"\\nProcessing Fold {fold_id}...\")\n",
    "        \n",
    "        # Load model for this fold\n",
    "        fold_dir = os.path.join(CHECKPOINTS_DIR, f'fold{fold_id}')\n",
    "        ckpt_files = [f for f in os.listdir(fold_dir) if f.endswith('.ckpt')]\n",
    "        \n",
    "        if not ckpt_files:\n",
    "            logger.error(f\"No checkpoint found for fold {fold_id}\")\n",
    "            continue\n",
    "        \n",
    "        # Sort by val_loss to get best checkpoint\n",
    "        ckpt_files.sort(key=lambda x: float(x.split('=')[-1].replace('.ckpt', '')))\n",
    "        ckpt_path = os.path.join(fold_dir, ckpt_files[0])\n",
    "        \n",
    "        logger.info(f\"Loading checkpoint: {ckpt_path}\")\n",
    "        teacher_model = BiomassTeacherModel.load_from_checkpoint(ckpt_path)\n",
    "        teacher_model.eval()\n",
    "        teacher_model = teacher_model.to(DEVICE)\n",
    "        \n",
    "        # Get validation data for this fold\n",
    "        val_df = tabular_df[tabular_df['fold'] == fold_id].reset_index(drop=True)\n",
    "        val_indices = tabular_df[tabular_df['fold'] == fold_id].index.values\n",
    "        \n",
    "        logger.info(f\"Validation samples for fold {fold_id}: {len(val_df)}\")\n",
    "        \n",
    "        # Prepare tabular features for validation fold\n",
    "        fold_preprocessor = ColumnTransformer(\n",
    "            transformers=[\n",
    "                ('num', StandardScaler(), num_features),\n",
    "                ('cat', OneHotEncoder(sparse_output=False, handle_unknown='ignore'), cat_features)\n",
    "            ]\n",
    "        )\n",
    "        \n",
    "        # Fit on train, transform on val\n",
    "        train_df = tabular_df[tabular_df['fold'] != fold_id]\n",
    "        fold_preprocessor.fit(train_df)\n",
    "        val_tabular = fold_preprocessor.transform(val_df)\n",
    "        \n",
    "        # Create validation dataset\n",
    "        val_dataset = BiomassDataset(\n",
    "            df=val_df,\n",
    "            tabular_features=val_tabular,\n",
    "            target_cols=target_cols,\n",
    "            img_dir=PATH_TRAIN_IMG,\n",
    "            transform=val_transform,  # No augmentation\n",
    "            is_test=False,\n",
    "            use_log_target=USE_LOG_TARGET\n",
    "        )\n",
    "        \n",
    "        # Create validation loader\n",
    "        val_loader = DataLoader(\n",
    "            val_dataset,\n",
    "            batch_size=BATCH_SIZE * 2,\n",
    "            shuffle=False,  # IMPORTANT: keep order!\n",
    "            num_workers=min(NUM_WORKERS, 8),\n",
    "            pin_memory=True if torch.cuda.is_available() else False\n",
    "        )\n",
    "        \n",
    "        # Make predictions\n",
    "        fold_predictions = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch_idx, batch in enumerate(val_loader):\n",
    "                # Move batch to device\n",
    "                batch = {k: v.to(DEVICE) if isinstance(v, torch.Tensor) else v \n",
    "                        for k, v in batch.items()}\n",
    "                \n",
    "                # Get predictions\n",
    "                preds = teacher_model.predict_step(batch, 0)  # [B, 3]\n",
    "                fold_predictions.append(preds.cpu().numpy())\n",
    "        \n",
    "        # Concatenate batch predictions\n",
    "        fold_preds_array = np.concatenate(fold_predictions, axis=0)  # [N_val, 3]\n",
    "        \n",
    "        logger.success(f\"Fold {fold_id} predictions shape: {fold_preds_array.shape}\")\n",
    "        \n",
    "        # Verify indices match\n",
    "        assert len(fold_preds_array) == len(val_indices), \\\n",
    "            f\"Predictions length {len(fold_preds_array)} != indices length {len(val_indices)}\"\n",
    "        \n",
    "        # Store predictions at correct indices\n",
    "        oof_predictions[val_indices] = fold_preds_array\n",
    "        oof_indices[val_indices] = True\n",
    "        \n",
    "        logger.info(f\"Stored predictions for indices: {val_indices[:5]}... (showing first 5)\")\n",
    "        \n",
    "        # Clean up\n",
    "        del teacher_model\n",
    "        torch.cuda.empty_cache() if torch.cuda.is_available() else None\n",
    "    \n",
    "    # Verify all samples got predictions\n",
    "    if not oof_indices.all():\n",
    "        missing_count = (~oof_indices).sum()\n",
    "        logger.warning(f\"Missing predictions for {missing_count} samples!\")\n",
    "    else:\n",
    "        logger.success(f\"All {len(oof_predictions)} samples have OOF predictions!\")\n",
    "    \n",
    "    # Create DataFrame with OOF soft targets\n",
    "    soft_targets_df = tabular_df.copy()\n",
    "    \n",
    "    # Add OOF soft target columns\n",
    "    soft_targets_df['Dry_Clover_g_soft'] = oof_predictions[:, 0]\n",
    "    soft_targets_df['Dry_Dead_g_soft'] = oof_predictions[:, 1]\n",
    "    soft_targets_df['Dry_Green_g_soft'] = oof_predictions[:, 2]\n",
    "    \n",
    "    logger.info(\"\\n=== OOF Soft Targets Statistics ===\")\n",
    "    logger.info(f\"Dry_Clover_g: mean={soft_targets_df['Dry_Clover_g_soft'].mean():.4f}, \"\n",
    "               f\"std={soft_targets_df['Dry_Clover_g_soft'].std():.4f}\")\n",
    "    logger.info(f\"Dry_Dead_g: mean={soft_targets_df['Dry_Dead_g_soft'].mean():.4f}, \"\n",
    "               f\"std={soft_targets_df['Dry_Dead_g_soft'].std():.4f}\")\n",
    "    logger.info(f\"Dry_Green_g: mean={soft_targets_df['Dry_Green_g_soft'].mean():.4f}, \"\n",
    "               f\"std={soft_targets_df['Dry_Green_g_soft'].std():.4f}\")\n",
    "    \n",
    "    return soft_targets_df, oof_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7067bfc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_oof_soft_targets(soft_targets_df: pd.DataFrame, output_path: str = './kaggle/input/'):\n",
    "    \"\"\"\n",
    "    Save OOF soft targets to CSV.\n",
    "    \n",
    "    Args:\n",
    "        soft_targets_df: DataFrame with OOF soft targets\n",
    "        output_path: Path to save CSV\n",
    "    \"\"\"\n",
    "    os.makedirs(output_path, exist_ok=True)\n",
    "    \n",
    "    output_file = os.path.join(output_path, 'train_with_oof_soft_targets.csv')\n",
    "    soft_targets_df.to_csv(output_file, index=False)\n",
    "    \n",
    "    logger.success(f\"Saved OOF soft targets to: {output_file}\")\n",
    "    logger.info(f\"Shape: {soft_targets_df.shape}\")\n",
    "    logger.info(f\"Columns: {soft_targets_df.columns.tolist()}\")\n",
    "    \n",
    "    return output_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92a2fd88",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_oof_vs_ensemble(oof_df: pd.DataFrame, ensemble_df: pd.DataFrame):\n",
    "    \"\"\"\n",
    "    Compare OOF predictions vs direct ensemble predictions.\n",
    "\n",
    "    Args:\n",
    "        oof_df: DataFrame with OOF soft targets\n",
    "        ensemble_df: DataFrame with ensemble soft targets\n",
    "    \"\"\"\n",
    "    logger.info(\"COMPARING OOF vs ENSEMBLE PREDICTIONS\")\n",
    "\n",
    "    for col in ['Dry_Clover_g_soft', 'Dry_Dead_g_soft', 'Dry_Green_g_soft']:\n",
    "        oof_vals = oof_df[col].values\n",
    "        ens_vals = ensemble_df[col].values\n",
    "\n",
    "        # Correlation\n",
    "        corr = np.corrcoef(oof_vals, ens_vals)[0, 1]\n",
    "\n",
    "        # Mean Absolute Difference\n",
    "        mad = np.mean(np.abs(oof_vals - ens_vals))\n",
    "\n",
    "        # RMSE\n",
    "        rmse = np.sqrt(np.mean((oof_vals - ens_vals)**2))\n",
    "\n",
    "        msgs = [\n",
    "            f\"\\n{col}:\",\n",
    "            f\"  Correlation: {corr:.4f}\",\n",
    "            f\"  Mean Absolute Diff: {mad:.4f}\",\n",
    "            f\"  RMSE: {rmse:.4f}\",\n",
    "            f\"  OOF  range: [{oof_vals.min():.2f}, {oof_vals.max():.2f}]\",\n",
    "            f\"  Ensemble range: [{ens_vals.min():.2f}, {ens_vals.max():.2f}]\"\n",
    "        ]\n",
    "        logger.info(\"\\n\".join(msgs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98a17903",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;2;161;247;255m\n",
      "[2025-12-11 14:42:18]\n",
      "INFO: CREATING OUT-OF-FOLD (OOF) SOFT TARGETS\u001b[0m\n",
      "\u001b[38;2;161;247;255m\n",
      "[2025-12-11 14:42:18]\n",
      "INFO: \n",
      "Processing Fold 0...\u001b[0m\n",
      "\u001b[38;2;161;247;255m\n",
      "[2025-12-11 14:42:18]\n",
      "INFO: Loading checkpoint: ./kaggle/checkpoints/teacher/fold0\\swinv2_tiny_window8_256-local_train[5]Folds_log_fusion-mean_epochs20_bs16_gradacc1_lr0.0001_wd0.05_dr0.2_hr0.5-fold0-epoch=13-val_loss=0.8938.ckpt\u001b[0m\n",
      "\u001b[38;2;161;247;255m\n",
      "[2025-12-11 14:42:19]\n",
      "INFO: Model initialized: backbone=swinv2_tiny_window8_256, feat_dim=768, combined_dim=789, fusion=mean, use_log_target=True\u001b[0m\n",
      "\u001b[38;2;161;247;255m\n",
      "[2025-12-11 14:42:19]\n",
      "INFO: Validation samples for fold 0: 72\u001b[0m\n",
      "\u001b[38;2;105;254;105m\n",
      "[2025-12-11 14:42:28]\n",
      "SUCCESS: Fold 0 predictions shape: (72, 3)\u001b[0m\n",
      "\u001b[38;2;161;247;255m\n",
      "[2025-12-11 14:42:28]\n",
      "INFO: Stored predictions for indices: [ 1  3  6 11 12]... (showing first 5)\u001b[0m\n",
      "\u001b[38;2;161;247;255m\n",
      "[2025-12-11 14:42:28]\n",
      "INFO: \n",
      "Processing Fold 1...\u001b[0m\n",
      "\u001b[38;2;161;247;255m\n",
      "[2025-12-11 14:42:28]\n",
      "INFO: Loading checkpoint: ./kaggle/checkpoints/teacher/fold1\\swinv2_tiny_window8_256-local_train[5]Folds_log_fusion-mean_epochs20_bs16_gradacc1_lr0.0001_wd0.05_dr0.2_hr0.5-fold1-epoch=11-val_loss=0.8113.ckpt\u001b[0m\n",
      "\u001b[38;2;161;247;255m\n",
      "[2025-12-11 14:42:30]\n",
      "INFO: Model initialized: backbone=swinv2_tiny_window8_256, feat_dim=768, combined_dim=789, fusion=mean, use_log_target=True\u001b[0m\n",
      "\u001b[38;2;161;247;255m\n",
      "[2025-12-11 14:42:30]\n",
      "INFO: Validation samples for fold 1: 72\u001b[0m\n",
      "\u001b[38;2;105;254;105m\n",
      "[2025-12-11 14:42:39]\n",
      "SUCCESS: Fold 1 predictions shape: (72, 3)\u001b[0m\n",
      "\u001b[38;2;161;247;255m\n",
      "[2025-12-11 14:42:39]\n",
      "INFO: Stored predictions for indices: [ 2 10 16 18 20]... (showing first 5)\u001b[0m\n",
      "\u001b[38;2;161;247;255m\n",
      "[2025-12-11 14:42:39]\n",
      "INFO: \n",
      "Processing Fold 2...\u001b[0m\n",
      "\u001b[38;2;161;247;255m\n",
      "[2025-12-11 14:42:39]\n",
      "INFO: Loading checkpoint: ./kaggle/checkpoints/teacher/fold2\\swinv2_tiny_window8_256-local_train[5]Folds_log_fusion-mean_epochs20_bs16_gradacc1_lr0.0001_wd0.05_dr0.2_hr0.5-fold2-epoch=19-val_loss=0.9368.ckpt\u001b[0m\n",
      "\u001b[38;2;161;247;255m\n",
      "[2025-12-11 14:42:41]\n",
      "INFO: Model initialized: backbone=swinv2_tiny_window8_256, feat_dim=768, combined_dim=789, fusion=mean, use_log_target=True\u001b[0m\n",
      "\u001b[38;2;161;247;255m\n",
      "[2025-12-11 14:42:41]\n",
      "INFO: Validation samples for fold 2: 71\u001b[0m\n",
      "\u001b[38;2;105;254;105m\n",
      "[2025-12-11 14:42:50]\n",
      "SUCCESS: Fold 2 predictions shape: (71, 3)\u001b[0m\n",
      "\u001b[38;2;161;247;255m\n",
      "[2025-12-11 14:42:50]\n",
      "INFO: Stored predictions for indices: [ 0  4  5 15 27]... (showing first 5)\u001b[0m\n",
      "\u001b[38;2;161;247;255m\n",
      "[2025-12-11 14:42:50]\n",
      "INFO: \n",
      "Processing Fold 3...\u001b[0m\n",
      "\u001b[38;2;161;247;255m\n",
      "[2025-12-11 14:42:50]\n",
      "INFO: Loading checkpoint: ./kaggle/checkpoints/teacher/fold3\\swinv2_tiny_window8_256-local_train[5]Folds_log_fusion-mean_epochs20_bs16_gradacc1_lr0.0001_wd0.05_dr0.2_hr0.5-fold3-epoch=16-val_loss=1.1476.ckpt\u001b[0m\n",
      "\u001b[38;2;161;247;255m\n",
      "[2025-12-11 14:42:52]\n",
      "INFO: Model initialized: backbone=swinv2_tiny_window8_256, feat_dim=768, combined_dim=789, fusion=mean, use_log_target=True\u001b[0m\n",
      "\u001b[38;2;161;247;255m\n",
      "[2025-12-11 14:42:52]\n",
      "INFO: Validation samples for fold 3: 71\u001b[0m\n",
      "\u001b[38;2;105;254;105m\n",
      "[2025-12-11 14:43:01]\n",
      "SUCCESS: Fold 3 predictions shape: (71, 3)\u001b[0m\n",
      "\u001b[38;2;161;247;255m\n",
      "[2025-12-11 14:43:01]\n",
      "INFO: Stored predictions for indices: [ 9 13 14 21 23]... (showing first 5)\u001b[0m\n",
      "\u001b[38;2;161;247;255m\n",
      "[2025-12-11 14:43:01]\n",
      "INFO: \n",
      "Processing Fold 4...\u001b[0m\n",
      "\u001b[38;2;161;247;255m\n",
      "[2025-12-11 14:43:01]\n",
      "INFO: Loading checkpoint: ./kaggle/checkpoints/teacher/fold4\\_swinv2_tiny_window8_256-local_train[5]Folds_log_fusion-mean_epochs20_bs16_gradacc1_lr0.0001_wd0.05_dr0.2_hr0.5-fold4-epoch=19-val_loss=1.0456.ckpt\u001b[0m\n",
      "\u001b[38;2;161;247;255m\n",
      "[2025-12-11 14:43:02]\n",
      "INFO: Model initialized: backbone=swinv2_tiny_window8_256, feat_dim=768, combined_dim=789, fusion=mean, use_log_target=True\u001b[0m\n",
      "\u001b[38;2;161;247;255m\n",
      "[2025-12-11 14:43:03]\n",
      "INFO: Validation samples for fold 4: 71\u001b[0m\n",
      "\u001b[38;2;105;254;105m\n",
      "[2025-12-11 14:43:11]\n",
      "SUCCESS: Fold 4 predictions shape: (71, 3)\u001b[0m\n",
      "\u001b[38;2;161;247;255m\n",
      "[2025-12-11 14:43:11]\n",
      "INFO: Stored predictions for indices: [ 7  8 19 24 28]... (showing first 5)\u001b[0m\n",
      "\u001b[38;2;105;254;105m\n",
      "[2025-12-11 14:43:11]\n",
      "SUCCESS: All 357 samples have OOF predictions!\u001b[0m\n",
      "\u001b[38;2;161;247;255m\n",
      "[2025-12-11 14:43:11]\n",
      "INFO: \n",
      "=== OOF Soft Targets Statistics ===\u001b[0m\n",
      "\u001b[38;2;161;247;255m\n",
      "[2025-12-11 14:43:11]\n",
      "INFO: Dry_Clover_g: mean=6.6410, std=13.9213\u001b[0m\n",
      "\u001b[38;2;161;247;255m\n",
      "[2025-12-11 14:43:11]\n",
      "INFO: Dry_Dead_g: mean=11.5125, std=10.1113\u001b[0m\n",
      "\u001b[38;2;161;247;255m\n",
      "[2025-12-11 14:43:11]\n",
      "INFO: Dry_Green_g: mean=26.0409, std=24.9338\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Create OOF soft targets\n",
    "oof_soft_targets_df, oof_preds = create_oof_soft_targets()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ea8ecac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;2;161;247;255m\n",
      "[2025-12-11 14:43:11]\n",
      "INFO: \n",
      "=== Soft Targets Validation ===\u001b[0m\n",
      "\u001b[38;2;161;247;255m\n",
      "[2025-12-11 14:43:11]\n",
      "INFO: Dry_Clover_g_soft:\n",
      "Correlation with hard target: 0.8748\n",
      "MSE vs hard target: 45.3763\n",
      "Range: [0.0000, 83.9534]\u001b[0m\n",
      "\u001b[38;2;161;247;255m\n",
      "[2025-12-11 14:43:11]\n",
      "INFO: Dry_Dead_g_soft:\n",
      "Correlation with hard target: 0.7059\n",
      "MSE vs hard target: 79.0653\n",
      "Range: [0.0000, 54.3228]\u001b[0m\n",
      "\u001b[38;2;161;247;255m\n",
      "[2025-12-11 14:43:11]\n",
      "INFO: Dry_Green_g_soft:\n",
      "Correlation with hard target: 0.8542\n",
      "MSE vs hard target: 184.7793\n",
      "Range: [0.0000, 140.0474]\u001b[0m\n",
      "\u001b[38;2;105;254;105m\n",
      "[2025-12-11 14:43:11]\n",
      "SUCCESS: Saved OOF soft targets to: ./kaggle/input/train_with_oof_soft_targets.csv\u001b[0m\n",
      "\u001b[38;2;161;247;255m\n",
      "[2025-12-11 14:43:11]\n",
      "INFO: Shape: (357, 15)\u001b[0m\n",
      "\u001b[38;2;161;247;255m\n",
      "[2025-12-11 14:43:11]\n",
      "INFO: Columns: ['image_path', 'Sampling_Date', 'State', 'Species', 'Height_Ave_cm', 'Pre_GSHH_NDVI', 'Dry_Clover_g', 'Dry_Dead_g', 'Dry_Green_g', 'Season', 'strat_group', 'fold', 'Dry_Clover_g_soft', 'Dry_Dead_g_soft', 'Dry_Green_g_soft']\u001b[0m\n",
      "File: ./kaggle/input/train_with_oof_soft_targets.csv\n",
      "Samples: 357\n"
     ]
    }
   ],
   "source": [
    "# Validate OOF soft targets\n",
    "validate_soft_targets(oof_soft_targets_df)\n",
    "\n",
    "# Save to CSV\n",
    "oof_output_file = save_oof_soft_targets(oof_soft_targets_df)\n",
    "print(f\"File: {oof_output_file}\")\n",
    "print(f\"Samples: {len(oof_soft_targets_df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cda9e9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;2;161;247;255m\n",
      "[2025-12-11 14:43:11]\n",
      "INFO: COMPARING OOF vs ENSEMBLE PREDICTIONS\u001b[0m\n",
      "\u001b[38;2;161;247;255m\n",
      "[2025-12-11 14:43:11]\n",
      "INFO: \n",
      "Dry_Clover_g_soft:\n",
      "  Correlation: 0.9606\n",
      "  Mean Absolute Diff: 1.5858\n",
      "  RMSE: 4.0862\n",
      "  OOF  range: [0.00, 83.95]\n",
      "  Ensemble range: [0.00, 97.15]\u001b[0m\n",
      "\u001b[38;2;161;247;255m\n",
      "[2025-12-11 14:43:11]\n",
      "INFO: \n",
      "Dry_Dead_g_soft:\n",
      "  Correlation: 0.9250\n",
      "  Mean Absolute Diff: 2.5321\n",
      "  RMSE: 4.0186\n",
      "  OOF  range: [0.00, 54.32]\n",
      "  Ensemble range: [0.00, 56.29]\u001b[0m\n",
      "\u001b[38;2;161;247;255m\n",
      "[2025-12-11 14:43:11]\n",
      "INFO: \n",
      "Dry_Green_g_soft:\n",
      "  Correlation: 0.9481\n",
      "  Mean Absolute Diff: 4.5954\n",
      "  RMSE: 8.1427\n",
      "  OOF  range: [0.00, 140.05]\n",
      "  Ensemble range: [0.00, 142.33]\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Compare with ensemble predictions (if available)\n",
    "if 'soft_targets_df' in locals():\n",
    "    compare_oof_vs_ensemble(oof_soft_targets_df, soft_targets_df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "image2biomass",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
