{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "8570f5a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch: 2.9.1+cu128\n",
      "Device: NVIDIA GeForce RTX 5050 Laptop GPU\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "\n",
    "import warnings\n",
    "from src.BiomassImprovedCNN import BiomassImprovedCNN\n",
    "from src.BiomassTransformer import BiomassTransformer\n",
    "from src.BiomassDINOv3 import BiomassDINOv3\n",
    "\n",
    "from src.DINOv3Wrapper import DINOv3InferenceWrapper\n",
    "from src.TransformerWrapper import TransformerInferenceWrapper\n",
    "from src.CNNWrapper import InferenceWrapper\n",
    "\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "print(f\"PyTorch: {torch.__version__}\")\n",
    "print(f\"Device: {torch.cuda.get_device_name(0) if torch.cuda.is_available() else 'CPU'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "00a55cf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "CKPT_FOLDER = \"./kaggle/checkpoints/improved_cnn/fold1/local_facebook/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "6722b40f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Competition metric definition (required for checkpoint loading)\n",
    "labels = [\"Dry_Clover_g\", \"Dry_Dead_g\", \"Dry_Green_g\", \"Dry_Total_g\", \"GDM_g\"]\n",
    "\n",
    "weights = {\n",
    "    'Dry_Clover_g': 0.1,\n",
    "    'Dry_Dead_g': 0.1,\n",
    "    'Dry_Green_g': 0.1,\n",
    "    'Dry_Total_g': 0.5,\n",
    "    'GDM_g': 0.2,\n",
    "}\n",
    "\n",
    "def competition_metric(y_true, y_pred) -> float:\n",
    "    \"\"\"Calculate competition's weighted R2 score.\"\"\"\n",
    "    weights_array = np.array([weights[l] for l in labels])\n",
    "\n",
    "    y_weighted_mean = np.average(y_true, weights=weights_array, axis=1).mean()\n",
    "\n",
    "    ss_res = np.average((y_true - y_pred)**2,\n",
    "                        weights=weights_array, axis=1).mean()\n",
    "    ss_tot = np.average((y_true - y_weighted_mean)**2,\n",
    "                        weights=weights_array, axis=1).mean()\n",
    "\n",
    "    return 1 - ss_res / ss_tot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "02a68432",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_comp_metric(ckpt_path: str) -> float:\n",
    "    \"\"\"Extract competition metric from checkpoint filename.\"\"\"\n",
    "    return float(ckpt_path.split('val_r2_score=')[-1].split('.ckpt')[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "da2618f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def choose_best_checkpoint(ckpt_folder: str) -> str:\n",
    "    \"\"\"Choose the best checkpoint based on validation RMSE in filename.\"\"\"\n",
    "    ckpt_files = glob.glob(os.path.join(ckpt_folder, \"*.ckpt\"))\n",
    "    best_r2 = -np.inf\n",
    "\n",
    "    for ckpt in ckpt_files:\n",
    "        r2 = get_comp_metric(ckpt)\n",
    "        if r2 > best_r2:\n",
    "            best_r2 = r2\n",
    "            best_ckpt = ckpt\n",
    "\n",
    "    for i, ckpt in enumerate(ckpt_files):\n",
    "        print(f\"[{i}] {ckpt} --> R2: {get_comp_metric(ckpt):.6f}\")\n",
    "\n",
    "    print(f\"Best checkpoint: {best_ckpt} with R2: {best_r2:.6f}\")\n",
    "    best_ckpt_id = input(\"Choose the index of the best checkpoint to use ('Enter' for default): \")\n",
    "    if best_ckpt_id != '':\n",
    "        best_ckpt = ckpt_files[int(best_ckpt_id)]\n",
    "        print(f\"Selected checkpoint: {best_ckpt}\")\n",
    "    print(f\"Using checkpoint: {best_ckpt}\")\n",
    "    return best_ckpt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "5a26783b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_output_path(ckpt_path: str) -> str:\n",
    "    \"\"\"Generate output filename based on checkpoint name.\"\"\"\n",
    "    base_name = os.path.basename(ckpt_path).replace('.ckpt', '.pt')\n",
    "    return os.path.join(os.path.dirname(ckpt_path), base_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "345f4f48",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_class(ckpt_path: str) -> type:\n",
    "    \"\"\"Return appropriate model wrapper based on checkpoint filename.\"\"\"\n",
    "    ckpt_path = ckpt_path.lower()\n",
    "    print(ckpt_path)\n",
    "    \n",
    "    if 'dinov3' in ckpt_path:\n",
    "        return BiomassDINOv3\n",
    "    elif 'patch' in ckpt_path or 'vit' in ckpt_path:\n",
    "        return BiomassTransformer\n",
    "    \n",
    "    return BiomassImprovedCNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "9ae18a12",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_size_mean_std(ckpt_path: str) -> tuple[int, list[float], list[float]]:\n",
    "    \"\"\"Get input size, mean, and std for a given model.\"\"\"\n",
    "    mean = [0.485, 0.456, 0.406]\n",
    "    std = [0.229, 0.224, 0.225]\n",
    "    img_size = 224  # Default\n",
    "\n",
    "    ckpt_path = ckpt_path.lower()\n",
    "    \n",
    "    if 'convnextv2_tiny' in ckpt_path:\n",
    "        img_size = 384\n",
    "        print(f\"✅ Config hardcoded for ConvNeXtV2 Tiny\")\n",
    "    elif any(x in ckpt_path for x in ['vit_large_patch14_dinov2.lvd142m', 'vit_giant_patch14_dinov2.lvd142m']):\n",
    "        img_size = 518\n",
    "        print(f\"✅ Config hardcoded for DINOv2 ViT Large/Giant\")\n",
    "\n",
    "    return img_size, mean, std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "2c29ff72",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_wrapper(ckpt_path: str, model: type) -> tuple[InferenceWrapper | TransformerInferenceWrapper | DINOv3InferenceWrapper, int]:\n",
    "    \"\"\"Return appropriate model wrapper based on model type.\"\"\"\n",
    "    # Choose appropriate wrapper based on model type\n",
    "    is_dinov3 = isinstance(model, BiomassDINOv3)\n",
    "    is_transformer = isinstance(model, BiomassTransformer)\n",
    "\n",
    "    input_size, mean, std = get_size_mean_std(ckpt_path)\n",
    "\n",
    "    wrapper_kwargs = {\n",
    "        'lightning_model': model,\n",
    "        'img_size': input_size,\n",
    "        'mean': mean,\n",
    "        'std': std,\n",
    "    }\n",
    "\n",
    "    if is_dinov3:\n",
    "        print(\"Using DINOv3InferenceWrapper for export...\")\n",
    "        return DINOv3InferenceWrapper(**wrapper_kwargs), input_size\n",
    "    elif is_transformer:\n",
    "        print(\"Using TransformerInferenceWrapper for export...\")\n",
    "        return TransformerInferenceWrapper(**wrapper_kwargs), input_size\n",
    "    \n",
    "    print(\"Using InferenceWrapper for export...\")\n",
    "    return InferenceWrapper(**wrapper_kwargs), input_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "123f2a59",
   "metadata": {},
   "outputs": [],
   "source": [
    "def export_to_torchscript(checkpoint_path: str, output_path: str):\n",
    "    \"\"\"Export model to TorchScript for inference without class definition.\"\"\"\n",
    "\n",
    "    ModelClass = get_model_class(checkpoint_path)\n",
    "    print(f\"Using model class: {ModelClass.__name__}\")\n",
    "\n",
    "    # Load model\n",
    "    model = ModelClass.load_from_checkpoint(\n",
    "        checkpoint_path,\n",
    "        weights_only=False,\n",
    "        map_location='cpu'\n",
    "    )\n",
    "    model.eval()\n",
    "\n",
    "    # Wrap model for export\n",
    "    wrapped_model, input_size = get_model_wrapper(checkpoint_path, model)\n",
    "\n",
    "    print(f\"Input size for tracing: {input_size}x{input_size}\")\n",
    "\n",
    "    wrapped_model.eval()\n",
    "\n",
    "    # Create dummy input\n",
    "    dummy_left = torch.randn(1, 3, input_size, input_size)\n",
    "    dummy_right = torch.randn(1, 3, input_size, input_size)\n",
    "\n",
    "    # Test wrapper first\n",
    "    print(\"Testing wrapper before tracing...\")\n",
    "    with torch.no_grad():\n",
    "        test_output = wrapped_model(dummy_left, dummy_right)\n",
    "        print(f\"Wrapper output shape: {test_output.shape}\")\n",
    "        print(f\"Sample prediction: {test_output[0]}\")\n",
    "\n",
    "    # Trace model\n",
    "    print(\"\\nTracing model...\")\n",
    "    with torch.no_grad():\n",
    "        traced_model = torch.jit.trace(\n",
    "            wrapped_model,\n",
    "            (dummy_left, dummy_right)\n",
    "        )\n",
    "\n",
    "    # Save\n",
    "    traced_model.save(output_path)\n",
    "    print(f\"\\nModel exported to: {output_path}\")\n",
    "\n",
    "    # Validate export\n",
    "    print(\"\\nValidating export...\")\n",
    "    with torch.no_grad():\n",
    "        original_output = wrapped_model(dummy_left, dummy_right)\n",
    "        traced_output = traced_model(dummy_left, dummy_right)\n",
    "        max_diff = (original_output - traced_output).abs().max().item()\n",
    "        print(f\"Max difference between original and traced: {max_diff:.8f}\")\n",
    "\n",
    "        if max_diff < 1e-5:\n",
    "            print(\"✅ Export successful!\")\n",
    "        else:\n",
    "            print(f\"⚠️ Export may have issues (difference: {max_diff})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "1f5e7b69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0] ./kaggle/checkpoints/improved_cnn/fold1/local_facebook\\dinov3-vitl16-pretrain-lvd1689m_train[5]Folds_log_fusion-gated_spatial_cross_epochs15_bs4_gradacc4_lr0.0001_wd0.05_dr0.2_hr0.5-fold1-epoch=04-val_r2_score=0.7666.ckpt --> R2: 0.766600\n",
      "[1] ./kaggle/checkpoints/improved_cnn/fold1/local_facebook\\dinov3-vitl16-pretrain-lvd1689m_train[5]Folds_log_fusion-gated_spatial_cross_epochs15_bs4_gradacc4_lr0.0001_wd0.05_dr0.2_hr0.5-fold1-epoch=10-val_r2_score=0.7823.ckpt --> R2: 0.782300\n",
      "Best checkpoint: ./kaggle/checkpoints/improved_cnn/fold1/local_facebook\\dinov3-vitl16-pretrain-lvd1689m_train[5]Folds_log_fusion-gated_spatial_cross_epochs15_bs4_gradacc4_lr0.0001_wd0.05_dr0.2_hr0.5-fold1-epoch=10-val_r2_score=0.7823.ckpt with R2: 0.782300\n",
      "Using checkpoint: ./kaggle/checkpoints/improved_cnn/fold1/local_facebook\\dinov3-vitl16-pretrain-lvd1689m_train[5]Folds_log_fusion-gated_spatial_cross_epochs15_bs4_gradacc4_lr0.0001_wd0.05_dr0.2_hr0.5-fold1-epoch=10-val_r2_score=0.7823.ckpt\n"
     ]
    }
   ],
   "source": [
    "input_path = choose_best_checkpoint(CKPT_FOLDER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "e34cdb1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./kaggle/checkpoints/improved_cnn/fold1/local_facebook\\dinov3-vitl16-pretrain-lvd1689m_train[5]folds_log_fusion-gated_spatial_cross_epochs15_bs4_gradacc4_lr0.0001_wd0.05_dr0.2_hr0.5-fold1-epoch=10-val_r2_score=0.7823.ckpt\n",
      "Using model class: BiomassDINOv3\n",
      "Backbone output dimension: 1024\n",
      "Using DINOv3InferenceWrapper for export...\n",
      "Input size for tracing: 224x224\n",
      "Testing wrapper before tracing...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`loss_type=None` was set in the config but it is unrecognized. Using the default loss: `ForCausalLMLoss`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrapper output shape: torch.Size([1, 3])\n",
      "Sample prediction: tensor([ 4.5393, 11.6273,  8.0996])\n",
      "\n",
      "Tracing model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\_GitHub\\CSIRO-Image2Biomass-Prediction\\.venv\\Lib\\site-packages\\transformers\\integrations\\sdpa_attention.py:81: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  is_causal = query.shape[2] > 1 and attention_mask is None and getattr(module, \"is_causal\", True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model exported to: ./kaggle/checkpoints/improved_cnn/fold1/local_facebook\\dinov3-vitl16-pretrain-lvd1689m_train[5]Folds_log_fusion-gated_spatial_cross_epochs15_bs4_gradacc4_lr0.0001_wd0.05_dr0.2_hr0.5-fold1-epoch=10-val_r2_score=0.7823.pt\n",
      "\n",
      "Validating export...\n",
      "Max difference between original and traced: 0.00000000\n",
      "✅ Export successful!\n"
     ]
    }
   ],
   "source": [
    "export_to_torchscript(\n",
    "    checkpoint_path=input_path,\n",
    "    output_path=get_output_path(input_path)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b29dc6ae",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "image2biomass",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
