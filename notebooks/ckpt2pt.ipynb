{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8570f5a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch: 2.9.1+cu128\n",
      "Device: NVIDIA GeForce RTX 5050 Laptop GPU\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "\n",
    "import warnings\n",
    "from notebooks_config import setup_logging, CustomLogger\n",
    "\n",
    "from src.BiomassImprovedCNN import BiomassImprovedCNN\n",
    "from src.BiomassTransformer import BiomassTransformer\n",
    "from src.BiomassDINOv3 import BiomassDINOv3\n",
    "\n",
    "from src.DINOv3Wrapper import DINOv3InferenceWrapper\n",
    "from src.TransformerWrapper import TransformerInferenceWrapper\n",
    "from src.CNNWrapper import InferenceWrapper\n",
    "\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "print(f\"PyTorch: {torch.__version__}\")\n",
    "print(f\"Device: {torch.cuda.get_device_name(0) if torch.cuda.is_available() else 'CPU'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "00a55cf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "CKPT_FOLDER = \"./kaggle/checkpoints/improved_cnn/fold1/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6722b40f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Competition metric definition (required for checkpoint loading)\n",
    "labels = [\"Dry_Clover_g\", \"Dry_Dead_g\", \"Dry_Green_g\", \"Dry_Total_g\", \"GDM_g\"]\n",
    "\n",
    "weights = {\n",
    "    'Dry_Clover_g': 0.1,\n",
    "    'Dry_Dead_g': 0.1,\n",
    "    'Dry_Green_g': 0.1,\n",
    "    'Dry_Total_g': 0.5,\n",
    "    'GDM_g': 0.2,\n",
    "}\n",
    "\n",
    "def competition_metric(y_true, y_pred) -> float:\n",
    "    \"\"\"Calculate competition's weighted R2 score.\"\"\"\n",
    "    weights_array = np.array([weights[l] for l in labels])\n",
    "\n",
    "    y_weighted_mean = np.average(y_true, weights=weights_array, axis=1).mean()\n",
    "\n",
    "    ss_res = np.average((y_true - y_pred)**2,\n",
    "                        weights=weights_array, axis=1).mean()\n",
    "    ss_tot = np.average((y_true - y_weighted_mean)**2,\n",
    "                        weights=weights_array, axis=1).mean()\n",
    "\n",
    "    return 1 - ss_res / ss_tot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "02a68432",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_comp_metric(ckpt_path: str) -> float:\n",
    "    \"\"\"Extract competition metric from checkpoint filename.\"\"\"\n",
    "    return float(ckpt_path.split('val_r2_score=')[-1].split('.ckpt')[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d5ab39ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['./kaggle/checkpoints/improved_cnn/fold1\\\\local_vit_large_patch16_dinov3_train[5]Folds_log_fusion-gated_spatial_cross_epochs15_bs4_gradacc4_lr0.0001_wd0.05_dr0.2_hr0.5-fold1-epoch=04-val_r2_score=0.8214.ckpt',\n",
       " './kaggle/checkpoints/improved_cnn/fold1\\\\local_vit_large_patch16_dinov3_train[5]Folds_log_fusion-gated_spatial_cross_epochs15_bs4_gradacc4_lr0.0001_wd0.05_dr0.2_hr0.5-fold1-epoch=10-val_r2_score=0.8189.ckpt']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glob.glob(os.path.join(CKPT_FOLDER, \"*.ckpt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "da2618f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def choose_best_checkpoint(ckpt_folder: str) -> str:\n",
    "    \"\"\"Choose the best checkpoint based on validation RMSE in filename.\"\"\"\n",
    "    ckpt_files = glob.glob(os.path.join(ckpt_folder, \"*.ckpt\"))\n",
    "    best_r2 = -np.inf\n",
    "\n",
    "    for ckpt in ckpt_files:\n",
    "        r2 = get_comp_metric(ckpt)\n",
    "        if r2 > best_r2:\n",
    "            best_r2 = r2\n",
    "            best_ckpt = ckpt\n",
    "\n",
    "    for i, ckpt in enumerate(ckpt_files):\n",
    "        print(f\"[{i}] {ckpt} --> R2: {get_comp_metric(ckpt):.6f}\")\n",
    "\n",
    "    print(f\"Best checkpoint: {best_ckpt} with R2: {best_r2:.6f}\")\n",
    "    best_ckpt_id = input(\"Choose the index of the best checkpoint to use ('Enter' for default): \")\n",
    "    if best_ckpt_id != '':\n",
    "        best_ckpt = ckpt_files[int(best_ckpt_id)]\n",
    "        print(f\"Selected checkpoint: {best_ckpt}\")\n",
    "    print(f\"Using checkpoint: {best_ckpt}\")\n",
    "    return best_ckpt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5a26783b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_output_path(ckpt_path: str) -> str:\n",
    "    \"\"\"Generate output filename based on checkpoint name.\"\"\"\n",
    "    base_name = os.path.basename(ckpt_path).replace('.ckpt', '.pt')\n",
    "    return os.path.join(os.path.dirname(ckpt_path), base_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "345f4f48",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_class(ckpt_path: str) -> type:\n",
    "    \"\"\"Return appropriate model wrapper based on checkpoint filename.\"\"\"\n",
    "    ckpt_path = ckpt_path.lower()\n",
    "    print(ckpt_path)\n",
    "    \n",
    "    if 'dinov3' in ckpt_path:\n",
    "        return BiomassTransformer\n",
    "    elif 'patch' in ckpt_path or 'vit' in ckpt_path:\n",
    "        return BiomassTransformer\n",
    "    \n",
    "    return BiomassImprovedCNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9ae18a12",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_size_mean_std(ckpt_path: str) -> tuple[int, list[float], list[float]]:\n",
    "    \"\"\"Get input size, mean, and std for a given model.\"\"\"\n",
    "    mean = [0.485, 0.456, 0.406]\n",
    "    std = [0.229, 0.224, 0.225]\n",
    "    img_size = 224  # Default\n",
    "\n",
    "    ckpt_path = ckpt_path.lower()\n",
    "    \n",
    "    if 'convnextv2_tiny' in ckpt_path:\n",
    "        img_size = 384\n",
    "        print(f\"✅ Config hardcoded for ConvNeXtV2 Tiny\")\n",
    "    elif any(x in ckpt_path for x in ['vit_large_patch14_dinov2.lvd142m', 'vit_giant_patch14_dinov2.lvd142m']):\n",
    "        img_size = 518\n",
    "        print(f\"✅ Config hardcoded for DINOv2 ViT Large/Giant\")\n",
    "\n",
    "    return img_size, mean, std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2c29ff72",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_wrapper(ckpt_path: str, model: type) -> tuple[InferenceWrapper | TransformerInferenceWrapper | DINOv3InferenceWrapper, int]:\n",
    "    \"\"\"Return appropriate model wrapper based on model type.\"\"\"\n",
    "    # Choose appropriate wrapper based on model type\n",
    "    is_dinov3 = isinstance(model, BiomassDINOv3)\n",
    "    is_transformer = isinstance(model, BiomassTransformer)\n",
    "\n",
    "    input_size, mean, std = get_size_mean_std(ckpt_path)\n",
    "\n",
    "    wrapper_kwargs = {\n",
    "        'lightning_model': model,\n",
    "        'img_size': input_size,\n",
    "        'mean': mean,\n",
    "        'std': std,\n",
    "    }\n",
    "\n",
    "    if is_dinov3:\n",
    "        print(\"Using DINOv3InferenceWrapper for export...\")\n",
    "        return DINOv3InferenceWrapper(**wrapper_kwargs), input_size\n",
    "    elif is_transformer:\n",
    "        print(\"Using TransformerInferenceWrapper for export...\")\n",
    "        return TransformerInferenceWrapper(**wrapper_kwargs), input_size\n",
    "    \n",
    "    print(\"Using InferenceWrapper for export...\")\n",
    "    return InferenceWrapper(**wrapper_kwargs), input_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "123f2a59",
   "metadata": {},
   "outputs": [],
   "source": [
    "def export_to_torchscript(checkpoint_path: str, output_path: str):\n",
    "    \"\"\"Export model to TorchScript specifically for CUDA inference.\"\"\"\n",
    "\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    if device.type == 'cpu':\n",
    "        print(\"⚠️ WARNING: CUDA not available. Exporting on CPU might cause issues on GPU later.\")\n",
    "\n",
    "    ModelClass = get_model_class(checkpoint_path)\n",
    "    print(f\"Using model class: {ModelClass.__name__}\")\n",
    "\n",
    "    # Load model on CPU for export\n",
    "    model = ModelClass.load_from_checkpoint(\n",
    "        checkpoint_path,\n",
    "        weights_only=False,\n",
    "        map_location=device\n",
    "    )\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "\n",
    "    # Wrap model for export\n",
    "    wrapped_model, input_size = get_model_wrapper(checkpoint_path, model)\n",
    "\n",
    "    if not hasattr(wrapped_model, '_img_size'):\n",
    "         wrapped_model.register_buffer('_img_size', torch.tensor(input_size, device=device))\n",
    "\n",
    "    print(f\"Input size for tracing: {input_size}x{input_size}\")\n",
    "\n",
    "    # IMPORTANT: Keep everything on GPU during tracing\n",
    "    wrapped_model.to(device)\n",
    "    wrapped_model.eval()\n",
    "\n",
    "    # Create dummy input on CPU\n",
    "    dummy_left = torch.randn(1, 3, input_size, input_size, device=device)\n",
    "    dummy_right = torch.randn(1, 3, input_size, input_size, device=device)\n",
    "\n",
    "    # Test wrapper first\n",
    "    print(\"Testing wrapper before tracing...\")\n",
    "    with torch.no_grad():\n",
    "        test_output = wrapped_model(dummy_left, dummy_right)\n",
    "        print(f\"Wrapper output shape: {test_output.shape}\")\n",
    "        print(f\"Sample prediction: {test_output[0]}\")\n",
    "\n",
    "    # Trace model on CPU\n",
    "    print(f\"\\nTracing model on {device.type.upper()}...\")\n",
    "    with torch.no_grad():\n",
    "        traced_model = torch.jit.trace(\n",
    "            wrapped_model,\n",
    "            (dummy_left, dummy_right),\n",
    "            check_trace=True,\n",
    "            strict=False  # Allow some flexibility for DINOv3\n",
    "        )\n",
    "\n",
    "    # Save\n",
    "    traced_model.save(output_path)\n",
    "    print(f\"\\nModel exported to: {output_path}\")\n",
    "\n",
    "    # Validate export on CPU\n",
    "    print(f\"\\nValidating export on {device.type.upper()}...\")\n",
    "    with torch.no_grad():\n",
    "        original_output = wrapped_model(dummy_left, dummy_right)\n",
    "        traced_output = traced_model(dummy_left, dummy_right)\n",
    "        max_diff = (original_output - traced_output).abs().max().item()\n",
    "        print(f\"Max difference between original and traced: {max_diff:.8f}\")\n",
    "\n",
    "        if max_diff < 1e-5:\n",
    "            print(\"✅ Export successful!\")\n",
    "        else:\n",
    "            print(f\"⚠️ Export may have issues (difference: {max_diff})\")\n",
    "    \n",
    "    # Test device compatibility\n",
    "    print(\"\\nTesting device compatibility...\")\n",
    "    loaded_model = torch.jit.load(output_path, map_location=device)\n",
    "    \n",
    "    if torch.cuda.is_available():\n",
    "        try:\n",
    "            # Move to CUDA and test\n",
    "            loaded_model = loaded_model.to('cuda')\n",
    "            test_left = dummy_left.to('cuda')\n",
    "            test_right = dummy_right.to('cuda')\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                cuda_output = loaded_model(test_left, test_right)\n",
    "            \n",
    "            print(\"✅ Model works on CUDA\")\n",
    "            \n",
    "            # Move back to CPU for final save\n",
    "            loaded_model = loaded_model.cpu()\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"⚠️ CUDA test failed: {e}\")\n",
    "            print(\"Model saved for CPU only\")\n",
    "    else:\n",
    "        print(\"⚠️ CUDA not available, skipping CUDA test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "1f5e7b69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0] ./kaggle/checkpoints/improved_cnn/fold1\\local_vit_large_patch16_dinov3_train[5]Folds_log_fusion-gated_spatial_cross_epochs15_bs4_gradacc4_lr0.0001_wd0.05_dr0.2_hr0.5-fold1-epoch=04-val_r2_score=0.8214.ckpt --> R2: 0.821400\n",
      "[1] ./kaggle/checkpoints/improved_cnn/fold1\\local_vit_large_patch16_dinov3_train[5]Folds_log_fusion-gated_spatial_cross_epochs15_bs4_gradacc4_lr0.0001_wd0.05_dr0.2_hr0.5-fold1-epoch=10-val_r2_score=0.8189.ckpt --> R2: 0.818900\n",
      "Best checkpoint: ./kaggle/checkpoints/improved_cnn/fold1\\local_vit_large_patch16_dinov3_train[5]Folds_log_fusion-gated_spatial_cross_epochs15_bs4_gradacc4_lr0.0001_wd0.05_dr0.2_hr0.5-fold1-epoch=04-val_r2_score=0.8214.ckpt with R2: 0.821400\n",
      "Using checkpoint: ./kaggle/checkpoints/improved_cnn/fold1\\local_vit_large_patch16_dinov3_train[5]Folds_log_fusion-gated_spatial_cross_epochs15_bs4_gradacc4_lr0.0001_wd0.05_dr0.2_hr0.5-fold1-epoch=04-val_r2_score=0.8214.ckpt\n"
     ]
    }
   ],
   "source": [
    "input_path = choose_best_checkpoint(CKPT_FOLDER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e34cdb1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./kaggle/checkpoints/improved_cnn/fold1\\local_vit_large_patch16_dinov3_train[5]folds_log_fusion-gated_spatial_cross_epochs15_bs4_gradacc4_lr0.0001_wd0.05_dr0.2_hr0.5-fold1-epoch=04-val_r2_score=0.8214.ckpt\n",
      "Using model class: BiomassTransformer\n",
      "DEBUG: Backbone output shape = torch.Size([1, 261, 1024])\n",
      "DEBUG: Detected [B, N, C] format, C = 1024\n",
      "Backbone output dimension: 1024\n",
      "Using TransformerInferenceWrapper for export...\n",
      "Input size for tracing: 224x224\n",
      "Testing wrapper before tracing...\n",
      "Wrapper output shape: torch.Size([1, 3])\n",
      "Sample prediction: tensor([ 5.6901, 11.6275,  6.5422], device='cuda:0')\n",
      "\n",
      "Tracing model on CUDA...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\_GitHub\\CSIRO-Image2Biomass-Prediction\\.venv\\Lib\\site-packages\\torch\\__init__.py:2185: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  assert condition, message\n",
      "c:\\_GitHub\\CSIRO-Image2Biomass-Prediction\\.venv\\Lib\\site-packages\\timm\\layers\\pos_embed_sincos.py:935: TracerWarning: Converting a tensor to a Python float might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  h_denom = float(height)\n",
      "c:\\_GitHub\\CSIRO-Image2Biomass-Prediction\\.venv\\Lib\\site-packages\\timm\\layers\\pos_embed_sincos.py:936: TracerWarning: Converting a tensor to a Python float might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  w_denom = float(width)\n",
      "c:\\_GitHub\\CSIRO-Image2Biomass-Prediction\\.venv\\Lib\\site-packages\\timm\\layers\\pos_embed_sincos.py:1073: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  assert self.periods.numel() == dim\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model exported to: ./kaggle/checkpoints/improved_cnn/fold1\\local_vit_large_patch16_dinov3_train[5]Folds_log_fusion-gated_spatial_cross_epochs15_bs4_gradacc4_lr0.0001_wd0.05_dr0.2_hr0.5-fold1-epoch=04-val_r2_score=0.8214.pt\n",
      "\n",
      "Validating export on CUDA...\n",
      "Max difference between original and traced: 0.00000286\n",
      "✅ Export successful!\n",
      "\n",
      "Testing device compatibility...\n",
      "✅ Model works on CUDA\n"
     ]
    }
   ],
   "source": [
    "export_to_torchscript(\n",
    "    checkpoint_path=input_path,\n",
    "    output_path=get_output_path(input_path)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b29dc6ae",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "image2biomass",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
